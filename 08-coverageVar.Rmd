## Analysis of coverage variability

We will use the methods described in Hart et al. (2013) [@Hart:2013aa] 
to characterize coverage variability in these data. 
These methods do not take multiple comparisons into account. 
Other tools for sample size calculation in RNA-Seq studies include 
Bi and Liu (2016) [@Bi:2016aa,], Baccarella (2018) [@Baccarella:2018aa], 
Guo (2014) [@Guo:2014aa], Yu (2017) [@Yu:2017aa], and 
Zhao (2018) [@Zhao:2018ab]. 
Poplawski (2018) [@Poplawski:2017aa] 
evaluated RNA-seq sample size tools identified from a systematic search. They found the six evaluated tools provided widely different answers, which were strongly affected by fold change.


The references listed above aim at providing guidance for RNA-Seq experimental design.
There is much discussion and a wide range of opinion on sample size requirements
to ensure reproducibility in RNA-Seq results.  At one end of the spectrum, 
Ein-Dor et al. (2006) [@Ein-Dor:2006aa] argue that thousands of samples are needed to 
generate a robust gene list for predicting outcome in cancer.  At the other end,
Dobbin et al. (2007, 2008) [@Dobbin:2007aa;@Dobbin:2008aa] claim that sample sizes in 
the range of 20–30 per class may be adequate for building a good predictor in many cases. 
Part of the disparity in sample size requirement recomendation comes from 
differences of opinion in terms of what constitutes `reproducible results`. 
In the context of sample classification, if we focus on  the predicted probabilities
for individual samples, we may find good  reproducibility across studies with
moderate samples sizes.  If, on the other hand, we closely inspect the gene
signatures reported across studies, much greater sample sizes may be required to
achieve concordance.  Kim (2009) [@Kim:2009aa], like Ein-Dor et al., also find
issues in RNA-Seq research in terms of the instability of identified prognostic 
gene signatures, few overlap between independently developed prognostic gene signatures, and
poor inter-study applicability of gene signatures.  Fan et al. (2006) [@Fan:2006aa],
on the other hand, found good concordance among gene-expression–based predictors 
for breast cancer.  We will return to this question when we examine
the relationship between classification model results and sample size in this
dataset later on this paper.  


For two groups comparisons, the basic formula for the required number of samples per group is:

$$ n = 2(z_{1-\alpha/2} + z_{\beta})^2 \frac{(1/\mu + \sigma^2)}{ln(\Delta^2)} $$

* The parameters $\alpha$ and $\beta$ are **size** and **power** of the test.  
* $\Delta$ is the targeted **effect size**.   
* $\mu$ and $\sigma$ are the **mean** and **coefficient of variation** 
of the distribution of measurement, gene representation indices in this case.

These three parameters will be fixed across genes or a given study, and are often dictated by
external requirements. Typical values might be an effect size of $\Delta = 1.5$ 
(a.k.a fold change), corresponding to detection of a 50% change in gene expression 
between the two groups.
$z_{1 - .05/2} = 1.96$, corresponding to a two sided test at size $\alpha = 0.05$;
and $z_{.90}= 1.28$ corresponding to 90% power.
The other two variables will be gene and experiment dependent: the normalized depth 
of coverage $\mu$ of the gene, and the coefficient of variation $\sigma$ in this 
gene between biological replicates.  The technical variation of the 
comparison is inversely proportional to the number of sequenced reads
for the gene and therefore decreases with sequencing depth.
The biological variation is a property of the particular gene/model system/condition under study.
One would expect it to be smaller for uniform systems such as cell culture and/or products that
are under tight regulatory control, and larger for less uniform replicates such 
as human subject samples.  The dataset under study in this report is the first of its
kind to give us an idea of variability levels in 5hmC representation.


<!-- SKIP THIS
Before examining biological variability lets take a look at
the sequencing rates.

-->
```{r sumCount, cache=T, cache.vars='DD.cpmSum.frm', eval=F, echo=F}

countSum_frm <- do.call(
  "rbind",
  lapply(unique(sampDescA$group), function(GRP) {
    GRP_featureCountsAF <- featureCountsAF[, sampDescA$group == GRP]
    N_GRP <- ncol(GRP_featureCountsAF)
    MeanTotCovM <- mean(colSums(featureCountsAF[, sampDescA$group == GRP])) / 1e6
    GRP_countSum_mtx <- do.call("cbind", lapply(
      1:ncol(GRP_featureCountsAF),
      function(CC) {
        CC_cpm_vec <- GRP_featureCountsAF[, CC]
        100 * table(cut(CC_cpm_vec, breaks = c(0, .1, 1, 10, 50, 100, 1000, Inf))) /
          length(CC_cpm_vec)
      }
    ))
    GRP_countSum_vec <- apply(GRP_countSum_mtx, 1, median)
    GRP_Zeros_vec <- colSums(GRP_featureCountsAF == 0)
    GRP_ZerosPct_vec <- 100 * GRP_Zeros_vec / nrow(GRP_featureCountsAF)
    data.frame(
      Group = GRP, N = N_GRP, MeanTotCovM = round(MeanTotCovM, 1),
      Zeros = median(GRP_Zeros_vec), ZeroPct = median(GRP_ZerosPct_vec),
      t(GRP_countSum_vec), check.names = F
    )
  })
)
knitr::kable(countSum_frm) %>%
  kableExtra::kable_styling(full_width = F)

```


As in Hart et al. (2013) [@Hart:2013aa] we estimate the 
biological coefficient of variation (CV) in expression across samples in the data set
using a negative binomial model.

<!--
From [edgeR User's Guide](https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf)
, the biolgical coefficient of variation (BCV), is the square root of the dispersion
parameter under the negative binomial model. Hence, it is equivalent to estimating the
dispersion(s) of the negative binomial model.
-->

```{r estBCV, cache=T, cache.vars=c('BCV_mtx','BCV_90perc_vec', 'BCV_50perc_vec'),message=F, echo=F,include=F}

BCV_mtx <- do.call("cbind", lapply(
  unique(sampDescA$group),
  function(GRP) {
    GRP_dgel <-
      edgeR::DGEList(counts = featureCountsAF[, sampDescA$group == GRP])
    GRP_dgel <- edgeR::estimateDisp(GRP_dgel)

    sqrt(GRP_dgel$tagwise.dispersion)
  }
))
colnames(BCV_mtx) <- unique(sampDescA$group)

```

```{r plotBCV,  fig.height=5, fig.width=10, fig.cap="Cumulative Distribution of CV - rug = 90th percentile",message=F, echo=F,include=T}

plot(spatstat::CDF(density(BCV_mtx[, 1])),
  col = groupCol[colnames(BCV_mtx)[1]],
  lwd = 2, ylab = "Prob(BCV<x)",
  xlim = c(0, 0.5)
)
for (JJ in 2:ncol(BCV_mtx)) {
  plot(spatstat::CDF(density(BCV_mtx[, JJ])),
    col = groupCol[colnames(BCV_mtx)[JJ]],
    lwd = 2, add = T, xlim = c(0, 2.0)
  )
}
legend("bottomright", legend = names(groupCol), col = groupCol, lwd = 2)
BCV_90perc_vec <- round(apply(BCV_mtx, 2, quantile, prob = 0.90), 2)
BCV_50perc_vec <- round(apply(BCV_mtx, 2, quantile, prob = 0.50), 2)
for (JJ in 1:length(BCV_90perc_vec)) {
  rug(BCV_90perc_vec[JJ],
    lwd = 2, ticksize = 0.05,
    col = groupCol[names(BCV_90perc_vec)[JJ]]
  )
}

```
<br/>

BCV values are quite low:

```{r bspBCV,  fig.height=5, fig.width=10, fig.cap="BCV distributions",message=F, echo=F,include=T}

 boxplot(BCV_mtx, outline=F, ylab='BCV')

```

<br/>

<!-- 
 SampleSize_f <- function(Alpha=.01, Beta=.80, Delta=2, Mu, Disp)
 {temp <- qnorm(1-Alpha/2) + qnorm(Beta)
  vtemp <- 1/Mu + Disp
  2*vtemp*(temp/log(Delta))^2}

 cat("SampleSize_f(Mu=10, Disp=.4^2):\n")
 SampleSize_f(Mu=10, Disp=.4^2)
-->

We can now look at sample size estimates to required to detect 
various effect sizes.  The effect sizes examined here are selected
based on the differential representation analysis in Section \@ref(dra) below.


```{r sampleSizeCurve, cache=T, cache.vars='', fig.height=8, fig.width=11,fig.cap='Sample Size Estimates', echo=F}

par(mfrow = c(2, 1), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 0))
for (EFFECT in c(1.10, 1.05)) {
  plot(
    x = 10:100, 
    y = RNASeqPower::rnapower(depth = 10:100, cv = max(BCV_90perc_vec), 
         effect = EFFECT, alpha = .05, power = .80),
    lwd = 2, ylab = "", xlab = "", type='l'
  )
  title(paste("Effect =", EFFECT, "cv =", 
     max(BCV_90perc_vec), "Alpha=0.05, Beta=0.80"))
}
mtext(side = 1, outer = T, "Gene Coverage")
mtext(side = 2, outer = T, "Samples Needed")
mtext(side = 3, outer = T, cex = 1.25, 
    "Negative Binomial 2 Group Sample Size Curves")

```

For the filtered reads, coverage looks like this:

```{r quantCountsAF, echo=F}

featureCountsAF_quant <- apply(featureCountsAF, 2, function(CC) {
  c(quantile(CC, prob = c(.15, (1:3) / 4)))###, totCovM = sum(CC) / 1e6)
})

featureCountsAF_quant2 <- apply(featureCountsAF_quant, 1, function(RR) {
  quantile(RR, prob = (1:3) / 4)
})

knitr::kable(featureCountsAF_quant2,
  digits = 1,
  caption = paste(
    "Coverage Summary - Columns are sample coverage quantiles and total coverage",
    "\nRows are quartiles across samples"
  )
) %>%
  kableExtra::kable_styling(full_width = F)

```


From this table, we see that 25% of the samples have an upper quartile of gene coverage
exceeding `r round(featureCountsAF_quant2["75%", "75%"],1)` reads, 25% of samples
have a 15 percentile of coverage lower than
`r featureCountsAF_quant2["25%", "15%"]`, etc.


Note that with these data, moderate sample sizes are adequate 
to detect genes with effect sizes as small as 1.05 (5% fold change).  This is due to the fact
that the biological variability in gene body 5hmC density is quite low.
For human samples, RNA-Seq within group biological variability is
typically in the 0.4-1.0 range [@Hart:2013aa].  

