# Modeling - Background {#modeling-background}

Refer to [first pass study](https://hcc-5hmc-analysis.netlify.app/) for
relevant exploratory data  analyis results.

<!--
In the section we look at  some models fitted to discriminate between
early stage HCC and healthy and benign samples (grouped as Controls here)
from the GSE112679 data set.  


* Some questions to address with the baseline model 
   - how separable are the data: what accuracy do we expect 
   - individual sample quality scores: which samples are hard to classify?  Compute a score
in [0, 1], where 1 is perfectly good classification and 0 is perfectly bad.

-->

## Predictive modeling for genomic data

The main challenge in calibrating predictive models to genomic data is that
there are many more features than there are example cases to fit to;
the  now classic $n << p$ problem.
In this scenario, fitting methods tend to over fit.  The problem
can be addressed by selecting variables, regularizing the fit or both.
<!--
See the Trevor Hastie talk: 
[Statistical Learning with Big Data - Trevor Hastie](https://web.stanford.edu/~hastie/TALKS/SLBD_new.pdf)
for a good discussion of this problem and potential solutions.
-->



### caret for model evaluation

[The `caret` Package](https://topepo.github.io/caret/index.html)
provide a set of functions that streamline the process for fitting and
evaluating a large number of predictive models in parallel. The package contains tools for:

* data splitting  
* pre-processing  
* feature selection  
* model tuning using re-sampling  
* variable importance estimation  

The tools facilitate the process of automating randomly splitting data sets into training, 
testing and evaluating so that predictive models can be evaluated on a comparable and
exhaustive basis.  Especially useful is the functionality that is provided to
repeatedly randomly stratify samples into train and test set so that any
sample selection bias is removed.  


What makes the `caret` package extremely useful is that it provides a common interface 
to an exhaustive collection of fitting procedures.  Without
this common interface one has to learn the specific syntax that
used in each fitting procedure to be included in a comparative analysis,
which can be quite burdensome.  

Some of the models which can be evaluated with caret include: 
(only some of these can be used with multinomial responses)

* FDA - Flexible Discriminant Analysis  
* stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection  
* stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection  
* knn - k nearest neighbors  
* pam - Nearest shrunken centroids  
* rf - Random forests  
* svmRadial - Support vector machines (RBF kernel)  
* gbm - Boosted trees  
* xgbLinear - eXtreme Gradient Boosting  
* xgbTree - eXtreme Gradient Boosting  
* neuralnet - neural network  

Many more models can be implemented and evaluated with `caret`, 
including some `deep learning` methods, `Simulated Annealing Feature Selection` 
and `Genetic Algorithms`.
Many other methods found [here](https://topepo.github.io/caret/available-models.html)
are also worth investigating.

We only mention `caret` here because it is an extremely useful tool for 
anyone interested in comparing many predictive models.  We have done that
in the past and have found that regularized regression models perform
as well as any in the context of classification based on genomic scale data
and will focus on the particular set of tools for fitting and analyzing
regularized regression models provided by the `glmnet` R package.



## glmnet

In this investigation we will focus on models that can be
analyzed with the the `glmnet` R  package [@Friedman:2010aa].  Several
factors favor this choice:  

* the glmnet package is a well supported package providing
extensive functionality for regularized regression and classification models.

* the hyper-parameters of the elastic net enable us to explore
the relationship between model size, or sparsity, and predictive accuracy.
ie. we can investigate the "bet on sparsity" principle:
*Use a procedure that does well in sparse problems, since no procedure
does well in dense problems*.

* in our experience building classifiers from genomic scale data, regularized
classification models using the elastic net penalty do as well as any other,
and are more economical in terms of computing time, especially in comparison to
the more exotic boosting algorithms.

* the `lasso` has been shown to be near optimal for the $n<<p$ problem
over a wide range of signal-to-noise regiments (Hastie et al. (2017) [@Hastie:2017aa]).


***


Much of the following comes from the 
[Glmnet Vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html).


Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. 
The regularization path is computed for the lasso or elastic net penalty at a 
grid of values for the regularization parameter lambda 
([@Friedman:2010aa;@Tibshirani:2012aa;@Simon:2011aa;@Simon:2013aa]). 

`glmnet` solves the following problem:

$$\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)||\beta||_2^2/2 + \alpha ||\beta||_1\right],$$

over a grid of values of $\lambda$.
Here $l(y,\eta)$ is the negative log-likelihood contribution for observation i; 
e.g. for the Gaussian case it is $\frac{1}{2}(y-\eta)^2$.


###  **alpha** hyper-parameter {-}

The elastic-net penalty is controlled by $\alpha$, and bridges the gap between 
lasso ($\alpha$=1, the default) and ridge ($\alpha$=0). 
The tuning parameter $\lambda$ controls the overall strength of the penalty. 

It is known that the ridge penalty shrinks the coefficients of correlated predictors 
towards each other while the lasso tends to pick one of them and discard the others. 
The elastic-net penalty mixes these two; if predictors are correlated in groups, 
an $\alpha$=0.5 tends to select the groups in or out together. 
This is a higher level parameter, and users might pick a value upfront, 
else experiment with a few different values. One use of $\alpha$ is for numerical stability; 
for example, the *elastic net with $\alpha = 1 - \epsilon$ for some small $\epsilon$>0 
performs much like the lasso, but removes any degeneracies and wild behavior caused 
by extreme correlations*.


### Signal-to-noise Ratio {-}

A key characteristic of classification problems is the 
prevailing signal-to-noise ratio (SNR) of the problem at hand.

To define SNR, let $(x_0, y_0) \in  \mathbb{R}^p \times \mathbb{R}$
be a pair of predcitor and response variables and define
$f(x_0) = \mathbb{E}(y_0|x_0)$ and $\epsilon = y_0 - f(x_0)$ so that

$$y_0 = f(x_0) + \epsilon_0.$$

The signal-to-noise ratio in this model is defined as

$$SNR=\frac{var(f(x_0))}{var(\epsilon_0)}.$$

It is useful to relate the SNR of a model to the proportion of variance explained (PVE).
For a given prediction function g --- eg. one trained on n samples
$(x_i, y_i) i = 1, \dots, n$ that are i.i.d. to $(x_0, y_0)$ --- its
associated proportion of variance explained is defined as

$$PVE(g)=1 - \frac{\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}.$$

This is maximized when we take $g$ to be the mean function $f$ itself,
in which case

$$PVE(f) = 1 - \frac{Var(\epsilon_0)}{Var(y_0)} = \frac{SNR}{1+SNR}.$$

Or equivalently,

$$SNR = \frac{PVE}{1-PVE}.$$

Hastie, Tibshirani, and Tibshirani (2017) [@Hastie:2017aa], point out that
PVE is typically in the 0.2 range, and much lower in financial data.  It
is also much lower in 5hmC data, as we will see in the next section.  

Note that the SNR is a different characterization of noise level than the
coefficient of variation:

$$c_v = \frac{\sigma}{\mu}=\frac{Var(y)}{\mathbb{E}(y)}$$

Note that for small SNR, SNR $\approx$ PVE.


See Xiang et al. (2020) [@Xiang:2020aa], Lozoya et al. (2018) [@Lozoya:2018aa], 
Simonson et al. (2018) [@Simonsen:2018aa] and
Rapaport et al. (2013) [@Rapaport:2013aa] for SNR in RNA-Seq

### Lasso vs Best Subset  {-} 

Best subset selection finds the subset of k predictors that 
produces the best fit in terms of squared error, solving the nonconvex problem:

\begin{equation}
 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_0 \leq k
 (\#eq:best-sub)
\end{equation}

The lasso solves a covex relaxation of \@ref(eq:best-sub) where we replace the 
$l_0$ norm by the $l_1$ norm, namely

\begin{equation}

 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_1 \leq t

 (\#eq:lasso)
\end{equation}

where $||\beta||_1 = \sum_{i=1}^{p} |\beta_i|$, and $t \geq 0$ is a tuning parameter.


Bertsimas et al. (2016) [@Bertsimas:2016aa] presented a mixed integer optimization (MIO) 
formulation for the best subset selection problem.  Using these MIO solvers, 
one can solve problems with p in the hundreds and even thousands.
Bertsimas et al. showed evidence that
best subset selection generally gives superior prediction accuracy compared 
to forward stepwise selection and the lasso, over a variety of problem setups. 

In Hastie et al. (2017) [@Hastie:2017aa], the authors countered by arguing that
neither best subset selection nor the lasso uniformly dominate the other, 
with best subset selection generally performing better in high signal-to-noise (SNR) 
ratio regimes, and **the lasso better in low SNR regimes**.
Best subset selection and forward stepwise perform quite similarly over
a range of SNR contexts, but the relaxed lasso is the overall best option, 
performing just about as well as the lasso in low SNR scenarios, 
and as well as best subset selection in high SNR scenarios. 
Hastie et al. conclude that a blended mix of lasso and relaxed lasso estimator,
the *shrunken relaxed lasso fit*, is able to use its auxiliary shrinkage 
parameter ($\gamma$) to get the “best of both worlds”: 
it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not.
   

<!--
* relaxed lasso

$$\hat{\beta}^{relax}(\lambda, \gamma) = \gamma \beta^{lasso}(\lambda) + (1 - \gamma)(\beta^{LS}(\lambda)$$

* shrunken relaxed lasso (aka the blended fit)
-->

Suppose the **glmnet** fitted linear predictor at $\lambda$ is $\hat{\eta}_\lambda(x)$
and the relaxed version is $\tilde{\eta}_\lambda(x)$, then the shrunken relaxed lasso fit is

\begin{equation}

\tilde{\eta}_{\lambda,\gamma}(x)=(1-\gamma)\tilde{\eta}_\lambda(x) + \gamma \hat{\eta}_\lambda(x)

 (\#eq:blended)
\end{equation}

$\gamma \in [0,\, 1]$ is an additional tuning parameter which can be selected by cross validation.

The de-biasing will potentially improve prediction performance, and 
cross-validation will typically select a model with a smaller number of variables. 
This procedure is very competitive with forward-stepwise and 
best-subset regression, and has a considerable speed advantage when the 
number of variables is large.  This is especially true for best-subset, 
but even so for forward stepwise.  The latter has to plod through the 
variables one-at-a-time, while glmnet will just plunge in and find a good active set.

Further details may be found in 
Friedman, Hastie, and Tibshirani (2010),
Tibshirani et al. (2012),
Simon et al. (2011),
Simon, Friedman, and Hastie (2013) and 
Hastie, Tibshirani, and Tibshirani (2017)
([@Friedman:2010aa;@Tibshirani:2012aa;@Simon:2011aa;@Simon:2013aa;@Hastie:2017aa]). 
