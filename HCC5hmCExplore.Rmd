--- 
title: "DNA Hydroxymethylation in Hepatocellular Carcinoma"
author: "Francois Collin"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
### see https://community.rstudio.com/t/bookdown-pdf-generation/12359
knit: "bookdown::render_book"  
documentclass: book
bibliography: [bib/HCC5hmc.bib]
#biblio-style: apalike
csl: csl/cell-numeric.csl
#csl: csl/american-medical-association-alphabetical.csl
link-citations: yes
description: "Data from Cai et al. (2019) paper are explored"
---

<!-- ONLY THIS FILE SHOULD HAVE YAML -->

<!-- THIS FILE DOESN'T HAVE TO HAVE ANY CONTENT ... -->
 

<style>

.watermark {
  opacity: 0.2;
  position: fixed;
  top: 50%;
  left: 50%;
  font-size: 500%;
  color: #00407d;
}

</style>

<!-- THIS DIDN'T DO ANYTHING
<div class="watermark">DRAFT</div>
-->

```{r setup, include=F}
   # file rmarkdown file management options: cache, figures
 figures_DIR <- file.path('Static', 'figures/')
 suppressMessages(dir.create(figures_DIR, recursive=T))
 knitr::opts_chunk$set(fig.path=paste0(figures_DIR))
 

KellyColors.vec <- c(
  "#222222", "#F3C300", "#875692", "#F38400", "#A1CAF1",
  "#BE0032", "#C2B280", "#848482", "#008856", "#E68FAC", "#0067A5",
  "#F99379", "#604E97", "#F6A600", "#B3446C", "#DCD300", "#882D17",
  "#8DB600", "#654522", "#E25822", "#2B3D26"
)
col_vec <- KellyColors.vec


 # This is for kableExtra::kable_styling to work
 # specify for html
 options(knitr.table.format = 'html')

 # specify for pdf
 #options(knitr.table.format = 'latex')
  

```


<!--chapter:end:index.Rmd-->

# Preamble {.unnumbered #index} 


This vignette offers some exploratory data analyses of 
DNA Hydroxymethylation data available from 
NCBI GEO [Series GSE112679](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE112679).
These data can be conveniently accessed through an R data package.
See [GSE112679 R Data Package page](https://12379monty.github.io/GSE112679/).

## License {-}

<!-- From https://github.com/santisoler/cc-licenses -->
<!-- THis doesnt work with pdf -->
<!-- COMMENT OUT FOR bookdown::pdf_book ????
![](CC_4_0.png)
![](https://i.creativecommons.org/l/by/4.0/88x31.png)

-->

`r knitr::include_graphics(
  "Static/images/CC_4_0.png",  dpi=100)`


This work by Francois Collin is licensed under a
[Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)



# Introduction {#intro}


The goal of detecting
cancer at the earliest stage of development with a non-invasive procedure
has busied many groups with the task of perfecting techniques to support
what has become commonly known as a 
liquid biopsy - the analysis of biomarkers circulating in fluids such as blood,
saliva or urine.  Epigenetic biomarkers present themselves as good candidates for this application
(Gai and Sun (2019) [@Gai:2019aa]).  In particular,
given their prevalence in the human genome, 
close correlation with gene expression and high chemical stability,
DNA modifications such as 5-methylcytosine (5mC) and 5-hydroxymethylcytosine (5hmC)
are DNA epigenetic marks that provide much promise as
cancer diagnosis biomarkers that could be profitably analyzed in liquid biopsies
[@Cai:2019aa; @Li:2017aa; @Song:2017aa; @Collin:2018aa].


<!--
This work has already led to some commercial products.
Thrive Earlier Detection Corp.  launched CancerSEEK, a liquid biopsy test to detect multiple cancers early in 2019.  Guardant Health offers a liquid biopsy test, Guardant360, for advanced solid 
tumor cancers. Others in the space include Karius, which focuses on a liquid biopsy for 
infectious disease, and GRAIL Bio, which was launched by Illumina in January 2016. 
In 2017, Verily Life Sciences, one of Google/Alphabet’s companies, invested in Freenome, 
another liquid biopsy company.
-->
Li et al. (2017) [@Li:2017aa] used a sensitive and selective chemical labeling technology
to extract genome-wide 5hmC profiles from circulating cell-free DNA (cfDNA) 
as well as from genomic DNA (gDNA) 
collected from a cohort of 260 patients recently diagnosed with colorectal, 
gastric, pancreatic, liver or thyroid cancer and normal tissues from 90 healthy individuals
They found 5hmC-based biomarkers of circulating cfDNA to be highly predictive of some cancer types.
Similar small sample size findings were reported in Song et al. (2017) [@Song:2017aa].  

Focusing on hepatocellular carcinoma, Cai et al. (2019) [@Cai:2019aa] assembled a sizable dataset
to demonstrate the feasibility of using features derived from 
5-hydroxymethylcytosines marks in circulating cell-free DNA as 
a non-invasive approach for the early detection of
hepatocellular carcinoma.  The data that are the basis of that
report are available on the NCBI GEO web site 
([Series GSE112679](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE112679)).
The data have also been bundled in a R data package which can be installed from github:

```
if (!requireNamespace("devtools", quietly = TRUE))
    install.packages("devtools")
devtools::install_github("12379Monty/GSE112679")
```

An important question in the early development of classifiers of the sorts
that are the basis of any liquid biopsy diagnostic tool is how many samples
should be collected to make properly informed decisions.  In this
report we will explore the GSE112679 data to shed some light on
the relationship between sample size and model performance
in the context classifying samples based on 5hmC data.

The layout of the paper is the following:  

* In Section \@ref(modeling-background) we provide some background on modeling
genomic data, ie. $n << p$ data.

* In Section \@ref(preproc) we preprocess the 5hmC data that
we will use for the classification analysis and perform some light QC analyses.  

* In Section \@ref(explore-sparsity) we explore some glmnet fits
that discriminate between early stage HCC and control samples.  

* In Section \@ref(model-suite) we examine the results of fitting a suite of models to
investigate the effect of sample size on model performance.  

* In Section \@ref(variable-importance) look at the question of assessing variable importance.

* Concluding remarks are in Section \@ref(conclusions).


<!--chapter:end:00-intro.Rmd-->

# Modeling - Background {#modeling-background}

Refer to [first pass study](https://hcc-5hmc-analysis.netlify.app/) for
relevant exploratory data  analyis results.

<!--
In the section we look at  some models fitted to discriminate between
early stage HCC and healthy and benign samples (grouped as Controls here)
from the GSE112679 data set.  


* Some questions to address with the baseline model 
   - how separable are the data: what accuracy do we expect 
   - individual sample quality scores: which samples are hard to classify?  Compute a score
in [0, 1], where 1 is perfectly good classification and 0 is perfectly bad.

-->

## Predictive modeling for genomic data

The main challenge in calibrating predictive models to genomic data is that
there are many more features than there are example cases to fit to;
the  now classic $n << p$ problem.
In this scenario, fitting methods tend to over fit.  The problem
can be addressed by selecting variables, regularizing the fit or both.
<!--
See the Trevor Hastie talk: 
[Statistical Learning with Big Data - Trevor Hastie](https://web.stanford.edu/~hastie/TALKS/SLBD_new.pdf)
for a good discussion of this problem and potential solutions.
-->



### caret for model evaluation

[The `caret` Package](https://topepo.github.io/caret/index.html)
provide a set of functions that streamline the process for fitting and
evaluating a large number of predictive models in parallel. The package contains tools for:

* data splitting  
* pre-processing  
* feature selection  
* model tuning using re-sampling  
* variable importance estimation  

The tools facilitate the process of automating randomly splitting data sets into training, 
testing and evaluating so that predictive models can be evaluated on a comparable and
exhaustive basis.  Especially useful is the functionality that is provided to
repeatedly randomly stratify samples into train and test set so that any
sample selection bias is removed.  


What makes the `caret` package extremely useful is that it provides a common interface 
to an exhaustive collection of fitting procedures.  Without
this common interface one has to learn the specific syntax that
used in each fitting procedure to be included in a comparative analysis,
which can be quite burdensome.  

Some of the models which can be evaluated with caret include: 
(only some of these can be used with multinomial responses)

* FDA - Flexible Discriminant Analysis  
* stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection  
* stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection  
* knn - k nearest neighbors  
* pam - Nearest shrunken centroids  
* rf - Random forests  
* svmRadial - Support vector machines (RBF kernel)  
* gbm - Boosted trees  
* xgbLinear - eXtreme Gradient Boosting  
* xgbTree - eXtreme Gradient Boosting  
* neuralnet - neural network  

Many more models can be implemented and evaluated with `caret`, 
including some `deep learning` methods, `Simulated Annealing Feature Selection` 
and `Genetic Algorithms`.
Many other methods found [here](https://topepo.github.io/caret/available-models.html)
are also worth investigating.

We only mention `caret` here because it is an extremely useful tool for 
anyone interested in comparing many predictive models.  We have done that
in the past and have found that regularized regression models perform
as well as any in the context of classification based on genomic scale data
and will focus on the particular set of tools for fitting and analyzing
regularized regression models provided by the `glmnet` R package.



## glmnet

In this investigation we will focus on models that can be
analyzed with the the `glmnet` R  package [@Friedman:2010aa].  Several
factors favor this choice:  

* the glmnet package is a well supported package providing
extensive functionality for regularized regression and classification models.

* the hyper-parameters of the elastic net enable us to explore
the relationship between model size, or sparsity, and predictive accuracy.
ie. we can investigate the "bet on sparsity" principle:
*Use a procedure that does well in sparse problems, since no procedure
does well in dense problems*.

* in our experience building classifiers from genomic scale data, regularized
classification models using the elastic net penalty do as well as any other,
and are more economical in terms of computing time, especially in comparison to
the more exotic boosting algorithms.

* the `lasso` has been shown to be near optimal for the $n<<p$ problem
over a wide range of signal-to-noise regiments (Hastie et al. (2017) [@Hastie:2017aa]).


***


Much of the following comes from the 
[Glmnet Vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html).


Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. 
The regularization path is computed for the lasso or elastic net penalty at a 
grid of values for the regularization parameter lambda 
([@Friedman:2010aa;@Tibshirani:2012aa;@Simon:2011aa;@Simon:2013aa]). 

`glmnet` solves the following problem:

$$\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)||\beta||_2^2/2 + \alpha ||\beta||_1\right],$$

over a grid of values of $\lambda$.
Here $l(y,\eta)$ is the negative log-likelihood contribution for observation i; 
e.g. for the Gaussian case it is $\frac{1}{2}(y-\eta)^2$.


###  **alpha** hyper-parameter {-}

The elastic-net penalty is controlled by $\alpha$, and bridges the gap between 
lasso ($\alpha$=1, the default) and ridge ($\alpha$=0). 
The tuning parameter $\lambda$ controls the overall strength of the penalty. 

It is known that the ridge penalty shrinks the coefficients of correlated predictors 
towards each other while the lasso tends to pick one of them and discard the others. 
The elastic-net penalty mixes these two; if predictors are correlated in groups, 
an $\alpha$=0.5 tends to select the groups in or out together. 
This is a higher level parameter, and users might pick a value upfront, 
else experiment with a few different values. One use of $\alpha$ is for numerical stability; 
for example, the *elastic net with $\alpha = 1 - \epsilon$ for some small $\epsilon$>0 
performs much like the lasso, but removes any degeneracies and wild behavior caused 
by extreme correlations*.


### Signal-to-noise Ratio {-}

A key characteristic of classification problems is the 
prevailing signal-to-noise ratio (SNR) of the problem at hand.

To define SNR, let $(x_0, y_0) \in  \mathbb{R}^p \times \mathbb{R}$
be a pair of predcitor and response variables and define
$f(x_0) = \mathbb{E}(y_0|x_0)$ and $\epsilon = y_0 - f(x_0)$ so that

$$y_0 = f(x_0) + \epsilon_0.$$

The signal-to-noise ratio in this model is defined as

$$SNR=\frac{var(f(x_0))}{var(\epsilon_0)}.$$

It is useful to relate the SNR of a model to the proportion of variance explained (PVE).
For a given prediction function g --- eg. one trained on n samples
$(x_i, y_i) i = 1, \dots, n$ that are i.i.d. to $(x_0, y_0)$ --- its
associated proportion of variance explained is defined as

$$PVE(g)=1 - \frac{\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}.$$

This is maximized when we take $g$ to be the mean function $f$ itself,
in which case

$$PVE(f) = 1 - \frac{Var(\epsilon_0)}{Var(y_0)} = \frac{SNR}{1+SNR}.$$

Or equivalently,

$$SNR = \frac{PVE}{1-PVE}.$$

Hastie, Tibshirani, and Tibshirani (2017) [@Hastie:2017aa], point out that
PVE is typically in the 0.2 range, and much lower in financial data.  It
is also much lower in 5hmC data, as we will see in the next section.  

Note that the SNR is a different characterization of noise level than the
coefficient of variation:

$$c_v = \frac{\sigma}{\mu}=\frac{Var(y)}{\mathbb{E}(y)}$$

Note that for small SNR, SNR $\approx$ PVE.


See Xiang et al. (2020) [@Xiang:2020aa], Lozoya et al. (2018) [@Lozoya:2018aa], 
Simonson et al. (2018) [@Simonsen:2018aa] and
Rapaport et al. (2013) [@Rapaport:2013aa] for SNR in RNA-Seq

### Lasso vs Best Subset  {-} 

Best subset selection finds the subset of k predictors that 
produces the best fit in terms of squared error, solving the nonconvex problem:

\begin{equation}
 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_0 \leq k
 (\#eq:best-sub)
\end{equation}

The lasso solves a covex relaxation of \@ref(eq:best-sub) where we replace the 
$l_0$ norm by the $l_1$ norm, namely

\begin{equation}

 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_1 \leq t

 (\#eq:lasso)
\end{equation}

where $||\beta||_1 = \sum_{i=1}^{p} |\beta_i|$, and $t \geq 0$ is a tuning parameter.


Bertsimas et al. (2016) [@Bertsimas:2016aa] presented a mixed integer optimization (MIO) 
formulation for the best subset selection problem.  Using these MIO solvers, 
one can solve problems with p in the hundreds and even thousands.
Bertsimas et al. showed evidence that
best subset selection generally gives superior prediction accuracy compared 
to forward stepwise selection and the lasso, over a variety of problem setups. 

In Hastie et al. (2017) [@Hastie:2017aa], the authors countered by arguing that
neither best subset selection nor the lasso uniformly dominate the other, 
with best subset selection generally performing better in high signal-to-noise (SNR) 
ratio regimes, and **the lasso better in low SNR regimes**.
Best subset selection and forward stepwise perform quite similarly over
a range of SNR contexts, but the relaxed lasso is the overall best option, 
performing just about as well as the lasso in low SNR scenarios, 
and as well as best subset selection in high SNR scenarios. 
Hastie et al. conclude that a blended mix of lasso and relaxed lasso estimator,
the *shrunken relaxed lasso fit*, is able to use its auxiliary shrinkage 
parameter ($\gamma$) to get the “best of both worlds”: 
it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not.
   

<!--
* relaxed lasso

$$\hat{\beta}^{relax}(\lambda, \gamma) = \gamma \beta^{lasso}(\lambda) + (1 - \gamma)(\beta^{LS}(\lambda)$$

* shrunken relaxed lasso (aka the blended fit)
-->

Suppose the **glmnet** fitted linear predictor at $\lambda$ is $\hat{\eta}_\lambda(x)$
and the relaxed version is $\tilde{\eta}_\lambda(x)$, then the shrunken relaxed lasso fit is

\begin{equation}

\tilde{\eta}_{\lambda,\gamma}(x)=(1-\gamma)\tilde{\eta}_\lambda(x) + \gamma \hat{\eta}_\lambda(x)

 (\#eq:blended)
\end{equation}

$\gamma \in [0,\, 1]$ is an additional tuning parameter which can be selected by cross validation.

The de-biasing will potentially improve prediction performance, and 
cross-validation will typically select a model with a smaller number of variables. 
This procedure is very competitive with forward-stepwise and 
best-subset regression, and has a considerable speed advantage when the 
number of variables is large.  This is especially true for best-subset, 
but even so for forward stepwise.  The latter has to plod through the 
variables one-at-a-time, while glmnet will just plunge in and find a good active set.

Further details may be found in 
Friedman, Hastie, and Tibshirani (2010),
Tibshirani et al. (2012),
Simon et al. (2011),
Simon, Friedman, and Hastie (2013) and 
Hastie, Tibshirani, and Tibshirani (2017)
([@Friedman:2010aa;@Tibshirani:2012aa;@Simon:2011aa;@Simon:2013aa;@Hastie:2017aa]). 

<!--chapter:end:00-modelingBackground.Rmd-->

# Preprocessing {#preproc}

<!--
 FN <- 'tmp'
 # Shotcuts for knitting and redering while in R session (Invoke interactive R from R/Scripts folder)
 kk <- function(n='') knitr::knit2html(paste("t", n, sep=''), envir=globalenv(),
       output=paste(FN,".html", sep=''))

 rr <- function(n='') rmarkdown::render(paste("t", n, sep=''), envir=globalenv(),
       output_file=paste(FN,".html", sep='')) ##, output_dir='Scripts')

 bb <- function(n='') browseURL(paste(FN,".html", sep=''))

 # The usual shotcuts
 zz <- function(n='') source(paste("t", n, sep=''))
-->



```{r setup-preproc, include=F}
   # file rmarkdown file management options: cache, figures
 figures_DIR <- file.path('Static', 'figures/')
 suppressMessages(dir.create(figures_DIR, recursive=T))
 knitr::opts_chunk$set(fig.path=paste0(figures_DIR))
```

## Load the data

<!-- THIS ENSURES NO EVALUATION TAKES PLACE BY DEFAULT -->
<!-- TO TURN ON, SET eval=T                            -->
```{r chunk-options, include=FALSE, eval=F}
library("knitr")
opts_chunk$set(eval = FALSE)
```

<!-- Add base libraries -->
```{r libraries, include=FALSE, eval=T}
library("magrittr")
```


The data that are available from NCBI GEO
[Series GSE112679](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE112679)
can be conveniently accessed through an R data package.
Attaching the GSE112679 package makes the count data tables 
available as well as a gene annotation table and a sample description table.
See [GSE112679 R Data Package page](https://12379monty.github.io/GSE112679/).
In the Cai et al. [@Cai:2019aa] paper, samples were separated into
`Train` and `Val-1` subsets for model fitting and analysis.
`Val-2` was used as an external validation set.

```{r loadData, cache=F}

if (!("GSE112679" %in% rownames(installed.packages()))) {
  if (!requireNamespace("devtools", quietly = TRUE)) {
    install.packages("devtools")
  }
  devtools::install_github("12379Monty/GSE112679")
}
library(GSE112679)
sampDesc$DxStage <- with(sampDesc, ifelse(outcome=='HCC', 
   paste0(outcome,':',stage), outcome))

with(
  sampDesc %>% dplyr::filter(sampType == "blood"),
  knitr::kable(table(DxStage, trainValGroup, exclude = NULL),
   caption="GSE112679 Samples by Dx Group and Subset") %>% 
  kableExtra::kable_styling(full_width = F)
)

```

For this analysis, we will consider early stage cancer samples
and healthy or benign samples from the `Train` or `Val-1` subsets.
The appropriate outcome variable will be renamed or aliased `group`

```{r subsetSamples, cache=T, cache.vars=c('sampDescA','groupCol'), echo=T}

# Use suffix 'A' for Analysis samples
sampDescA <-
  sampDesc %>%
  dplyr::filter(sampType == "blood" &
    (trainValGroup %in% c("Train", "Val-1")) &
    ((outcome2 == "BenignHealthy") |
      (outcome2 == "HCC" & stage == "Early"))) %>%
  dplyr::rename(group = outcome2) %>%
  dplyr::arrange(group, sampID)
# Recode group
sampDescA$group <- with(
  sampDescA,
  ifelse(group == "BenignHealthy", "Control", group)
)
# set groupCol for later
groupCol <- c("#F3C300", "#875692")
names(groupCol) <- unique(sampDescA$group)

with(sampDescA, 
 knitr::kable(table(group, exclude = NULL),
  caption="Samples used in this analysis") %>%
  kableExtra::kable_styling(full_width = F)
)

```


The features are counts of reads captured by chemical labeling, and indicate
the level of 5-hydroxymethylcytosines within each gene body.  Cai et al. (2019),
Li et al. (2017) and Song et al. (2017) [@Cai:2019aa;@Li:2017aa;@Song:2017aa]
all analyze 5hmC gene body counts using standard RNA-Seq methodologies, and we will
do the same here.  

Note that before conducting any substantive analyses, the data would normally
be very carefully examined for any sign of quality variation between groups
of samples.  This analysis would integrate sample meta data - where and when were
the blood samples collected - as well as library preparation and sequencing metrics
in order to detect any sign of processing artifacts that may be present in the dataset.
This is particularly important when dealing with blood samples as variable
DNA quality degradation is a well known challenge that is encountered when dealing with
such samples [@Huang:2017aa].  Although blood specimen handling protocols can be 
put in place to minimize quality variation [@Permenter:2015aa], variability
can never be completely eradicated, especially in the context of blood samples
collected by different groups, working in different environments.  The problem
of variable DNA quality becomes paricularly pernicuous when it is compounded
with a confounding factor that sneaks in when the control sample collection
events are separated in time and space from the cancer sample collection events;
an all too common occurence.  

As proper data QC requires an intimate familiarity with the details of
data collection and processing, such a task cannot be untertaken here.
We will simply run a *minimal set of QC sanity checks* to make sure that
there are no apparent systematic effects in the data.


```{r getfeatures, cache=T, cache.vars=c('featureCountsA'), echo=T}

featureCountsA <- cbind(
  Train_featureCount,
  Val1_featureCount,
  Val2_featureCount
)[, rownames(sampDescA)]

```

We first look at coverage - make sure there isn't too much disparity of coverage 
across samples. To detect shared variability, samples can be annotated and ordered
according to sample features that may be linked to sample batch processing.  Here we 
the samples have been ordered by group and sample id (an alias of geoAcc).

```{r lcpmBxp, cache=T, fig.height=4, fig.width=10, fig.cap='Sample log2 count boxplots', echo=T}

par(mar = c(1, 3, 2, 1))
boxplot(log2(featureCountsA + 1),
  ylim = c(3, 11), ylab='log2 Count',
  staplewex = 0,       # remove horizontal whisker lines
  staplecol = "white", # just to be totally sure :)
  outline = F,         # remove outlying points
  whisklty = 0,        # remove vertical whisker lines
  las = 2, horizontal = F, xaxt = "n",
  border = groupCol[sampDescA$group]
)
legend("top", legend = names(groupCol), text.col = groupCol, 
  ncol = 2, bty = "n")
# Add reference lines
SampleMedian <- apply(log2(featureCountsA + 1), 2, median)
abline(h = median(SampleMedian), col = "grey")
axis(side = 4, at = round(median(SampleMedian), 1), 
  las = 2, col = "grey", line = -1, tick = F)

```

<!--
Coverage level looks fairly comparable across samples.  It is sometimes helpful to
keep track of the actual coverage which can be adequetely tracked by distribution
quantiles.
-->

```{r quantCountsA, echo=T}

featureCountsA_quant <- apply(featureCountsA, 2, function(CC) {
  c(quantile(CC, prob = c(.15, (1:3) / 4)), totCovM = sum(CC) / 1e6)
})

featureCountsA_quant2 <- apply(featureCountsA_quant, 1, function(RR) {
  quantile(RR, prob = (1:3) / 4)
})

knitr::kable(featureCountsA_quant2,
  digits = 1,
  caption = paste(
    "Coverage Summary - Columns are sample coverage quantiles and total coverage",
    "\nRows are quartiles across samples"
  )
) %>% kableExtra::kable_styling(full_width = F)

```


From this table, we see that 25% of the samples have total coverage exceeding
$`r round(featureCountsA_quant2["75%", "totCovM"],1)`$M reads, 25% of samples
have a 15 percentile of coverage lower than
$`r featureCountsA_quant2["25%", "15%"]`$, etc.  


<!-- SKIP
We next look at relative log representation (RLR) (in the context of measuring the density of 
5hmC marks in genes, we refer to `representation` as opposed to `expression`; the
two can be used interchangibly) -
make sure the shapes of the distributions are not widely different.
-->

```{r rlr, cache=T, cache.vars='lcpm_mtx', fig.height=4, fig.width=10, fig.cap='Sample RLR', eval=T, echo=T,include=F}

lcpm_mtx <- edgeR::cpm(featureCountsA, log = T)
median_vec <- apply(lcpm_mtx, 1, median)
RLR_mtx <- sweep(lcpm_mtx, 1, median_vec, "-")

par(mar = c(1, 3, 2, 1))
boxplot(RLR_mtx,
  xlab = "", ylab='Relative Log Representation', ylim = c(-.6, .6),
  staplewex = 0, # remove horizontal whisker lines
  staplecol = "white", # just to be totally sure :)
  outline = F, # remove outlying points
  whisklty = 0, # remove vertical whisker lines
  las = 2, horizontal = F, xaxt = "n",
  border = groupCol[sampDescA$group]
)
legend("top", legend = names(groupCol), 
  text.col = groupCol, ncol = 2, bty = "n")
# Add group Q1, Q3
for (GRP in unique(sampDescA$group)) {
  group_ndx <- which(sampDescA$group == GRP)
  group_Q1Q3_mtx <- apply(RLR_mtx[, group_ndx], 2, 
     quantile, prob = c(.25, .75))
  abline(h = apply(group_Q1Q3_mtx, 1, median), 
     col = groupCol[GRP], lwd = 2)
}



```

<!-- SKIPPED
We note that the HCC samples have slightly more variable coverage distribution.
A few samples are quite different.
-->

## Differential representation analysis {#dra}


In the remainder of this section, we will process the data and
perform differential expression analysis as outlined in 
Law et al. (2018) [@Law:2018aa].   The main analysis steps are: 

* remove lowly expressed genes
* normalize gene expression distributions
* remove heteroscedascity
* fit linear models and examine DE results


It is good practice to perform this differential expression analysis prior to 
fitting models to get an idea of how difficult it will be to discriminate 
between samples belonging to the different subgroups. The pipeline
outlined in Law et al. (2018) [@Law:2018aa] also provides some
basic quality assessment opportunities.


### Remove lowly expressed genes {-}

Genes that are not expressed at a biologically 
meaningful level in any condition should be discarded to reduce the 
subset of genes to those that are of interest, and to reduce the number of tests 
carried out downstream when looking at differential expression.  Carrying
un-informative genes may also be a hindrance to classification and other
downtream analyses.  

To determine a sensible threshold we can begin by examining the shapes of the distributions.

```{r densityLcpm, fig.height=4, fig.width=10, fig.cap='Sample $log_2$ CPM densities', eval=T, echo=T}

par(mar = c(4, 3, 2, 1))
plot(density(lcpm_mtx[, 1]),
  col = groupCol[sampDescA$group[1]],
  lwd = 2, ylim = c(0, .25), las = 2, main = "", xlab = "log2 CPM"
)
abline(v = 0, col = 3)
# After verifying no outliers, can plot a random subset 
for (JJ in sample(2:ncol(lcpm_mtx), size = 100)) {
  den <- density(lcpm_mtx[, JJ])
  lines(den$x, den$y, col = groupCol[sampDescA$group[JJ]], lwd = 2)
} # for(JJ
legend("topright", legend = names(groupCol), 
  text.col = groupCol, bty = "n")

```

$`r  LibSizeSum <- summary( colSums(featureCountsA) / 1e6 )`$
$`r CPM_THR <- 3; SAMP_THR <- 25`$ 

As is typically the case with RNA-Seq data, we notice many weakly represented genes 
in this dataset.  A cpm value of 1 appears to adequatly separate 
the expressed from the un-expressed genes, but we will be slightly more strict here
and require a CPM threshold of $`r CPM_THR`$ .  Using a nominal CPM value of 
$`r CPM_THR`$, genes are deeemed to be `represented` if their expression is 
above this threshold, and not represented otherwise. 
For this analysis we will require that genes be `represented` in at least 
$`r SAMP_THR`$ samples across the entire dataset to be retained for downstream analysis.
Here, a CPM value of $`r CPM_THR`$ means that a gene is represented if it 
has at least $`r round(CPM_THR*LibSizeSum['Min.'])`$ reads in the sample with the 
lowest sequencing depth (library size $`r round(LibSizeSum['Min.'],1)`$ million).
Note that the thresholds used here are arbitrary as there are no hard and fast 
rules to set these by.
The voom-plot, which is part of analyses done to remove heteroscedasticity,
can be examined to verify that the filtering performed is adequate.


<!-- or at least 
$`r round(CPM_THR*LibSizeSum['Max.'])`$ counts in the sample with the 
greatest sequencing depth (library size $`r round(LibSizeSum['Max.'],1)`$ million).
-->

Remove weakly represented genes and replot densities.

$`r weak_flg <- rowSums(edgeR::cpm(featureCountsA) > CPM_THR) < SAMP_THR `$
Removing $`r round(100 * mean(weak_flg), 1)`$%  of genes...

```{r removeWeak, cache=T, cache.vars=c('featureCountsAF', 'genes_annotAF', 'lcpm_mtx'), echo=T, include=F}

# Use suffix 'F' for Filtered genes
featureCountsAF <- featureCountsA[!weak_flg, ]

genes.ndx <- match(rownames(featureCountsAF), genes_annot$Symbol)
if(sum(is.na(genes.ndx))) stop("featureCountsA/genes_annot mismatch")
genes_annotAF <- genes_annot[genes.ndx,]

lcpm_mtx <- edgeR::cpm(featureCountsAF, log = T)
dim(lcpm_mtx)

rm(featureCountsA)

```


```{r densityLcpm2, fig.height=4, fig.width=10, fig.cap='Sample $log_2$ CPM densities after removing weak genes', eval=T, echo=T}

par(mar = c(4, 3, 2, 1))
plot(density(lcpm_mtx[, 1]),
  col = groupCol[sampDescA$group[1]],
  lwd = 2, ylim = c(0, .25), las = 2, main = "", xlab = "log2 CPM"
)
#abline(v = 0, col = 3)
# After verifying no outliers, can plot a random subset 
for (JJ in sample(2:ncol(lcpm_mtx), size = 100)) {
  den <- density(lcpm_mtx[, JJ])
  lines(den$x, den$y, col = groupCol[sampDescA$group[JJ]], lwd = 2)
} # for(JJ
legend("topright", legend = names(groupCol), 
  text.col = groupCol, bty = "n")

```

<!--
Note that the $log_2(CMP)$ distribution is not quite symmetric.
-->

As another sanity check, we will look at a 
multidimensional scaling plot of distances between gene expression
profiles.  We use `plotMDS` in limma package [@Ritchie:2015aa]),
which plots samples on a two-dimensional scatterplot so that distances on
the plot approximate the typical log2 fold changes between the
samples.   

Before producing the MDS plot we will normalize the distributions.
We will store the data into s `DGEList` object as this is convenient
when running many of the analyses implemented in the edgeR and limma packages.
Call the set 'AF', for set 'A', 'Filtered'.

```{r getDGEL, cache=T, cache.var=c('AF_dgel', 'AF_lcmp_mtx')}

AF_dgel <- edgeR::DGEList(
  counts = featureCountsAF,
  genes = genes_annotAF,
  samples = sampDescA,
  group = sampDescA$group
)
AF_dgel <- edgeR::calcNormFactors(AF_dgel)
AF_lcmp_mtx <- edgeR::cpm(AF_dgel, log = T)

# Save AF_dgel to facilitate restarting
# remove from final version
save(list = "AF_dgel", file = "RData/AF_dgel")
```

Verify that the counts are properly normalized.


```{r normedLcpmBxp, cache=T, fig.height=4, fig.width=10, fig.cap='Sample log2 count boxplots', echo=T}

par(mar = c(1, 3, 2, 1))
boxplot(AF_lcmp_mtx,
  ylim = c(1, 8), ylab='Normalized Log CPM',
  staplewex = 0,       # remove horizontal whisker lines
  staplecol = "white", # just to be totally sure :)
  outline = F,         # remove outlying points
  whisklty = 0,        # remove vertical whisker lines
  las = 2, horizontal = F, xaxt = "n",
  border = groupCol[sampDescA$group]
)
legend("top", legend = names(groupCol), text.col = groupCol,
  ncol = 2, bty = "n")
# Add reference lines
SampleMedian <- apply(AF_lcmp_mtx, 2, median)
abline(h = median(SampleMedian), col = "grey")
axis(side = 4, at = round(median(SampleMedian), 1),
  las = 2, col = "grey", line = -1, tick = F)

```

Proceed with MDS plots.

```{r plotMDS, cache=T, fig.height=5, fig.width=10, fig.cap='MDS plots of log-CPM values', echo=T}

par(mfcol = c(1, 2), mar = c(4, 4, 2, 1), xpd = NA, oma = c(0, 0, 2, 0))

# wo loss of generality, sample 500 samples
# simply a matter of convenience to save time
# remove from final version
set.seed(1)
samp_ndx <- sample(1:ncol(AF_lcmp_mtx), size = 500)
MDS.out <- limma::plotMDS(AF_lcmp_mtx[, samp_ndx],
  col = groupCol[sampDescA$group[samp_ndx]], pch = 1
)
legend("topleft",
  legend = names(groupCol),
  text.col = groupCol, bty = "n"
)

MDS.out <- limma::plotMDS(AF_lcmp_mtx[, samp_ndx],
  col = groupCol[sampDescA$group[samp_ndx]], pch = 1,
  dim.plot = 3:4
)
```

The MDS plot, which is analogous to a PCA plot adapted to gene exression data,
does not indicate strong clustering of samples.  The fanning pattern observed in the
first two dimensions indicates that a few samples are drifting way from the
core set, but in no particular direction.  There is some structure in the
3rd and 4th dimension plot which should be investigated.  
`glMDSPlot` from package `Glimma` provides an interactive MDS 
plot that can extremely usful for exploration

```{r GlMDSplot, echo=T,cache=T, cache.vars='', fig.height=6, fig.width=11,fig.cap="MDS plots of log-CPM values", echo=T}

Glimma::glMDSPlot(AF_dgel[, samp_ndx],
  groups = AF_dgel$samples[
    samp_ndx,
    c("group", "trainValGroup", "sampType", "tissue", "title", "stage")
  ],
  main = paste("MDS plot: filtered counts"), #### , Excluding outlier samples"),
  path = ".", folder = figures_DIR,
  html = paste0("GlMDSplot"), launch = F
)
```

Link to glMDSPlot: 
[Here]($`r file.path(figures_DIR, paste0("GlMDSplot.html"))`$)  

No obvious factor links the samples in the 3 clusters observed on the
4th MDS dimensions. The percent of variance exaplained by this dimension or 
$\approx$ 4%.   The glMDSPlot indicates further segregation along
the 6th dimension.  The percent of variance exaplained by this dimension or 
$\approx$ 2%.  Tracking down this source of variability may be quite challenging,
especially without having the complete information about the sample attributes 
and provenance.  

Unwanted variability is a well-documented problem in the analysis of RNA-Seq data
(see Peixoto et al. (2015) [@Peixoto:2015aa]), and many procedures have been proposed
to reduce the effect of unwanted variation on RNA-Seq analsys results 
([@Gandolfo:2018aa;@Peixoto:2015aa;@Risso:2014aa]).  There are undoubtedly
some similar sources of systematic variation in the 5hmC data, but it is
beyond the scope of this work to investigate these in this particular dataset.
Given that the clustering of samples occurs in MDS dimensions that explain
a small fraction of variability, and that these is no assocation with the
factor of interest, HCC vs Control, these sources of variability should not
interfere too much with our classification analysis.  It would nonetheless be interesting
to assess whether downstream results can be improved by removing this variability.



### Creating a design matrix and contrasts  {-}

Before proceeding with the statistical modeling used for the 
differential expression analysis, we need to set up a
model design matrix.

```{r DEADesign, cache=F, include=T, echo=T, include=T}

Design_mtx <- model.matrix( ~  -1 + group, data=AF_dgel$samples)
colnames(Design_mtx) <- sub('group', '', colnames(Design_mtx))

cat("colSums(Design_mtx):\n")
colSums(Design_mtx)

Contrasts_mtx <- limma::makeContrasts(
  HCCvsControl = HCC  - Control,
  levels=colnames(Design_mtx))

cat("Contrasts:\n")
Contrasts_mtx

```

```{r printDesign, echo=T, include=F}
 knitr::kable(head(Design_mtx), caption='Design Matrix') %>%
  kableExtra::kable_styling(full_width = F)
```


### Removing heteroscedasticity from the count data {-}


As for RNA-Seq data, for 5hmC count data the variance is not independent of the mean.
In `limma`, the R package we are using for our analyses, 
linear modeling is carried out on the log-CPM values which are assumed to be 
normally distributed and the mean-variance relationship is accommodated using precision 
weights calculated by the voom function.  We apply this transformation next.


```{r Voom1, cache=T, cache.vars=c('filteredCountsAF_voom'), fig.height=6, fig.width=11, fig.cap="Removing heteroscedascity", echo=T}

par(mfrow=c(1,1))
filteredCountsAF_voom <- limma::voom(AF_dgel, Design_mtx, plot=T)

```

Note that the voom-plot provides a visual check on the level of filtering performed upstream.
If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be 
observed at the low end of the expression scale due to very small counts. 

<!--
Means (x-axis) and variances (y-axis) of each gene are plotted to show the dependence between the two before voom is applied to the data ( A) and how the trend is removed after voom precision weights are applied to the data ( B). The plot on the left is created within the voom function which extracts residual variances from fitting linear models to log-CPM transformed data. Variances are then re-scaled to quarter-root variances (or square-root of standard deviations) and plotted against the mean expression of each gene. The means are log 2-transformed mean-counts with an offset of 2. The plot on the right is created using plotSA which plots log 2 residual standard deviations against mean log-CPM values. The average log 2 residual standard deviation is marked by a horizontal blue line. In both plots, each black dot represents a gene and a red curve is fitted to these points.
-->

<!--

To get a sense of how this compares with RNA-Seq dsta, we can take a look at Figure 4 in Law et al. [@Law:2018aa]:

TICKr knitr::include_graphics(
  "Static/images/Law-fig4.gif",  dpi=100)TICK

We observe that the variability in the 5hmC data is quite a bit lower.  Statistical summaries would give
a better idea.  

-->


### Fit linear models and examine the results {-}

Having properly filtered and  normalized the data,
the linear models can be fitted to each gene and the results
examined to assess differential expression between the two groups
of interest, in our case HCC vs Control.

Table \@ref(tab:lmFit) displays the counts of genes in each DE category:

```{r lmFit, cache=T, echo=T, cache.vars=c('filteredCountsAF_voom_efit','filteredCountsAF_voom_efit_dt'),echo=T}


 filteredCountsAF_voom_fit <- limma::lmFit(filteredCountsAF_voom, Design_mtx)
 colnames(filteredCountsAF_voom_fit$coefficients) <- sub("\\(Intercept\\)", "Intercept",
 colnames(filteredCountsAF_voom_fit$coefficients) )

 filteredCountsAF_voom_fit <- limma::contrasts.fit(
    filteredCountsAF_voom_fit, contrasts=Contrasts_mtx)

 filteredCountsAF_voom_efit <- limma::eBayes(filteredCountsAF_voom_fit)

 filteredCountsAF_voom_efit_dt <-
 limma::decideTests(filteredCountsAF_voom_efit,adjust.method = "BH", p.value = 0.05)
 
 knitr::kable(t(summary(filteredCountsAF_voom_efit_dt)),
  caption="DE Results at FDR = 0.05") %>% 
  kableExtra::kable_styling(full_width = F)

```

### Graphical representations of DE results: MD Plots {-}

To summarise results for all genes visually, mean-difference plots
(aka MA plot), which display log-FCs from the linear model fit against 
the average log-CPM values can be generated using the plotMD function,
with the differentially expressed genes highlighted.

We may also be interested in whether certain gene features are 
related to gene identification.  Gene GC content, for example, might be
of interest.

```{r mdPlotEfit, cache=T, cache.vars='GC_vec', fig.height=5, fig.width=11, fig.cap="HCC vs Control - Genes Identified at FDR = 0,05", echo=T}


par(mfrow=c(1,3), mar=c(4.5,4.5,2,1),oma=c(1,1,2,0))

# log-fold-change vs ave-expr
limma::plotMD(filteredCountsAF_voom_efit,
 ylim = c(-0.4, 0.4),
 column='HCCvsControl',
 status=filteredCountsAF_voom_efit_dt[,'HCCvsControl'],
 hl.pch = 16, hl.col = c("lightblue", "pink"), hl.cex = .5,
 bg.pch = 16, bg.col = "grey", bg.cex = 0.5,
 main = '',
 xlab = paste0(
    "Average log-expression: IQR=",
    paste(round(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 3) / 4), 2),
      collapse = ", "
    )
  ),
  ylab = paste0(
    "log-fold-change: IQR=",
    paste(round(quantile(filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'], prob = c(1, 3) / 4), 2),
      collapse = ", "
    )
  ),
  legend = F, cex.lab=1.5
)
abline(h = 0, col = "black")
rug(quantile(filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'], prob = c(1, 2, 3) / 4),
  col = "purple",
  ticksize = .03, side = 2, lwd = 2
)
rug(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 2, 3) / 4),
  col = "purple",
  ticksize = .03, side = 1, lwd = 2
)

# log-fold-change vs identification

boxplot(split(
 filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'],
 filteredCountsAF_voom_efit_dt[,'HCCvsControl']),
 outline=F,
 border=c("pink", "grey", "lightblue"), xaxt='n',
 ylab='log-fold-change', ylim=c(-.4, .4),
 cex.lab=1.5
)
axis(side=1, at=1:3, c('down', 'notDE', 'up'), cex.axis=1.5)

# gc vs identification
genes_ndx <- match(rownames(filteredCountsAF_voom_efit), genes_annotAF$Symbol)
if(sum(is.na(genes_ndx))) stop("filteredCountsAF_voom_efit/genes_annotAF: genes mismatch")
GC_vec <- with(genes_annotAF[genes_ndx,],(G+C)/(A+C+G+T))


boxplot(split(
 GC_vec,
 filteredCountsAF_voom_efit_dt[,'HCCvsControl']),
 outline=F,
 border=c("pink", "grey", "lightblue"), xaxt='n',
 ylab='gene-gc', cex.lab=1.5
)
axis(side=1, at=1:3, c('down', 'notDE', 'up'), cex.axis=1.5)

 #mtext(side=3, outer=T, cex=1.25, "Genes identified at adjusted p-value=0.05")
```

```{r quantlogFC,echo=T}

featureCountsAF_logFC_sum <- sapply(
 split(
 filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'],
 filteredCountsAF_voom_efit_dt[,'HCCvsControl']),
 quantile, prob = (1:3) / 4)

colnames(featureCountsAF_logFC_sum) <- as.character(factor(
 colnames(featureCountsAF_logFC_sum), 
 levels=c("-1", "0", "1"),
 labels=c('down', 'notDE', 'up')
))


knitr::kable(featureCountsAF_logFC_sum,
  digits = 2,
  caption = "log FC quartiles by gene identification") %>%
  kableExtra::kable_styling(full_width = F)

```


While many genes are identified, the effect sizes are quite small,
which results in a low signal-to-noise ratio context.   See 
Section \@ref(snr-regime) below.

The log-fold-change distribution for up-represented genes is long-tailed,
with many high log fold-change values.
By contrast, log-fold-change distribution for down-represented genes
closer to symmetric and has few genes with low log fold-change values.
We will see how this affects the results of identifying genes with
an effect size requirement.

The GC content of down regulated genes tends to be slightly lower than the
rest of the genes.  A statistical test would find that the difference
between the mean of the down regulated gene population is singificantly different
than the mean of the other gene population even though the difference is
quite small
($`r round( 
mean(GC_vec[filteredCountsAF_voom_efit_dt[,'HCCvsControl']=='-1']) -
mean(GC_vec[filteredCountsAF_voom_efit_dt[,'HCCvsControl']!='-1']),
3)`$).

These asymmetries are minor, but it would still be good to establish that
they relfect biology rather than processing artifacts.  


### DE genes at 10% fold change {-}

For a stricter definition on significance, one may require log-fold-changes 
(log-FCs) to be above a minimum value. The treat method 
(McCarthy and Smyth 2009 [@McCarthy:2009aa]) can be used to calculate p-values 
from empirical Bayes moderated t-statistics with a minimum log-FC requirement. 
The number of differentially expressed genes are greatly reduced if we 
impose a minimal fold-change requirement of 10%.

```{r mdPlotTfit, cache=T, cache.vars='', fig.height=5, fig.width=11, fig.cap="HCC vs Control - Identified Genes at FDR = 0,05 and logFC > 10%",echo=T}

filteredCountsAF_voom_tfit <- limma::treat(filteredCountsAF_voom_fit, lfc=log2(1.10))
filteredCountsAF_voom_tfit_dt <- limma::decideTests(filteredCountsAF_voom_tfit)

cat("10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05:\n")
summary(filteredCountsAF_voom_tfit_dt)

# log-fold-change vs ave-expr
limma::plotMD(filteredCountsAF_voom_efit,
 ylim = c(-0.5, 0.5),
 column='HCCvsControl',
 status=filteredCountsAF_voom_tfit_dt[,'HCCvsControl'],
 hl.pch = 16, hl.col = c("blue", "red"), hl.cex = .7,
 bg.pch = 16, bg.col = "grey", bg.cex = 0.5,
 main = '',
 xlab = paste0(
    "Average log-expression: IQR=",
    paste(round(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 3) / 4), 2),
      collapse = ", "
    )
  ),
  ylab = paste0(
    "log-fold-change: IQR=",
    paste(round(quantile(filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'], prob = c(1, 3) / 4), 2),
      collapse = ", "
    )
  ),
  legend = F
)
abline(h = 0, col = "black")
rug(quantile(filteredCountsAF_voom_efit$coefficients[, 'HCCvsControl'], prob = c(1, 2, 3) / 4),
  col = "purple",
  ticksize = .03, side = 2, lwd = 2
)
rug(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 2, 3) / 4),
  col = "purple",
  ticksize = .03, side = 1, lwd = 2
)


```

As noted above, the log-fold-change distribution for the up-represented genes
is long-tailes in  comparison to log-fold-change distribution for the down-represented genes.
As a result fewer down-represented than up-regulated genes are identified when a 
minimum log-FC requirement is imposed.


## Signal-to-noise ratio regime {#snr-regime}

In Hastie et al. (2017) [@Hastie:2017aa]) results from `lasso` fits are
compared with `best subset` and `forward selection` fits and it is argued
that while `best subset` is optimal for high signal-to-noise regimes, 
the lasso gains some competitive advantage when the prevailing signal-to-noise
ratio of the dataset is lowered.  

We can extract sigma and signal from the fit objects to get SNR values for each gene
to see in what SNR regime the 5hmC gene body data are.

 
```{r altCV, cache=T, cache.vars='',fig.height=4, fig.width=6, fig.cap="Alternative CV Calculation"}
lib.size <- colSums(AF_dgel$counts)

fit <- filteredCountsAF_voom_efit
sx <- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06)
sy <- sqrt(fit$sigma)

CV <- sy/sx    


```

<!-- DEBUG BCV from Section \@ref(analysis-of-coverage-variability) vs CV
pairs(cbind(BCV_mtx, CV)) 
boxplot(cbind(BCV_mtx, CV), outline=F) 
-->


```{r plotSNR,  fig.height=5, fig.width=10, fig.cap="Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile",message=F, echo=T,include=T}

Effect <- abs(filteredCountsAF_voom_efit$coefficients[,'HCCvsControl'])
Noise <- filteredCountsAF_voom_efit$sigma
SNR <- Effect/Noise

plot(spatstat::CDF(density(SNR)),
  col = 1, lwd = 2, ylab = "Prob(SNR<x)",
  xlim = c(0, 0.2)
)

SNR_quant <- quantile(SNR, prob=c((1:3)/4,.9))
rug(SNR_quant,
    lwd = 2, ticksize = 0.05, col = 1
  )


knitr::kable(t(SNR_quant),
  digits = 3,
  caption = paste(
    "SNR Quantiles") 
) %>% kableExtra::kable_styling(full_width = F)



```

These SNR values are in the range where the lasso and relaxed lasso gain some advantage over
best subset and forward selection fits (see  Hastie et al. (2017) [@Hastie:2017aa]).


<!--chapter:end:00-preprocessing.Rmd-->

# The bet on sparsity {#explore-sparsity}

In this section we explore various fits that can be computed 
and analyzed with tools provided in the `glmnet` package.
Refer to the [Glmnet Vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
for a quick reference guide.  

We focus our analyses on lasso fits which tend to favor sparse models.

## Cross-validation analysis setup 

```{r setParameters}

K_FOLD <- 10
trainP <- 0.8

```

<!-- NOT CURRENTLY USED 
EPS <- 0.05    # Have no idea what "small" epsilon means
-->


First we divide the analysis dataset into `train` and `test` in a $`r trainP/(1-trainP)`$:1 ratio.  

```{r getTrainVal, cache=T, cache.vars=c('train_sampID_vec', 'test_sampID_vec','train_group_vec','test_group_vec','train_lcpm_mtx','test_lcpm_mtx')}

set.seed(1)
train_sampID_vec <- with(AF_dgel$samples,
AF_dgel$samples$sampID[caret::createDataPartition(y=group, p=trainP, list=F)]
)

test_sampID_vec <- with(AF_dgel$samples,
setdiff(sampID, train_sampID_vec)
)

train_group_vec <- AF_dgel$samples[train_sampID_vec, 'group']
names(train_group_vec) <- AF_dgel$samples[train_sampID_vec, 'sampID']

test_group_vec <- AF_dgel$samples[test_sampID_vec, 'group']
names(test_group_vec) <- AF_dgel$samples[test_sampID_vec, 'sampID']

knitr::kable(table(train_group_vec),
  caption="Train set") %>%
   kableExtra::kable_styling(full_width = F)

knitr::kable(table(test_group_vec),
  caption="Test set") %>%
   kableExtra::kable_styling(full_width = F)

train_lcpm_mtx <- t(lcpm_mtx[,train_sampID_vec])
test_lcpm_mtx <- t(lcpm_mtx[,test_sampID_vec])

```

We explore some glmnet fits and the "bet on sparsity".
We consider three models, specified by the value of the
**alpha** parameter in the elastic net parametrization:  
    - lasso: $\alpha = 1.0$ - sparse models  
    - ridge $\alpha = 0$ - shrunken coefficients models  
    - elastic net:  $\alpha = 0.5$  - semi sparse model  
<!-- - lassoC: $\alpha = 1-\epsilon =$ $TICKr 1- EPSTICK - lasso for correlated predictors  -->

Some questions of interest include:  

* How sparse are models enabling good 5hmC classification of Early HCC vs Control samples?    

<!--
    - The exploratory analysis done in the preprocessing step
indicates that differential gene body 5hmC representation effects are small.
With reasonably large sample sizes we would expect many genes to be selected.
???
-->

<!-- DROP THIS - havign trouble getting at pure lassoR
* Does the relaxed lasso improve performance in this case?  
-->

* Does the shrunken relaxed lasso (aka the blended mix) improve performance in this case? 

* Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set?  

In this analysis, we will only evaluate models in terms of 
model sparsity, stability and performance.  We leave the question
of significance testing of hypotheses about model parameters
completely out.  See Lockhart et al. (2014) [@Lockhart:2014aa]
and Wassermam (2014) [@Wasserman:2014aa] for a discussion of this topic.

In this section we look at the relative performance and sparsity of the models
considered.  The effect of the size of the sample set on the level and 
stability of performance will be investigated in the next section.


***

First we create folds for $`r K_FOLD`$-fold cross-validation of models fitted to
training data.  We'll use caret::createFolds to assign samples
to folds while keeping the outcome ratios constant across folds.


```{r getTrainFolds, cache=T, cache.vars='train_foldid_vec'}
# This is too variable, both in terms of fold size And composition
#foldid_vec <- sample(1:10, size=length(train_group_vec), replace=T)

set.seed(1)
train_foldid_vec <- caret::createFolds(
 factor(train_group_vec), 
 k=K_FOLD,
 list=F)

knitr::kable(sapply(split(train_group_vec, train_foldid_vec), 
  table), caption="training samples fold composition") %>%
   kableExtra::kable_styling(full_width = F)
 
```

Note that the folds identify samples that are left-out of the training
data for each fold fit.


## Fit and compare models 

`glmnet` provides cross-validation methods to pick the parameter **lambda** which
controls to size of the penalty function. The  “one standard error rule” 
produces a model with fewer predictors  then the minimum cv error model.
On the training data, this usually results in increased MSE and more 
biased parameter estimates 
(see Engebretsen et al. (2019) [@Engebretsen:2019aa] for example).
The question of iterest though is the performance on unseen data; not on the training data.
In the analysis below, we compare the cv error rates with out-of-fold and
test set error rates.  The results show that out-of-fold error rates computed from
the training data are good indicators of test set error rates, and that
the one standard error rule models do as well as the minimim cv error models
for the lasso, which has the best overall performance.



### Logistic regression in `glmnet`

`glmnet` provides functionality to extract various predicted of fitted values
from calibrated models.  Note in passing that some folks make a distinction between
**fitted** or **estimated** values for sample points in the training data 
versus **predicted** values for sample points that
are not in the training dataset.  `glmnet` makes no such distinction and the
`predict` function is used to produce both fitted as well as predicted values.
When predict is invoked to make predictions for design points that are part 
of the training dataset, what is returned are fitted values.  
When predict is invoked to make predictions for design points that are not part 
of the training dataset, what is returned are predicted values.  

For logistic regressions, which is the model fitted in a regularized fashion
when models are fitted by glmnet with the parameter `family='binomial'`, three
fitted or predicted values can be extracted at a given design point.
Suppose our response variable Y is either 0 or 1 (Control or HCC in our case).
These are specified by the `type` parameter.  `type='resp'` returns
the fitted or predicted probability of $Y=1$.  `type='class'` returns the fitted or
predicted class for the design point, which is simply dichotomizing the 
response: class = 1 if the fitted or predicted probability is greater than 0.5
(check to make sure class is no the Bayes estimate).  `type='link'` returns
the fitted or predicted value of the linear predictor $\beta'x$.  The relationship
between the linear predictor and the response can be derided from  the 
logistic regression model:

$$P(Y=1|x,\beta) = g^{-1}(\beta'x) = h(\beta'x) = \frac{e^{\beta'x}}{1+e^{\beta'x}}$$

where $g$ is the link function, $g^{-1}$ the mean function.
The link function is given by:

$$g(y) = h^{-1}(y) = ln(\frac{y}{1-y})$$

This link function is called the *logit* function, and its inverse the *logistic*
function.


```{r logistic_f}

logistic_f <- function(x) exp(x)/(1+exp(x))

```

It is important to note that all *predicted* values extracted from 
`glmnet` fitted models by the **predict()** extraction method
yield **fitted** values for design points that are part of the
training data set.  This includes the predicted class for training data
which are used  to estimate misclassification error rates.  As a result, the cv error 
rates quoted in various `glmnet` summaries are generally optimistic.
`glmnet` fitting functions have a 
parameter, *keep*, which instructs the fitting function to keep the
`out-of-fold`, or `prevalidated`, predictions as part of the returned object.  The
`out-of-fold` predictions are predicted values for the samples in the
left-out folds, pooled across all cv folds.  For each hyper-parameter
specification, we get one full set of `out-of-fold` predictions for
the training set samples.  Performance assessments based on these
values are usually more generalizable - ie. predictive of
performance in unseen data - than assessments based on values
produced from the full fit, which by default is what `glmnet` extraction
methods provide.  See Höfling and Tibshirani (2008) [@Hofling:2008aa] 
for a description of the use of pre-validation in model assessment.  


<!--
Because the `keep=T` option will store predicted values for 
all models evaluated in the cross-validation process, we will
limit the number of models tested by setting **nlambda=30**
when calling the fitting functions.  This has no effect on 
performance in this data set.
-->


```{r doMC, include=F}
require(doMC)
registerDoMC(cores=14)
```


```{r fit-lasso, cache=T, cache.vars=c('cv_lasso')}

start_time <-  proc.time()

cv_lasso <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=1,
 family='binomial', 
 type.measure = "class",
 keep=T,
 nlambda=30
)

message("lasso time: ", round((proc.time() - start_time)[3],2),"s")

```

```{r fit-ridge, cache=T, cache.vars=c('cv_ridge')}
start_time <-  proc.time()

cv_ridge <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=0,
 family='binomial', 
 type.measure = "class",
 keep=T,
 nlambda=30
)

message("ridge time: ", round((proc.time() - start_time)[3],2),"s")

```


```{r fit-enet, cache=T, cache.vars=c('cv_enet')}
start_time <-  proc.time()

cv_enet <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=0.5,
 family='binomial',
 type.measure = "class",
 keep=T,
 nlambda=30
)

message("enet time: ", round((proc.time() - start_time)[3],2),"s")

```


```{r fit-lassoC, cache=T, cache.vars=c('cv_lassoC'), eval=F, echo=F}
start_time <-  proc.time()

cv_lassoC <-  glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=1-EPS,
 family='binomial',
 type.measure = "class",
 keep=T,
 nlambda=30
)

message("lassoC time: ", round((proc.time() - start_time)[3],2),"s")

```

<!--
The ridge regression model takes over 10 times longer to compute.
-->

<!-- do not show
Define plotting function.
Maybe show in appendix??
-->
```{r plot_cv_f,echo=T}

plot_cv_f <- function(cv_fit, Nzero=T, ...) {
 
 suppressPackageStartupMessages(require(glmnet))

 # No nonger used
 #lambda.1se_p <- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]
 #lambda.min_p <- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]
 
 # Get oof error
 ndx_1se <- match(cv_fit$lambda.1se,cv_fit$lambda)
 train_oofPred_1se_vec <- ifelse(
  cv_fit$fit.preval[,ndx_1se] > 0.5, 'HCC', 'Control')
 train_oofPred_1se_error <- mean(train_oofPred_1se_vec != train_group_vec)

 ndx_min <- match(cv_fit$lambda.min,cv_fit$lambda)
 train_oofPred_min_vec <- ifelse(
  cv_fit$fit.preval[,ndx_min] > 0.5, 'HCC', 'Control')
 train_oofPred_min_error <- mean(train_oofPred_min_vec != train_group_vec)

 # Get test set error
 test_pred_1se_vec <- predict(
  cv_fit, 
  newx=test_lcpm_mtx, 
  s="lambda.1se",
  type="class"
 )
 test_pred_1se_error <- mean(test_pred_1se_vec != test_group_vec)
 
 test_pred_min_vec <- predict(
  cv_fit, 
  newx=test_lcpm_mtx, 
  s="lambda.min",
  type="class"
 )
 test_pred_min_error <- mean(test_pred_min_vec != test_group_vec)
 
  
 plot(
  log(cv_fit$lambda),
  cv_fit$cvm,
  pch=16,col="red",
  xlab='',ylab='',
  ...
 )
 abline(v=log(c(cv_fit$lambda.1se, cv_fit$lambda.min)))
 if(Nzero)
 axis(side=3, tick=F, at=log(cv_fit$lambda), 
  labels=cv_fit$nzero, line = -1
 )
 LL <- 2
 #mtext(side=1, outer=F, line = LL, "log(Lambda)")
 #LL <- LL+1
 mtext(side=1, outer=F, line = LL, paste(
  #ifelse(Nzero, paste("1se p =", lambda.1se_p),''),
  "1se: cv =", round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], 1),
  "oof =", round(100*train_oofPred_1se_error, 1),
  "test =", round(100*test_pred_1se_error, 1)
 ))
 LL <- LL+1
 mtext(side=1, outer=F, line = LL, paste(
  #ifelse(Nzero, paste("min p =", lambda.min_p),''),
  "min: cv =", round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], 1),
  "oof =", round(100*train_oofPred_min_error, 1),
  "test =", round(100*test_pred_min_error, 1)
 ))
 
 tmp <-
 cbind(
  error_1se = c(
   p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se],
   train_cv = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se],
   train_oof = 100*train_oofPred_1se_error,
   test = 100*test_pred_1se_error),
  error_min = c(
   p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min],
   train_cv = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min],
   train_oof = 100*train_oofPred_min_error,
   test = 100*test_pred_min_error)
  )
  # Need to fix names  
  rownames(tmp) <- c('p', 'train_cv', 'train_oof', 'test')
  tmp 
}

```

Examine model performance.

```{r lookFits, cache=F, cache.vars='', fig.height=5, fig.width=11, fig.cap="compare fits", echo=T, warnings=F,message=F}

 par(mfrow=c(1,3), mar=c(5, 2, 3, 1), oma=c(3,2,0,0)) 

 lasso_errors_mtx <- plot_cv_f(cv_lasso, ylim=c(0,.5))
 title('lasso')

 rifge_errors_mtx <- plot_cv_f(cv_ridge, Nzero=F, ylim=c(0,.5))
 title('ridge')

 enet_errors_mtx <-  plot_cv_f(cv_enet, ylim=c(0,.5))
 title('enet')

 mtext(side=1, outer=T, cex=1.25, 'log(Lambda)')
 mtext(side=2, outer=T, cex=1.25, cv_lasso$name)

```

```{r printErrors, fig.cap='model errors'}


errors_frm <- data.frame(
  lasso = lasso_errors_mtx, ridge = rifge_errors_mtx, enet = enet_errors_mtx
)

knitr::kable(t(errors_frm),
 caption = 'Misclassifiaction error rates',
 digits=1) %>% 
  kableExtra::kable_styling(full_width = F)

```

We see that the lasso and enet models do better than the ridge model.
The *1se* lambda lasso fit is only slightly more parsimonious than the 
*1se* elastic net fit, but its test set accuracy is better.
The *min* lambda elastic net fit performs as well as the lasso model,
but is much less parsimonious.

We also see that the training data out-of-fold
estimates of misclassification error rates are much closer to the
test set estimates than are the cv estimated rates.  This has
been our experience with regularized regression models fitted to
genomic scale data.  It should also be noted that the cv estimates of 
misclassification rates become more biased as the sample size decreases,
as we will show in Section \@ref(model-suite).  


## The relaxed lasso and blended mix models

Next we look at the so-called `relaxed lasso` model, and 
the `blended mix` which is an optimized shrinkage
between the relaxed lasso and the regular lasso.
See \@ref(eq:blended) in Section \@ref(modeling-background).  


```{r fitLassoR, cache=T, cache.vars=c('cv_lassoR'), include=F}

require(doMC)
registerDoMC(cores=14)


start_time <-  proc.time()

cv_lassoR <-  glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 # for stability
 alpha=1, ###-EPS,this didn't do anything
 relax=T,
 family='binomial',
 type.measure = "class",
 keep=T,
 nlambda=30
)


message("lassoR time: ", round((proc.time() - start_time)[3],2),"s")

```

<!--
The relaxed fit takes quite a bit longer.  
-->

```{r lookLassoR, cache=T, cache.vars='', fig.height=5, fig.width=6, fig.cap="Relaxed lasso fit", echo=T}

library(glmnet)

cv_lassoR_sum <- print(cv_lassoR)

plot(cv_lassoR)

```

```{r lookLassoR2,cache=T, cache.vars=''}
# only report  1se
ndx_1se <- match(cv_lassoR$lambda.1se, cv_lassoR$lambda)
ndx_min <- match(cv_lassoR$lambda.min, cv_lassoR$lambda)

# only show 1se anyway
# if(ndx_1se != ndx_min) stop("lambda.1se != lambda.min")


# train oof data
# Get relaxed lasso (gamma=0) oof error
train_oofPred_relaxed_1se_vec <- ifelse(
  cv_lassoR$fit.preval[["g:0"]][, ndx_1se] > 0.5, "HCC", "Control"
)
train_oofPred_relaxed_1se_error <- mean(train_oofPred_relaxed_1se_vec != train_group_vec)

# blended mix (gamma=0.5)
train_oofPred_blended_1se_vec <- ifelse(
  cv_lassoR$fit.preval[["g:0.5"]][, ndx_1se] > 0.5, "HCC", "Control"
)
train_oofPred_blended_1se_error <- mean(train_oofPred_blended_1se_vec != train_group_vec)


# Test set error - relaxed
test_pred_relaxed_1se_vec <- predict(
  cv_lassoR,
  newx = test_lcpm_mtx,
  s = "lambda.1se",
  type = "class",
  gamma = 0
)
test_pred_relaxed_1se_error <- mean(test_pred_relaxed_1se_vec != test_group_vec)

# Test set error - blended
test_pred_blended_1se_vec <- predict(
  cv_lassoR,
  newx = test_lcpm_mtx,
  s = "lambda.1se",
  type = "class",
  gamma = 0.5
)
test_pred_blended_1se_error <- mean(test_pred_blended_1se_vec != test_group_vec)



cv_lassoR_1se_error <- cv_lassoR$cvm[cv_lassoR$lambda==cv_lassoR$lambda.min]

cv_blended_statlist <- cv_lassoR$relaxed$statlist[['g:0.5']]
cv_blended_1se_error <- cv_blended_statlist$cvm[cv_blended_statlist$lambda==
   cv_lassoR$relaxed$lambda.1se]
 


knitr::kable(t(data.frame(
  train_relaxed_cv = cv_lassoR_1se_error,
  train_blended_cv = cv_blended_1se_error,
  train_relaxed_oof = train_oofPred_relaxed_1se_error,
  train_blended_oof = train_oofPred_blended_1se_error,
  test_relaxed_oof = test_pred_relaxed_1se_error,
  test_blended_oof = test_pred_blended_1se_error
)) * 100,
digits = 1,
caption = "Relaxed lasso and blended mix error rates"
) %>%
  kableExtra::kable_styling(full_width = F)

```

The relaxed lasso and blended mix error rates are comparable to the
regular lasso fit error rate.  We see here too that the reported cv 
error rates are quite optimistic, while out-of-fold error rates
continue to be good indicators of unseen data error rates, as captured
by the test set.  

The *1se* lambda rule applied to the relaxed lasso fit selected a model with 
$`r cv_lassoR$nzero[cv_lassoR$lambda==cv_lassoR$lambda.1se]`$ features,
while for the blended mix model 
(See \@ref(eq:blended) in Section \@ref(modeling-background))
the *1se* lambda rule selected
$`r cv_lassoR$relaxed$nzero.1se`$ features (vertical 
dotted reference line in Figure \@ref(fig:lookLassoR)).



## Examination of sensitivity vs specificity

In the results above we reported error rates without inspecting the 
sensitivity versus specificity trade-off.  ROC curves can be examined
to get a sense of the trade-off.

### Training data out-of-fold ROC curves


```{r trainROC, cache=F, cache.vars='', fig.height=5, fig.width=5, fig.cap="Train data out-of-sample ROCs"}

# train
# lasso
ndx_1se <- match(cv_lasso$lambda.1se,cv_lasso$lambda)
train_lasso_oofProb_vec <- logistic_f(cv_lasso$fit.preval[,ndx_1se])
train_lasso_roc <- pROC::roc(
 response = as.numeric(train_group_vec=='HCC'),
 predictor = train_lasso_oofProb_vec)

# enet
ndx_1se <- match(cv_enet$lambda.1se,cv_enet$lambda)
train_enet_oofProb_vec <- logistic_f(cv_enet$fit.preval[,ndx_1se])
train_enet_roc <- pROC::roc(
 response = as.numeric(train_group_vec=='HCC'),
 predictor = train_enet_oofProb_vec)

# lasso - relaxed
ndx_1se <- match(cv_lassoR$lambda.1se,cv_lassoR$lambda)
train_relaxed_oofProb_vec <- logistic_f(cv_lassoR$fit.preval[['g:0']][,ndx_1se])
train_relaxed_roc <- pROC::roc(
 response = as.numeric(train_group_vec=='HCC'),
 predictor = train_relaxed_oofProb_vec)

# blended mix (gamma=0.5)
ndx_1se <- match(cv_lassoR$lambda.1se,cv_lassoR$lambda)
train_blended_oofProb_vec <- logistic_f(cv_lassoR$fit.preval[['g:0.5']][,ndx_1se])
train_blended_roc <- pROC::roc(
 response = as.numeric(train_group_vec=='HCC'),
 predictor = train_blended_oofProb_vec)

plot(train_lasso_roc, col=col_vec[1])
lines(train_enet_roc, col=col_vec[2])
lines(train_relaxed_roc, col=col_vec[3])
lines(train_blended_roc, col=col_vec[4])

legend('bottomright', title='AUC',
 legend=c(
  paste('lasso =', round(train_lasso_roc[['auc']],3)),
  paste('enet =', round(train_enet_roc[['auc']],3)),
  paste('relaxed =', round(train_relaxed_roc[['auc']],3)),
  paste('blended =', round(train_blended_roc[['auc']],3))
 ),
 text.col = col_vec[1:4],
 bty='n'
)

```

Compare thresholds for 90% Specificity:

```{r thresh90, cache=F, cache.vars='', fig.cap='90% Specificity Thresholds'}

 lasso_ndx <- with(as.data.frame(pROC::coords(train_lasso_roc, transpose=F)), 
   min(which(specificity >= 0.9)))

 enet_ndx <- with(as.data.frame(pROC::coords(train_enet_roc, transpose=F)), 
   min(which(specificity >= 0.9)))

 lassoR_ndx <- with(as.data.frame(pROC::coords(train_relaxed_roc, transpose=F)), 
   min(which(specificity >= 0.9)))

 blended_ndx <- with(as.data.frame(pROC::coords(train_blended_roc, transpose=F)), 
   min(which(specificity >= 0.9)))

  spec90_frm <- data.frame(rbind(
  lasso=as.data.frame(pROC::coords(train_lasso_roc, transpose=F))[lasso_ndx,],
  enet=as.data.frame(pROC::coords(train_enet_roc, transpose=F))[enet_ndx,],
  relaxed=as.data.frame(pROC::coords(train_relaxed_roc, transpose=F))[lassoR_ndx,],
  blended=as.data.frame(pROC::coords(train_blended_roc, transpose=F))[blended_ndx,]
 ))


knitr::kable(spec90_frm,
  digits=3,
  caption="Specificity = .90 Coordinates"
) %>%
  kableExtra::kable_styling(full_width = F)

```

This is strange.


```{r trainOOFprops, cache=F, cache.vars='', fig.height=8, fig.width=10, fig.cap="Train data out-of-fold predicted probabilities"}

par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2))

# lasso
plot(density(train_lasso_oofProb_vec[train_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(train_lasso_oofProb_vec[train_group_vec == "HCC"]),
  col = "red"
)
title("lasso")
legend("topright", legend = c("Control", "HCC"), text.col = c("green", "red"))

# enet
plot(density(train_enet_oofProb_vec[train_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(train_enet_oofProb_vec[train_group_vec == "HCC"]),
  col = "red"
)
title("enet")

# lassoR
plot(density(train_relaxed_oofProb_vec[train_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(train_relaxed_oofProb_vec[train_group_vec == "HCC"]),
  col = "red"
)
title("lassoR")

# blended
plot(density(train_blended_oofProb_vec[train_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(train_blended_oofProb_vec[train_group_vec == "HCC"]),
  col = "red"
)
title("blended")

mtext(side = 1, outer = T, "out-of-fold predicted probability", cex = 1.25)
mtext(side = 2, outer = T, "density", cex = 1.25)

```

The relaxed lasso fit results in essentially dichotomized predicted probability
distribution - predicted probabilities are very close to 0 or 1.

 
Look at test data ROC curves.

```{r testROC, cache=F, cache.vars='', fig.height=5, fig.width=5, fig.cap="Test data out-of-sample ROCs", echo=T, include=F}

# train
# lasso
test_lasso_predProb_vec <- predict(
  cv_lasso,
  type = "resp",
  lambda = "1se",
  newx = test_lcpm_mtx
)

test_lasso_roc <- pROC::roc(
  response = as.numeric(test_group_vec == "HCC"),
  predictor = test_lasso_predProb_vec
)

# enet
test_enet_predProb_vec <- predict(
  cv_enet,
  type = "resp",
  lambda = "1se",
  newx = test_lcpm_mtx
)

test_enet_roc <- pROC::roc(
  response = as.numeric(test_group_vec == "HCC"),
  predictor = test_enet_predProb_vec
)


# lassoR
test_relaxed_predProb_vec <- predict(
  cv_lassoR,
  type = "resp",
  lambda = "1se",
  newx = test_lcpm_mtx,
  gamma = 0,
)

test_relaxed_roc <- pROC::roc(
  response = as.numeric(test_group_vec == "HCC"),
  predictor = test_relaxed_predProb_vec
)

# blended mix (gamma=0.5)
test_blended_predProb_vec <- predict(
  cv_lassoR,
  type = "resp",
  lambda = "1se",
  newx = test_lcpm_mtx,
  gamma = 0.5,
)

test_blended_roc <- pROC::roc(
  response = as.numeric(test_group_vec == "HCC"),
  predictor = test_blended_predProb_vec
)

```

```{r testROC2, cache=F, cache.vars='', fig.height=5, fig.width=5, fig.cap="Test data out-of-sample ROCs", echo=T, include=T}
# plot all
plot(test_lasso_roc, col = col_vec[1])
lines(test_enet_roc, col = col_vec[2])
lines(test_relaxed_roc, col = col_vec[3])
lines(test_blended_roc, col = col_vec[4])

legend("bottomright",
  title = "AUC",
  legend = c(
    paste("lasso =", round(test_lasso_roc[["auc"]], 3)),
    paste("enet =", round(test_enet_roc[["auc"]], 3)),
    paste("relaxed =", round(test_relaxed_roc[["auc"]], 3)),
    paste("blended =", round(test_blended_roc[["auc"]], 3))
  ),
  text.col = col_vec[1:4],
  bty='n'
)

```

Look at densities of predicted probabilities.

```{r testOOFprobs, cache=F, cache.vars='', fig.height=8, fig.width=10, fig.cap="Test data out-of-fold predicted probabilities", echo=T}

par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2))

# lasso
plot(density(test_lasso_predProb_vec[test_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(test_lasso_predProb_vec[test_group_vec == "HCC"]),
  col = "red"
)
title("lasso")
legend("topright", legend = c("Control", "HCC"), text.col = c("green", "red"))

# enet
plot(density(test_enet_predProb_vec[test_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(test_enet_predProb_vec[test_group_vec == "HCC"]),
  col = "red"
)
title("enet")

# relaxed
plot(density(test_relaxed_predProb_vec[test_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(test_relaxed_predProb_vec[test_group_vec == "HCC"]),
  col = "red"
)
title("relaxed")

#sapply(split(test_relaxed_predProb_vec, test_group_vec), summary)


# blended
plot(density(test_blended_predProb_vec[test_group_vec == "Control"]),
  xlim = c(0, 1), main = "", xlab = "", ylab = "", col = "green"
)
lines(density(test_blended_predProb_vec[test_group_vec == "HCC"]),
  col = "red"
)
title("blended")

mtext(side = 1, outer = T, "test set predicted probability", cex = 1.25)
mtext(side = 2, outer = T, "density", cex = 1.25)

```

```{r fitPrevalByGroup, cache=F, cache.vars='', fig.height=8, fig.width=8,fig.cap="Predicted Probabilities - Train and Test",echo=T}

# Define plotting function
bxpPredProb_f <- function(cv_fit, Gamma=NULL) {
  # Train - preval is out-of-fold linear predictor for training design points
  onese_ndx <- match(cv_fit$lambda.1se, cv_fit$lambda)
  if(is.null(Gamma)) 
   train_1se_preval_vec <- cv_fit$fit.preval[, onese_ndx] else
   train_1se_preval_vec <- cv_fit$fit.preval[[Gamma]][, onese_ndx] 

  train_1se_predProb_vec <- logistic_f(train_1se_preval_vec)

  # Test
  test_1se_predProb_vec <- predict(
    cv_fit,
    newx = test_lcpm_mtx,
    s = "lambda.1se",
    type = "resp"
  )

  tmp <- c(
    train = split(train_1se_predProb_vec, train_group_vec),
    test = split(test_1se_predProb_vec, test_group_vec)
  )
  names(tmp) <- paste0("\n", sub("\\.", "\n", names(tmp)))

  boxplot(tmp)
}

par(mfrow = c(2, 2), mar = c(5, 3, 2, 1), oma = c(2, 2, 2, 2))

bxpPredProb_f(cv_lasso)
title('lasso')

bxpPredProb_f(cv_enet)
title('enet')

bxpPredProb_f(cv_lassoR, Gamma='g:0')
title('relaxed')

bxpPredProb_f(cv_lassoR, Gamma='g:0.5')
title('blended')


```

<!--
Another look - plot train and test set logistic curves with annotation.

The following shows that predicted classes come from fitted
probabilities - not out of sample probabilities.

Also shows that threshold is at 0.5 

SKIP
-->

```{r trainLassoPred, cache=F, cache.vras='',fig.height=5, fig.width=11,fig.cap="train data lassofit", eval=F, echo=F}

# Train - preval is out-of-fold linear predictor for training design points
onese_ndx <- match(cv_lasso$lambda.1se,cv_lasso$lambda)
train_1se_preval_vec <- cv_lasso$fit.preval[,onese_ndx]
train_1se_predProb_vec <- logistic_f(train_1se_preval_vec)

train_1se_class_vec <- predict(
 cv_lasso,
 newx=train_lcpm_mtx,
 s="lambda.1se",
 type='class'
)
#


plot(
 x=train_1se_preval_vec, xlab='linear predictor (truncated)',
 y=train_1se_predProb_vec, ylab='predicted probability',
 col=ifelse(train_1se_class_vec == 'Control', 'green', 'red'),
 pch=ifelse(train_group_vec == 'Control', 1, 4),
 xlim=c(-5,5)
 )  

# compare with fitted probabilities
train_1se_link_vec <- predict(
 cv_lasso,
 newx=train_lcpm_mtx,
 s="lambda.1se",
 type='link'
)

train_1se_fittedProb_vec <- logistic_f(train_1se_link_vec)


plot(
 x=train_1se_link_vec, xlab='linear predictor (truncated)',
 y=train_1se_fittedProb_vec, ylab='predicted probability',
 col=ifelse(train_1se_class_vec == 'Control', 'green', 'red'),
 pch=ifelse(train_group_vec == 'Control', 1, 4),
 xlim=c(-5,5)
 ) 

```
  
We have seen above that assessments of model performance based on the out-of-fold 
predicted values are close to the test set assessments, and that
assessments based on prediction extracted from glmnet object are optimistic.
Here we look at confusion matrices to see how this affects the
classification results.

Here we us a threshold of 0.5 to dichotomize the predicted
probabilities into a class prediction, as is done in the
glmnet predictions.


```{r confMtxTrainLasso, cache=F, cache.vars='', fig.cap="Train set confusion", echo=T}

# lasso 
##########################
# train - cv predicted
train_lasso_predClass_vec <- predict(
 cv_lasso,
 newx=train_lcpm_mtx,
 s='lambda.1se',
 type='class'
)

# train - oof
ndx_1se <- match(cv_lasso$lambda.1se,cv_lasso$lambda)
train_lasso_oofProb_vec <- logistic_f(cv_lasso$fit.preval[,ndx_1se])
train_lasso_oofClass_vec <- ifelse(
   train_lasso_oofProb_vec > 0.5, 'HCC', 'Control')

# test 
test_lasso_predClass_vec <- predict(
 cv_lasso,
 newx=test_lcpm_mtx,
 s='lambda.1se',
 type='class'
)

# enet
##########################
# train - cv predicted
train_enet_predClass_vec <- predict(
 cv_enet,
 newx=train_lcpm_mtx,
 s='lambda.1se',
 type='class'
)

# train - oof
ndx_1se <- match(cv_enet$lambda.1se,cv_enet$lambda)
train_enet_oofProb_vec <- logistic_f(cv_enet$fit.preval[,ndx_1se])
train_enet_oofClass_vec <- ifelse(
   train_enet_oofProb_vec > 0.5, 'HCC', 'Control')

# test
test_enet_predClass_vec <- predict(
 cv_enet,
 newx=test_lcpm_mtx,
 s='lambda.1se',
 type='class'
)



# relaxed lasso (gamma=0)
##########################
# train - cv predicted
train_relaxed_predClass_vec <- predict(
 cv_lassoR,
 g=0,
 newx=train_lcpm_mtx,
 s='lambda.1se',
 type='class'
)

# RECALL: cv_lassoR$nzero[cv_lassoR$lambda==cv_lassoR$lambda.1se]
# train - oof
ndx_1se <- match(cv_lassoR$lambda.1se,cv_lassoR$lambda)
train_relaxed_oofProb_vec <- logistic_f(cv_lassoR$fit.preval[['g:0']][,ndx_1se])
train_relaxed_oofClass_vec <- ifelse(
   train_relaxed_oofProb_vec > 0.5, 'HCC', 'Control')

# test 
test_relaxed_predClass_vec <- predict(
 cv_lassoR,
 g=0,
 newx=test_lcpm_mtx,
 s='lambda.1se',
 type='class'
)


# blended mix (gamma=0.5)
###############################
# train - cv predicted
train_blended_predClass_vec <- predict(
 cv_lassoR,
 g=0.5,
 newx=train_lcpm_mtx,
 s='lambda.1se',
 type='class'
)

# RECALL $`r cv_lassoR$relaxed$nzero.1se`$ features (vertical 
#  cv_blended_statlist <- cv_lassoR$relaxed$statlist[['g:0.5']]
#  cv_blended_1se_error <- cv_blended_statlist$cvm[cv_blended_statlist$lambda==
      #cv_lassoR$relaxed$lambda.1se]

# train - oof
cv_blended_statlist <- cv_lassoR$relaxed$statlist[['g:0.5']]
ndx_1se <- match(cv_lassoR$relaxed$lambda.1se, cv_blended_statlist$lambda)
train_blended_oofProb_vec <- logistic_f(cv_lassoR$fit.preval[['g:0.5']][,ndx_1se])
train_blended_oofClass_vec <- ifelse(
   train_blended_oofProb_vec > 0.5, 'HCC', 'Control')

# test
test_blended_predClass_vec <- predict(
 cv_lassoR,
 g=0.5,
 newx=test_lcpm_mtx,
 s='lambda.1se',
 type='class' 
)

# put it all together
########################
all_models_confustion_mtx <- rbind(
 train_lasso_cv = as.vector(table(train_lasso_predClass_vec, train_group_vec)),
 train_lasso_oof = as.vector(table(train_lasso_oofClass_vec, train_group_vec)),
 test_lasso = as.vector(table(test_lasso_predClass_vec, test_group_vec)),

 train_enet_cv = as.vector(table(train_enet_predClass_vec, train_group_vec)),
 train_enet_oof = as.vector(table(train_enet_oofClass_vec, train_group_vec)),
 test_enet = as.vector(table(test_enet_predClass_vec, test_group_vec)),

 train_relaxed_cv = as.vector(table(train_relaxed_predClass_vec, train_group_vec)),
 train_relaxed_oof = as.vector(table(train_relaxed_oofClass_vec, train_group_vec)),
 test_relaxed = as.vector(table(test_relaxed_predClass_vec, test_group_vec)),

 train_blended_cv = as.vector(table(train_blended_predClass_vec, train_group_vec)),
 train_blended_oof = as.vector(table(train_blended_oofClass_vec, train_group_vec)),
 test_blended = as.vector(table(test_blended_predClass_vec, test_group_vec))
)
colnames(all_models_confustion_mtx) <- c('C:C','C:H','H:C', 'H:H')


all_models_confustionRates_mtx <- sweep(
 all_models_confustion_mtx, 1, rowSums(all_models_confustion_mtx), '/')

all_models_confustionRates_mtx <- cbind(all_models_confustionRates_mtx,
  error = rowSums(all_models_confustionRates_mtx[,2:3]))

knitr::kable(100*all_models_confustionRates_mtx, 
  caption="confusion: Columns are Truth:Predicted",
  digits=1) %>%
  kableExtra::kable_styling(full_width = F)

```

The out-of-fold error rates are larger for the relaxed lasso and blended fit models.
On the test set, errors are slightly higher for the elastic net model.


## Compare predictions at misclassified samples

It is useful to examine classification errors more carefully.
If models have different failure modes, one might get improved
performance by combining model  predictions.  Note that the models
considered here are not expected to compliment each other usefully
as they are too similar in nature.

```{r misclassTrain, cache=F, cache.vars='', fig.height=5, fig.width=8, fig.cap="out-of-fold predicted probabilities at miscassified samples"}

misclass_id_vec <- unique(c(
 names(train_lasso_oofClass_vec)[train_lasso_oofClass_vec!=train_group_vec],
 names(train_enet_oofClass_vec)[train_enet_oofClass_vec!=train_group_vec],
 names(train_relaxed_oofClass_vec)[train_relaxed_oofClass_vec!=train_group_vec],
 names(train_blended_oofClass_vec)[train_blended_oofClass_vec!=train_group_vec]
 )
)


missclass_oofProb_mtx <- cbind(
 train_lasso_oofProb_vec[misclass_id_vec],
 train_enet_oofProb_vec[misclass_id_vec],
 train_relaxed_oofProb_vec[misclass_id_vec],
 train_blended_oofProb_vec[misclass_id_vec]
)
colnames(missclass_oofProb_mtx) <- c('lasso','enet', 'lassoR', 'blended')

row_med_vec <- apply(missclass_oofProb_mtx, 1, median)
missclass_oofProb_mtx <- missclass_oofProb_mtx[
  order(train_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),]

plot(
 x=c(1,nrow(missclass_oofProb_mtx)), xlab='samples',
 y=range(missclass_oofProb_mtx), ylab='out-of-fold predicted probability',
 xaxt='n', type='n')

for(RR in 1:nrow(missclass_oofProb_mtx))
points(
 rep(RR, ncol(missclass_oofProb_mtx)), 
 missclass_oofProb_mtx[RR,],
 col=ifelse(train_group_vec[rownames(missclass_oofProb_mtx)[RR]] == 'Control',
  'green', 'red'),
 pch=1:ncol(missclass_oofProb_mtx))

legend('top', ncol=2, legend=colnames(missclass_oofProb_mtx), 
 pch=1:4, bty='n')

abline(h=0.5)
    
```

As we've seen above, predictions from lassoR  and the blended mix model 
are basically dichotomous; 0 or 1.  Samples have been order by group, and
median P(HCC) within group.  For the Controls (green), predicted probabilities
less than 0.5 are considered correct here.  For the HCC (red) samples,
predicted probabilities greater than 0.5 are considered correct here.

Now look at the same plot on the test data set.


```{r misclassTest, cache=F, cache.vars='', fig.height=5, fig.width=8, fig.cap="Test data predicted probabilities at miscassified samples"}

misclass_id_vec <- unique(c(
 names(test_lasso_predClass_vec[,1])[test_lasso_predClass_vec!=test_group_vec],
 names(test_enet_predClass_vec[,1])[test_enet_predClass_vec!=test_group_vec],
 names(test_relaxed_predClass_vec[,1])[test_relaxed_predClass_vec!=test_group_vec],
 names(test_blended_predClass_vec[,1])[test_blended_predClass_vec!=test_group_vec]
 )
)


missclass_oofProb_mtx <- cbind(
 test_lasso_predProb_vec[misclass_id_vec,],
 test_enet_predProb_vec[misclass_id_vec,],
 test_relaxed_predProb_vec[misclass_id_vec,],
 test_blended_predProb_vec[misclass_id_vec,]
)
colnames(missclass_oofProb_mtx) <- c('lasso','enet', 'lassoR', 'blended')

row_med_vec <- apply(missclass_oofProb_mtx, 1, median)
missclass_oofProb_mtx <- missclass_oofProb_mtx[
  order(test_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),]

plot(
 x=c(1,nrow(missclass_oofProb_mtx)), xlab='samples',
 y=range(missclass_oofProb_mtx), ylab='out-of-fold predicted probability',
 xaxt='n', type='n')

for(RR in 1:nrow(missclass_oofProb_mtx))
points(
 rep(RR, ncol(missclass_oofProb_mtx)), 
 missclass_oofProb_mtx[RR,],
 col=ifelse(test_group_vec[rownames(missclass_oofProb_mtx)[RR]] == 'Control',
  'green', 'red'),
 pch=1:ncol(missclass_oofProb_mtx))

legend('top', ncol=2, legend=colnames(missclass_oofProb_mtx), 
 pch=1:4, bty='n')

abline(h=0.5)
    
```

The relaxed lasso fit results in essentially dichotomized predicted probability
distribution - predicted probabilities are very close to 0 or 1.

We see that for design points in the training set, the predicted probabilies from the relaxed lasso
are  essentially dichotomized to be tightly distributed at the extremes of the
response range.  For design points in the test set, the predicted probabilies from the relaxed lasso
are comparable to the lasso model predicted porbabilities.  This seems to indicate over-fitting
in the relaxed lasso fit.



## Compare coefficient profiles

```{r compCoeffProf, cache=F, cache.vars='', fig.height=6, fig.width=8, fig.cap="Coefficient Profiles"}

# lasso 
##########################
# train - cv predicted
lasso_coef <- coef(
 cv_lasso,
 s='lambda.1se'
)
lasso_coef_frm <- data.frame(
 gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])],
 lasso=lasso_coef@x)


# enet
##########################
enet_coef <- coef(
 cv_enet,
 s='lambda.1se'
)
enet_coef_frm <- data.frame(
 gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])],
 enet=enet_coef@x)

# THESE ARE NOT CORRECT - SKIP
# relaxed lasso (gamma=0)
##########################
SKIP <- function() {
lassoR_coef <- coef(
 cv_lassoR,
 s='lambda.1se',
 g=0
)
lassoR_coef_frm <- data.frame(
 gene=lassoR_coef@Dimnames[[1]][c(1, lassoR_coef@i[-1])],
 lassoR=lassoR_coef@x)
}

# blended mix (gamma=0.5)
###############################
blended_coef <- coef(
 cv_lassoR,
 s='lambda.1se',
 g=0.5
)
blended_coef_frm <- data.frame(
 gene=blended_coef@Dimnames[[1]][c(1, blended_coef@i[-1])],
 blended=blended_coef@x)


# put it all together
all_coef_frm <- 
 base::merge(
 x = lasso_coef_frm, 
 y = base::merge(
     x = enet_coef_frm,
     y = blended_coef_frm,
         by='gene', all=T),
 by='gene', all=T)

# SKIPPED
#base::merge(
         #x = lassoR_coef_frm,
         #y = blended_coef_frm,
         #by='gene', all=T),

all_coef_frm[,-1][is.na(all_coef_frm[,-1])] <- 0

par(mfrow=c(ncol(all_coef_frm)-1,1), mar=c(0,5,0,1), oma=c(3,1,2,0))

for(CC in 2:ncol(all_coef_frm)) {
 plot(
  x=1:(nrow(all_coef_frm)-1), xlab='', 
  y=all_coef_frm[-1, CC], ylab=colnames(all_coef_frm)[CC],
  type='h', xaxt='n')
}


```

Note that there is little difference between the elastic net and the lasso
in the selected features, and when the coefficient is zero in one set, it 
is smaell in the other.  By contrast, the blended fit produces more shrinkage.

```{r zreros, fig.cap=''}

knitr::kable(
with(all_coef_frm[,-1], table(lassoZero=lasso==0, enetZero=enet==0)),
 caption='Zero Ceofficient: rows are lasso, columns enet') %>%
  kableExtra::kable_styling(full_width = F)

```



<!--
Coefficients in the relaxed lasso fit are much larger than those in the
lasso fit, or zero.  As a consequence, the blended fit coefficients look 
like a shrunken version of the relaxed lasso fit coefficients.  
-->

Coefficients in the blended fit are larger than those in the
lasso fit, or zero.  


We can also examine these with a scatter plot matrix.

```{r pairsCoeffProf, cache=F, cache.vars='', fig.height=6, fig.width=8, fig.cap="Coefficients from fits"}


pairs(all_coef_frm[-1,-1],
  lower.panel = NULL,
  panel = function(x, y) {
    points(x, y, pch = 16, col = "blue")
  }
)

```


## Examine feature selection

Recall from `glmnet` vignette:

```
It is known that the ridge penalty shrinks the coefficients of correlated predictors
towards each other while the lasso tends to pick one of them and discard the others.
The elastic-net penalty mixes these two; if predictors are correlated in groups,
an $\alpha$=0.5 tends to select the groups in or out together.
This is a higher level parameter, and users might pick a value upfront,
else experiment with a few different values. One use of $\alpha$ is for numerical stability;
for example, the *elastic net with $\alpha = 1 - \epsilon$ for some small $\epsilon$>0
performs much like the lasso, but removes any degeneracies and wild behavior caused
by extreme correlations*.
```

To see how this plays out in this dataset, we can look at feature expression
heat maps.  


```{r heatmapLasso, cache=T, cache.vars='', fig.height=6, fig.width=8, fig.cap="Lasso Model Genes"}
 suppressPackageStartupMessages(require(gplots))

# train - cv predicted
lasso_coef <- coef(
 cv_lasso,
 s='lambda.1se'
)
lasso_coef_frm <- data.frame(
 gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])],
 lasso=lasso_coef@x)

 
  Mycol <- colorpanel(1000, "blue", "red")
  heatmap.2(
    x=t(train_lcpm_mtx[,lasso_coef_frm$gene[-1]]),
    scale="row",
    labRow=lasso_coef_frm$gene,
    labCol=train_group_vec,
    col=Mycol, 
    trace="none", density.info="none", 
    #margin=c(8,6), lhei=c(2,10), 
    #lwid=c(0.1,4), #lhei=c(0.1,4)
    key=F,
    ColSideColors=ifelse(train_group_vec=='Control', 'green','red'),
    dendrogram="both",
    main=paste('lasso genes - N =', nrow(lasso_coef_frm)-1))

```

```{r heatmapEnet, cache=T, cache.vars='', fig.height=6, fig.width=8, fig.cap="Enet Model Genes"}
 suppressPackageStartupMessages(require(gplots))

# train - cv predicted
enet_coef <- coef(
 cv_enet,
 s='lambda.1se'
)
enet_coef_frm <- data.frame(
 gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])],
 enet=enet_coef@x)

 
  Mycol <- colorpanel(1000, "blue", "red")
  heatmap.2(
    x=t(train_lcpm_mtx[,enet_coef_frm$gene[-1]]),
    scale="row",
    labRow=enet_coef_frm$gene,
    labCol=train_group_vec,
    col=Mycol, 
    trace="none", density.info="none", 
    #margin=c(8,6), lhei=c(2,10), 
    #lwid=c(0.1,4), #lhei=c(0.1,4)
    key=F,
    ColSideColors=ifelse(train_group_vec=='Control', 'green','red'),
    dendrogram="both",
    main=paste('enet genes - N =', nrow(enet_coef_frm)-1))

```


<!--chapter:end:00-glmnetFits.Rmd-->

# Fitted Model Suite {#model-suite}

We now examine the results of fitting a suite of models to
investigate the effect of sample size on 
various aspects of model performance:  

* assessed accuracy: out-of-fold estimates of precision and variability  and
cv assessed accuracy and bias.

* selected feature profile stability - to what extent does the
feature set implicitly selected by the lasso vary across random
sampling and what is the effect of sample size.

It is hypothesized that below a certain threshold,
sample sizes are too small to provide reliable estimates
of performance or stable selected feature profiles.  

We will attempt to separate variability which is due to
sample size from variability due to sample composition.
To to this we will track sample quality.
Predicted probabilities from fitted model can be transformed into sample
quality scores: $Q_i = p_i^{y_i}(1-p_i)^{1-y_i}$, where $p_i$ is the
estimated probability of HCC for sample i and $y_i$ is 1 for HCC samples and
0 for Controls.  ie. we use the fitted sample contribution to the
likelihood function as a sample quality score.  To derive the quality scores,
we will use the predicted response from a lasso model fitted to the entire data set.

Hard to classify samples will have low quality scores.
In the results that we discuss below, when we look at variability across repeated 
random sampling of different sizes, we can use sample quality scores to investigate 
how much of the variability is due to sample selection.
Note that quality here is not used to say anything about the sample data quality.
Low quality here only means that a sample is different from the 
core of the data set in a way that makes it hard to properly classify.
That could happen if the sample were mislabeled, in which case we could 
think of this sample as being poor quality of course.

## Sample quality scores

To get sample quality scores, fit a lasso model, extracted predicted probabilities and
convert to a quality score.

```{r lasso-fit-all, cache=T, cache.vars=c('all_lcpm_mtx', 'all_group_vec','cv_lassoAll')}

# combine train and test 
all_lcpm_mtx <- rbind(train_lcpm_mtx, test_lcpm_mtx)

# we have to be careful with factors!
# We'll keep as a character and change to factor when needed
all_group_vec <- c(
 as.character(train_group_vec), 
 as.character(test_group_vec)
)
# I suspect adding names to vectors breaks one of the tidy commandments,
# but then again I am sure I have already offended the creed beyond salvation
names(all_group_vec) <- c(
 names(train_group_vec),
 names(test_group_vec)
)

start_time <-  proc.time()

# since we have not set the foldid, set seed here
set.seed(1)
cv_lassoAll <- glmnet::cv.glmnet(
 x = all_lcpm_mtx,
 y = factor(all_group_vec,levels = c('Control', 'HCC')),
 alpha = 1,
 family = 'binomial',
 type.measure  =  "class",
 keep = F,
 nlambda = 100
)

message("lassoAll time: ", round((proc.time() - start_time)[3],2),"s")

```

We can examine the fit.

```{r look-lassoAll, as.is=T, comments='', cache=T, cache.vars=''}

cv_lassoAll

```

```{r plot-lassoAll, cache=T, cache.vars='', fig.height=5, fig.width=6, fig.cap='lasso model fitted to all samples'}

plot(cv_lassoAll)

```

More features are now selected, but the cv error rate is comparable to the results
of the lasso fit to the training set.  We can use this fit to compute sample
quality scores.  We will use the minimum lambda model to provide
the fitted probabilities.

```{r get-sample-qual, cache=T, cache.vars=c('sample_qual_vec','lassoAll_conf_vec', 'lassoAll_conf_mtx')}

# predicted probs
lassoAll_predResp_vec <- predict(
 cv_lassoAll,
 newx = all_lcpm_mtx,
 s = "lambda.min",
 type = "resp"
 )

# also get predicted class
lassoAll_predClass_vec <- predict( 
 cv_lassoAll,
 newx = all_lcpm_mtx,
 s = "lambda.min",
 type = "class"
 )

# get qual scores

# Note here that factors are a little but of a pain.
y <- as.numeric(all_group_vec == 'HCC')
p <- lassoAll_predResp_vec
sample_qual_vec <- p^y*(1-p)^(1-y)
sample_qual_vec <- sample_qual_vec[,1] # to drop the matrix structure

# Look at quality scores as a finction of classification
lassoAll_conf_vec <- paste(
 y, 
 as.numeric(lassoAll_predClass_vec=='HCC'),
 sep = ':'
)

lassoAll_conf_mtx <- table(
 truth = all_group_vec,
 pred = lassoAll_predClass_vec
)

```

```{r print-conf, fig.cap=''}

knitr::kable(lassoAll_conf_mtx,
  caption = "Rows are truth.  Columns are predicted") %>%
   kableExtra::kable_styling(full_width = F)

```


The model makes few errors (although in most liquid biopsy applications,
this error rate would arguably be too high).  We can examine how quality
varies among classification groups.

```{r plot-qual-conf, cache=T, cache.vars='', fig.height=5, fig.width=5, fig.cap='quality scores by classification - HCC=1'}

gplots::boxplot2(split(sample_qual_vec, lassoAll_conf_vec), ylab = 'Quality Score')
title(sub = 'Classification - Truth:Predicted')

```

## Simulation Design

We are now ready to run the simulations.

```{r simParms, cahce=F}
 SIM <- 30
 SIZE <- c(25, 50, 100, 200, 300)
 CV_REP <- 30

```

Simluation parameters:  

* Number of simulations : SIM = $`r SIM`$

* Sample sizes: SIZE = $`r SIZE`$  

* Number of CV Replicates:  CV_REP = $`r CV_REP`$


We will repeat the simulation process SIM = $`r SIM`$ times.
For each simulation iteration, we will select $`r max(SIZE)`$ Control and 
$`r max(SIZE)`$ HCC samples at random.  Models will be fitted and analyzed
to balanced subsets of sise SIZE = $`r SIZE`$, in a telescopic manner to
emulated a typical sample accrual process.  Note that in this accural process
there is no time effect - the accrual process is completely randomized.  In practice,
there could be significant time effects.  For example, the first 25 HCC samples could come
from Center A, while the next 25 could come from Center B.  In other words,
there is no batch effect or shared variability in our simulation,
while these are almost always present in real data, including 
batch effects that are associated with class labels - controls being in
different batches than affected samples is an all too common occurence,
for example.  One should be especially watchful of potential batch effects
when dealing with blood samples as blood is notoriously finicky in
character [@Huang:2017aa; @Permenter:2015aa;].
Presented with results that look impressively good based on a small data set,
one should definitely be skeptical of the promise of future equally good results.

For a given simulation and a given sample size, we will obtain
CV_REP = $`r CV_REP`$ cross-validated lasso fits.  From these fits,
we can obtain $`r CV_REP`$ out-of-fold assessments of classification accuracy 
to get a sense if its variability. From each cv replicate, we also obtain
an estimated model size and a set of selected features.  We will want
to examine how these stabilize as the sample size increases.


## Setup simulation 

To setup the simulation, we only need two master tables: one for the selection of Controls
and one for the selection of HCC samples.

```{r get-all-vec, cache=T, cache.vars=c('all_control_vec', 'all_affected_vec')}

all_control_vec <- names(all_group_vec[all_group_vec=='Control']) 
all_affected_vec <- names(all_group_vec[all_group_vec=='HCC'])  

```

We have $`r length(all_control_vec)`$ IDs of control samples in stored in `all_control_vec`
and $`r length(all_affected_vec)`$ IDs of affected samples in stored in `all_affected_vec`.
To create random samples from these we only need to randomly select indices from
each vector.

  
```{r getSimTable, cache=T, cache.vars=c('sim_control_mtx', 'sim_affected_mtx')}

set.seed(12379)

sim_control_mtx <- sapply(
 1:SIM, 
 function(dummy) 
   sample(1:length(all_control_vec), size =  max(SIZE))
)


sim_affected_mtx <- sapply(
 1:SIM, 
 function(dummy) 
   sample(1:length(all_affected_vec), size =  max(SIZE))
)


```

Each simulation is specified by a given column of the simulation design matrices:
`sim_control_mtx` and `sim_affected_mtx`.  Within each simulation, we can run
the analyses of size $`r SIZE`$.

We can examine how much variability we have in the quality scores of the selected samples.

```{r look-sim-qual, cache=T, cache.vars=c('sim_control_qual_mtx', 'sim_affected_qual_mtx'), fig.height=8, fig.width=10, fig.cap='sample quality by simulation run'}

all_control_qual_vec <- sample_qual_vec[all_control_vec]
sim_control_qual_mtx <- sapply(
  1:ncol(sim_control_mtx), 
  function(CC) all_control_qual_vec[sim_control_mtx[,CC]]
 )

all_affected_qual_vec <- sample_qual_vec[all_affected_vec]
sim_affected_qual_mtx <- sapply(
  1:ncol(sim_affected_mtx),  
  function(CC) all_affected_qual_vec[sim_affected_mtx[,CC]]
 )

# Get stage from SIZE 
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)

sim_control_qual_byStage_lst <- do.call('c', 
 lapply(1:ncol(sim_control_qual_mtx), 
  function(CC) c(split(sim_control_qual_mtx[,CC], stage_vec),NA)
 )
)

sim_affected_qual_byStage_lst <- do.call('c', 
 lapply(1:ncol(sim_affected_qual_mtx), 
  function(CC) c(split(sim_affected_qual_mtx[,CC], stage_vec),NA)
 )
)

# PLOT
par(mfrow=c(2,1), mar = c(2,5,2,1))
# control
boxplot(
  sim_control_qual_byStage_lst, 
  outline = F, 
  border = 1:6,
  ylab = 'Quality Score',
  xaxt = 'n'
)
legend('bottomright', title = 'Stage', ncol = 2,
 legend = names(sim_control_qual_byStage_lst[1:5]), 
 text.col = 1:5,
 bty = 'n', horiz = F
)
sim_ndx <- which(names(sim_control_qual_byStage_lst) =='')
abline(v = sim_ndx, col = 'grey')
axis(
  side = 1, 
  at = sim_ndx-2, 
  label = 1:length(sim_ndx),
  tick = F, 
  line = -1, las = 2,
  cex.axis = 0.8)
title("Control sample quality by stage and simulation")

# affected
boxplot(
  sim_affected_qual_byStage_lst, 
  outline = F, 
  border = 1:6,
  ylab = 'Quality Score',
  xaxt = 'n'
)
sim_ndx <- which(names(sim_affected_qual_byStage_lst)=='')
abline(v = which(names(sim_affected_qual_byStage_lst)==''), col = 'grey')
axis(
  side=1, 
  at = sim_ndx-2,        
  label = 1:length(sim_ndx),
  tick = F, 
  line = -1, las = 2,
  cex.axis = 0.8)
title("Affected sample quality by stage and simulation")

```

We see some variability in sample quality in the smaller analysis stages.
This may lead observers to be overly optimistic, or overly pessimistic,
in the early accrual stages.  

## Run simulations


As these make take a while to run, and space to store,
we will save the results of each similation to a different
object and store to disk.


The simulation saves results to the file system and
only needs to be run once.  The simulation takes $\approx$ 8 minutes
per iteration, or 4 hours of run time on a laptop.
(Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Mojave 10.14.6)

```{r runSim, cache=T, cache.vars='', eval=F}

# Get stage from SIZE
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0, SIZE), include.lowest = T)

for (SIMno in 1:ncol(sim_control_qual_mtx)) {
  start_time <- proc.time()

  cat("Running simulation ", SIMno, "\n")

  sim_cv_lst <- lapply(1:length(levels(stage_vec)), function(STGno) {
    Stage_rows_vec <- which(stage_vec %in% levels(stage_vec)[1:STGno])
    #cat("Stage ", STGno, "- analyzing", length(Stage_rows_vec), "paired samples.\n")

    sim_stage_samples_vec <- c(
      all_control_vec[sim_control_mtx[Stage_rows_vec, SIMno]],
      all_affected_vec[sim_affected_mtx[Stage_rows_vec, SIMno]]
    )
    sim_stage_lcpm_mtx <- all_lcpm_mtx[sim_stage_samples_vec, ]
    sim_stage_group_vec <- all_group_vec[sim_stage_samples_vec]
    print(table(sim_stage_group_vec))

    sim_stage_cv_lst <- lapply(1:CV_REP, function(CV) {
      cv_fit <- glmnet::cv.glmnet(
        x = sim_stage_lcpm_mtx,
        y = sim_stage_group_vec,
        alpha = 1,
        family = "binomial",
        type.measure = "class",
        keep = T,
        nlambda = 30
      )
      ndx_1se <- which(cv_fit$lambda == cv_fit$lambda.1se)

      nzero_1se <- cv_fit$nzero[ndx_1se]
      cvm_1se <- cv_fit$cvm[ndx_1se]

      # oof error
      oofPred_1se_vec <- ifelse(
        cv_fit$fit.preval[, ndx_1se] > 0.5, "HCC", "Control"
      )
      oofPred_1se_error <- mean(oofPred_1se_vec != sim_stage_group_vec)

      # test error
      sim_stage_test_samples_vec <- setdiff(rownames(all_lcpm_mtx), sim_stage_samples_vec)
      sim_stage_test_lcpm_mtx <- all_lcpm_mtx[sim_stage_test_samples_vec,]
      sim_stage_test_group_vec <- all_group_vec[sim_stage_test_samples_vec]

      test_pred_1se_vec <- predict(
       cv_fit,
       newx=sim_stage_test_lcpm_mtx,
       s="lambda.1se",
       type="class"
      )
      test_1se_error <- mean(test_pred_1se_vec != sim_stage_test_group_vec)

      # genes
      coef_1se <- coef(
        cv_fit,
        s = "lambda.1se"
      )
      genes <- coef_1se@Dimnames[[1]][coef_1se@i[-1]]

      list(p = nzero_1se, cv_error = cvm_1se, oof_error = oofPred_1se_error, 
           test_error=test_1se_error, genes = genes)
    })
    sim_stage_cv_lst
  })

  # save  sim_cv_lst
  fName <- paste0("sim_", SIMno, "_cv_lst")
  assign(fName, sim_cv_lst)
  save(list = fName, file=file.path("RData", fName))

  message("simulation time: ", round((proc.time() - start_time)[3], 2), "s")
}

```


## Simulation results


* error: cv, oof, test
    - examine relationship to each other and with respect to sample size
* 


First look at results for one simulation.
```{r simRes-Sim1, cache=T, cache.vars='what-to-keep', fig.heigth=5, fig.width=8, fig.cap='fig cap'}

sim_files_vec <- list.files('RData', '^sim_')

JJ <- 1
load(file=file.path('RData', sim_files_vec[JJ]))
assign('sim_cv_lst', get(sim_files_vec[JJ]))
rm(list=sim_files_vec[JJ])


# define extraction methods
siz1_sim_cv_lst <-  sim_cv_lst[[1]]

cvList2frm_f <- function(cv_lst) {
 frm1 <- as.data.frame(t(sapply(cv_lst, function(x) x)))
 frm2 <- data.frame(
  unlist(frm1[[1]]),
  unlist(frm1[[2]]),
  unlist(frm1[[3]]),
  unlist(frm1[[4]]),
  frm1[5])
  names(frm2) <- names(frm1)
  frm2}

cvList2frm_f(sim_cv_lst[[1]])[1:5,]

sim_cv_frm <- do.call('rbind', lapply(1:length(sim_cv_lst),
  function(JJ) {
    siz_frm <- cvList2frm_f(sim_cv_lst[[JJ]])
    data.frame(Size=JJ, siz_frm)
  }))





par(mfrow=c(5,1), mar=c(1,3,2,1), oma=c(2,2,1,0))
for(JJ in 1:length(sim_cv_lst)){
   boxplot(extractCvData_f(sim_cv_lst[[JJ]])[,c('cv_error','oof_error', 'test_error')],
   xaxt='n')
   title(paste('Size -', SIZE[JJ]))
}
   


summary(siz1_frm2[, c('cv_error','oof_error', 'test_error')])
boxplot(siz1_frm2$p)

boxplot(siz1_sim_cv_frm$cv_error)


for(SIZ in 1:length(sim_cv_lst)){
  



<!--chapter:end:00-glmnetSuite.Rmd-->

# Variable importance  {#variable-importance}

Cai et al. [@Cai:2019aa] used the fequency of selection across
bootstrap replicates to select features form their final model.
We sus[ect that this simply selects features with large coefficients
which would correspond the the variable importance metric
used in the `caret` package.  In this section we measure
variable importance as the loss in performance when the variable is left out
of the model.  Depending on the correlation structure, it is quite possible
for a feature to have a large coefficient and be un-important, in our sense of the word.
It is also possible for a feature to be frequently selected in bootstrap samples, and
still not un-important.  In a sense, this measure of importance is really a measure
of `single importance`.  We should therefore also have a measure of `group importance`.

Other questions ...


<!--chapter:end:00-variableImportance.Rmd-->

# Conclusions {#conclusions}

We have found that ...

Other questions ...


<!--chapter:end:00-summary.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:00-references.Rmd-->

