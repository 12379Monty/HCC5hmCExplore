<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma</title>
  <meta name="description" content="Data from Cai et al. (2019) paper are explored" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Data from Cai et al. (2019) paper are explored" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  
  <meta name="twitter:description" content="Data from Cai et al. (2019) paper are explored" />
  

<meta name="author" content="Francois Collin" />


<meta name="date" content="2020-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preproc.html"/>
<link rel="next" href="model-suite.html"/>
<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script type="text/javascript">
// source:https://stackoverflow.com/questions/45360998/code-folding-in-bookdown
// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preamble</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="modeling-background.html"><a href="modeling-background.html"><i class="fa fa-check"></i><b>2</b> Modeling - Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modeling-background.html"><a href="modeling-background.html#predictive-modeling-for-genomic-data"><i class="fa fa-check"></i><b>2.1</b> Predictive modeling for genomic data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="modeling-background.html"><a href="modeling-background.html#caret-for-model-evaluation"><i class="fa fa-check"></i><b>2.1.1</b> caret for model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="modeling-background.html"><a href="modeling-background.html#glmnet"><i class="fa fa-check"></i><b>2.2</b> glmnet</a>
<ul>
<li><a href="modeling-background.html#alpha-hyper-parameter"><strong>alpha</strong> hyper-parameter</a></li>
<li class="chapter" data-level="" data-path="modeling-background.html"><a href="modeling-background.html#signal-to-noise-ratio"><i class="fa fa-check"></i>Signal-to-noise Ratio</a></li>
<li class="chapter" data-level="" data-path="modeling-background.html"><a href="modeling-background.html#lasso-vs-best-subset"><i class="fa fa-check"></i>Lasso vs Best Subset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="preproc.html"><a href="preproc.html"><i class="fa fa-check"></i><b>3</b> Preprocessing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="preproc.html"><a href="preproc.html#load-the-data"><i class="fa fa-check"></i><b>3.1</b> Load the data</a></li>
<li class="chapter" data-level="3.2" data-path="preproc.html"><a href="preproc.html#dra"><i class="fa fa-check"></i><b>3.2</b> Differential representation analysis</a>
<ul>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#remove-lowly-expressed-genes"><i class="fa fa-check"></i>Remove lowly expressed genes</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#creating-a-design-matrix-and-contrasts"><i class="fa fa-check"></i>Creating a design matrix and contrasts</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#removing-heteroscedasticity-from-the-count-data"><i class="fa fa-check"></i>Removing heteroscedasticity from the count data</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#fit-linear-models-and-examine-the-results"><i class="fa fa-check"></i>Fit linear models and examine the results</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#graphical-representations-of-de-results-md-plots"><i class="fa fa-check"></i>Graphical representations of DE results: MD Plots</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#de-genes-at-10-fold-change"><i class="fa fa-check"></i>DE genes at 10% fold change</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="preproc.html"><a href="preproc.html#snr-regime"><i class="fa fa-check"></i><b>3.3</b> Signal-to-noise ratio regime</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="explore-sparsity.html"><a href="explore-sparsity.html"><i class="fa fa-check"></i><b>4</b> The bet on sparsity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#cv-analysis-setup"><i class="fa fa-check"></i><b>4.1</b> CV analysis setup</a></li>
<li class="chapter" data-level="4.2" data-path="explore-sparsity.html"><a href="explore-sparsity.html#fit-and-compare-models"><i class="fa fa-check"></i><b>4.2</b> Fit and compare models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#logistic-regression-in-glmnet"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression in <code>glmnet</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="explore-sparsity.html"><a href="explore-sparsity.html#relaxed-lasso-and-blended-mix"><i class="fa fa-check"></i><b>4.3</b> Relaxed lasso and blended mix</a></li>
<li class="chapter" data-level="4.4" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examination-of-sensitivity-vs-specificity"><i class="fa fa-check"></i><b>4.4</b> Examination of sensitivity vs specificity</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#training-data-out-of-fold-roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> Training data out-of-fold ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-predictions-at-misclassified-samples"><i class="fa fa-check"></i><b>4.5</b> Compare predictions at misclassified samples</a></li>
<li class="chapter" data-level="4.6" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-coefficient-profiles"><i class="fa fa-check"></i><b>4.6</b> Compare coefficient profiles</a></li>
<li class="chapter" data-level="4.7" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examine-feature-selection"><i class="fa fa-check"></i><b>4.7</b> Examine feature selection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-suite.html"><a href="model-suite.html"><i class="fa fa-check"></i><b>5</b> Fitted Model Suite</a>
<ul>
<li class="chapter" data-level="5.1" data-path="model-suite.html"><a href="model-suite.html#sample-quality-scores"><i class="fa fa-check"></i><b>5.1</b> Sample quality scores</a></li>
<li class="chapter" data-level="5.2" data-path="model-suite.html"><a href="model-suite.html#simulation-design"><i class="fa fa-check"></i><b>5.2</b> Simulation Design</a></li>
<li class="chapter" data-level="5.3" data-path="model-suite.html"><a href="model-suite.html#setup-simulation"><i class="fa fa-check"></i><b>5.3</b> Setup simulation</a></li>
<li class="chapter" data-level="5.4" data-path="model-suite.html"><a href="model-suite.html#run-simulations"><i class="fa fa-check"></i><b>5.4</b> Run simulations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>6</b> Variable importance</a></li>
<li class="chapter" data-level="7" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>7</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="LICENSE.md" style="font:em;" target="blank">Copyright (c) 2020 Francois Collin</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="explore-sparsity" class="section level1" number="4">
<h1><span class="header-section-number">Section 4</span> The bet on sparsity</h1>
<p>In this section we explore various fits that can be computed
and analyzed with tools provided in the <code>glmnet</code> package.
Refer to the <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">Glmnet Vignette</a>
for a quick reference guide.</p>
<div id="cv-analysis-setup" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> CV analysis setup</h2>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="explore-sparsity.html#cb34-1"></a>K_FOLD &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb34-2"><a href="explore-sparsity.html#cb34-2"></a>trainP &lt;-<span class="st"> </span><span class="fl">0.8</span></span>
<span id="cb34-3"><a href="explore-sparsity.html#cb34-3"></a>EPS &lt;-<span class="st"> </span><span class="fl">0.05</span>    <span class="co"># Have no idea what &quot;small&quot; epsilon means</span></span></code></pre></div>
<p>First we divide the analysis dataset into <code>train</code> and <code>test</code> in a <span class="math inline">\(4\)</span>:1 ratio.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="explore-sparsity.html#cb35-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb35-2"><a href="explore-sparsity.html#cb35-2"></a>train_sampID_vec &lt;-<span class="st"> </span><span class="kw">with</span>(AF_dgel<span class="op">$</span>samples,</span>
<span id="cb35-3"><a href="explore-sparsity.html#cb35-3"></a>AF_dgel<span class="op">$</span>samples<span class="op">$</span>sampID[caret<span class="op">::</span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>group, <span class="dt">p=</span>trainP, <span class="dt">list=</span>F)]</span>
<span id="cb35-4"><a href="explore-sparsity.html#cb35-4"></a>)</span>
<span id="cb35-5"><a href="explore-sparsity.html#cb35-5"></a></span>
<span id="cb35-6"><a href="explore-sparsity.html#cb35-6"></a>test_sampID_vec &lt;-<span class="st"> </span><span class="kw">with</span>(AF_dgel<span class="op">$</span>samples,</span>
<span id="cb35-7"><a href="explore-sparsity.html#cb35-7"></a><span class="kw">setdiff</span>(sampID, train_sampID_vec)</span>
<span id="cb35-8"><a href="explore-sparsity.html#cb35-8"></a>)</span>
<span id="cb35-9"><a href="explore-sparsity.html#cb35-9"></a></span>
<span id="cb35-10"><a href="explore-sparsity.html#cb35-10"></a>train_group_vec &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[train_sampID_vec, <span class="st">&#39;group&#39;</span>]</span>
<span id="cb35-11"><a href="explore-sparsity.html#cb35-11"></a><span class="kw">names</span>(train_group_vec) &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[train_sampID_vec, <span class="st">&#39;sampID&#39;</span>]</span>
<span id="cb35-12"><a href="explore-sparsity.html#cb35-12"></a></span>
<span id="cb35-13"><a href="explore-sparsity.html#cb35-13"></a>test_group_vec &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[test_sampID_vec, <span class="st">&#39;group&#39;</span>]</span>
<span id="cb35-14"><a href="explore-sparsity.html#cb35-14"></a><span class="kw">names</span>(test_group_vec) &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[test_sampID_vec, <span class="st">&#39;sampID&#39;</span>]</span>
<span id="cb35-15"><a href="explore-sparsity.html#cb35-15"></a></span>
<span id="cb35-16"><a href="explore-sparsity.html#cb35-16"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">table</span>(train_group_vec),</span>
<span id="cb35-17"><a href="explore-sparsity.html#cb35-17"></a>  <span class="dt">caption=</span><span class="st">&quot;Train set&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-18"><a href="explore-sparsity.html#cb35-18"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainVal">Table 4.1: </span>Train set
</caption>
<thead>
<tr>
<th style="text-align:left;">
train_group_vec
</th>
<th style="text-align:right;">
Freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
623
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
444
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="explore-sparsity.html#cb36-1"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">table</span>(test_group_vec),</span>
<span id="cb36-2"><a href="explore-sparsity.html#cb36-2"></a>  <span class="dt">caption=</span><span class="st">&quot;Test set&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb36-3"><a href="explore-sparsity.html#cb36-3"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainVal">Table 4.1: </span>Test set
</caption>
<thead>
<tr>
<th style="text-align:left;">
test_group_vec
</th>
<th style="text-align:right;">
Freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
155
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
111
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="explore-sparsity.html#cb37-1"></a>train_lcpm_mtx &lt;-<span class="st"> </span><span class="kw">t</span>(lcpm_mtx[,train_sampID_vec])</span>
<span id="cb37-2"><a href="explore-sparsity.html#cb37-2"></a>test_lcpm_mtx &lt;-<span class="st"> </span><span class="kw">t</span>(lcpm_mtx[,test_sampID_vec])</span></code></pre></div>
<p>We explore some glmnet fits and the “bet on sparsity”</p>
<p>We consider three models, specified by the value of the
<strong>alpha</strong> parameter in the elastic net parametrization:<br />
- lasso: <span class="math inline">\(\alpha = 1.0\)</span> - sparse models<br />
- ridge <span class="math inline">\(\alpha = 0\)</span> - shrunken coefficients models
- elastic net: <span class="math inline">\(\alpha = 0.5\)</span> - semi sparse model
<!-- - lassoC: $\alpha = 1-\epsilon =$ $0.95$ - lasso for correlated predictors  --></p>
<p>Some questions of interest include:
* How sparse is the model undelying best 5hmC classifier for Early HCC vs Control?</p>
<!--
    - The exploratory analysis done in the preprocessing step
indicates that differential gene body 5hmC representation effects are small.
With reasonably large sample sizes we would expect many genes to be selected.
???
-->
<ul>
<li><p>Does the relaxed lasso improve performance in this case?</p></li>
<li><p>Does the shrunken relaxed lasso (aka the blended mix) improve performance</p></li>
<li><p>Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set?</p></li>
</ul>
<p>In this analysis, we will only evaluate models in terms of
model size, stability and performance. We leave the question
of significance testing of hypotheses about model parameters
completely out. See Lockhart et al. (2014) <span class="citation">[<a href="references.html#ref-Lockhart:2014aa" role="doc-biblioref">24</a>]</span>
and Wassermam (2014) <span class="citation">[<a href="references.html#ref-Wasserman:2014aa" role="doc-biblioref">25</a>]</span> for a discussion of this topic.</p>
<p>In this section we look at the relative performance and size of the models
considered. The effect of the size of the sample set on the level and
stability of performance will be investigated in the next section.</p>
<hr />
<p>First we create folds for <span class="math inline">\(10\)</span>-fold cross-validation of models fitted to
training data. We’ll use caret::createFolds to assign samples
to folds while keeping the outcome ratios constant across folds.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="explore-sparsity.html#cb38-1"></a><span class="co"># This is too variable, both in terms of fold size And composition</span></span>
<span id="cb38-2"><a href="explore-sparsity.html#cb38-2"></a><span class="co">#foldid_vec &lt;- sample(1:10, size=length(train_group_vec), replace=T)</span></span>
<span id="cb38-3"><a href="explore-sparsity.html#cb38-3"></a></span>
<span id="cb38-4"><a href="explore-sparsity.html#cb38-4"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb38-5"><a href="explore-sparsity.html#cb38-5"></a>train_foldid_vec &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createFolds</span>(</span>
<span id="cb38-6"><a href="explore-sparsity.html#cb38-6"></a> <span class="kw">factor</span>(train_group_vec), </span>
<span id="cb38-7"><a href="explore-sparsity.html#cb38-7"></a> <span class="dt">k=</span>K_FOLD,</span>
<span id="cb38-8"><a href="explore-sparsity.html#cb38-8"></a> <span class="dt">list=</span>F)</span>
<span id="cb38-9"><a href="explore-sparsity.html#cb38-9"></a></span>
<span id="cb38-10"><a href="explore-sparsity.html#cb38-10"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">sapply</span>(<span class="kw">split</span>(train_group_vec, train_foldid_vec), </span>
<span id="cb38-11"><a href="explore-sparsity.html#cb38-11"></a>  table), <span class="dt">caption=</span><span class="st">&quot;training samples fold composition&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb38-12"><a href="explore-sparsity.html#cb38-12"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainFolds">Table 4.2: </span>training samples fold composition
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
<th style="text-align:right;">
7
</th>
<th style="text-align:right;">
8
</th>
<th style="text-align:right;">
9
</th>
<th style="text-align:right;">
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
44
</td>
</tr>
</tbody>
</table>
<p>Note that the folds identify samples that are left-out of the training
data for each fold fit.</p>
</div>
<div id="fit-and-compare-models" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Fit and compare models</h2>
<p><code>glmnet</code> provides cross-validation methods to pick the parameter <strong>lambda</strong> which
controls to size of the penalty function. The “one standard error rule”
produces a model with fewer predictors then the minimum cv error model.
On the training data, this usually results in increased MSE and more
biased parameter estimates
(see Engebretsen et al. (2019) <span class="citation">[<a href="references.html#ref-Engebretsen:2019aa" role="doc-biblioref">26</a>]</span> for example).
The question of iterest though is the performance on unseen data; not on the training data.
In the analysis below, we compare the cv error rates with out-of-fold and
test set error rates. The results show that out-of-fold error rates computed from
the training data are good indicators of test set error rates, and that
the one standard error rule models do as well as the minimim cv error models
for the lasso, which has the best overall performance.</p>
<div id="logistic-regression-in-glmnet" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Logistic regression in <code>glmnet</code></h3>
<p><code>glmnet</code> provides functionality to extract various predicted of fitted values
from calibrated models. Note in passing that some folks make a distinction between
<strong>fitted</strong> or <strong>estimated</strong> values for sample points in the training data
versus <strong>predicted</strong> values for sample points that
are not in the training dataset. <code>glmnet</code> makes no such distinction and the
<code>predict</code> function is used to produce both fitted as well as predicted values.
When predict is invoked to make predictions for design points that are part
of the training dataset, what is returned are fitted values.<br />
When predict is invoked to make predictions for design points that are not part
of the training dataset, what is returned are predicted values.</p>
<p>For logistic regressions, which is the model fitted in a regularized fashion
when models are fitted by glmnet with the parameter <code>family='binomial'</code>, three
fitted or predicted values can be extracted at a given design point.
Suppose our response variable Y is either 0 or 1 (Control or HCC in our case).
These are specified by the <code>type</code> parameter. <code>type='resp'</code> returns
the fitted or predicted probability of <span class="math inline">\(Y=1\)</span>. <code>type='class'</code> returns the fitted or
predicted class for the design point, which is simply dichotomizing the
response: class = 1 if the fitted or predicted probability is greater than 0.5
(check to make sure class is no the Bayes estimate). <code>type='link'</code> returns
the fitted or predicted value of the linear predictor <span class="math inline">\(\beta&#39;x\)</span>. The relationship
between the linear predictor and the response can be derided from the
logistic regression model:</p>
<p><span class="math display">\[P(Y=1|x,\beta) = g^{-1}(\beta&#39;x) = h(\beta&#39;x) = \frac{e^{\beta&#39;x}}{1+e^{\beta&#39;x}}\]</span></p>
<p>where <span class="math inline">\(g\)</span> is the link function, <span class="math inline">\(g^{-1}\)</span> the mean function.
The link function is given by:</p>
<p><span class="math display">\[g(y) = h^{-1}(y) = ln(\frac{y}{1-y})\]</span></p>
<p>This link function is called the <em>logit</em> function, and its inverse the <em>logistic</em>
function.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="explore-sparsity.html#cb39-1"></a>logistic_f &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))</span></code></pre></div>
<p>It is important to note that all <em>predicted</em> values extracted from
<code>glmnet</code> fitted models by the <strong>predict()</strong> extraction method
yield <strong>fitted</strong> values for design points that are part of the
training data set. This includes the predicted class for training data
which are used to estimate misclassification error rates. As a result, the cv error
rates quoted in various <code>glmnet</code> summaries are generally optimistic.
<code>glmnet</code> fitting functions have a
parameter, <em>keep</em>, which instructs the fitting function to keep the
<code>out-of-fold</code>, or <code>prevalidated</code>, predictions as part of the returned object. The
<code>out-of-fold</code> predictions are predicted values for the samples in the
left-out folds, pooled across all cv folds. For each hyper-parameter
specification, we get one full set of <code>out-of-fold</code> predictions for
the training set samples. Performance assessments based on these
values are usually more generalizable - ie. predictive of
performance in unseen data - than assessments based on values
produced from the full fit, which by default is what <code>glmnet</code> extraction
methods provide. See Höfling and Tibshirani (2008) <span class="citation">[<a href="references.html#ref-Hofling:2008aa" role="doc-biblioref">27</a>]</span>
for a description of the use of pre-validation in model assessment.</p>
<p>Because the <code>keep=T</code> option will store predicted values for
all models evaluated in the cross-validation process, we will
limit the number of models tested by setting <strong>nlambda=30</strong>
when calling the fitting functions. This has no effect on
performance in this data set.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="explore-sparsity.html#cb40-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb40-2"><a href="explore-sparsity.html#cb40-2"></a></span>
<span id="cb40-3"><a href="explore-sparsity.html#cb40-3"></a>cv_lasso &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb40-4"><a href="explore-sparsity.html#cb40-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb40-5"><a href="explore-sparsity.html#cb40-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb40-6"><a href="explore-sparsity.html#cb40-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb40-7"><a href="explore-sparsity.html#cb40-7"></a> <span class="dt">alpha=</span><span class="dv">1</span>,</span>
<span id="cb40-8"><a href="explore-sparsity.html#cb40-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>, </span>
<span id="cb40-9"><a href="explore-sparsity.html#cb40-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb40-10"><a href="explore-sparsity.html#cb40-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb40-11"><a href="explore-sparsity.html#cb40-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb40-12"><a href="explore-sparsity.html#cb40-12"></a>)</span>
<span id="cb40-13"><a href="explore-sparsity.html#cb40-13"></a></span>
<span id="cb40-14"><a href="explore-sparsity.html#cb40-14"></a><span class="kw">message</span>(<span class="st">&quot;lasso time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## lasso time: 11.97s</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="explore-sparsity.html#cb42-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb42-2"><a href="explore-sparsity.html#cb42-2"></a></span>
<span id="cb42-3"><a href="explore-sparsity.html#cb42-3"></a>cv_ridge &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb42-4"><a href="explore-sparsity.html#cb42-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb42-5"><a href="explore-sparsity.html#cb42-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb42-6"><a href="explore-sparsity.html#cb42-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb42-7"><a href="explore-sparsity.html#cb42-7"></a> <span class="dt">alpha=</span><span class="dv">0</span>,</span>
<span id="cb42-8"><a href="explore-sparsity.html#cb42-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>, </span>
<span id="cb42-9"><a href="explore-sparsity.html#cb42-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb42-10"><a href="explore-sparsity.html#cb42-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb42-11"><a href="explore-sparsity.html#cb42-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb42-12"><a href="explore-sparsity.html#cb42-12"></a>)</span>
<span id="cb42-13"><a href="explore-sparsity.html#cb42-13"></a></span>
<span id="cb42-14"><a href="explore-sparsity.html#cb42-14"></a><span class="kw">message</span>(<span class="st">&quot;ridge time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## ridge time: 103.89s</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="explore-sparsity.html#cb44-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb44-2"><a href="explore-sparsity.html#cb44-2"></a></span>
<span id="cb44-3"><a href="explore-sparsity.html#cb44-3"></a>cv_enet &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb44-4"><a href="explore-sparsity.html#cb44-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb44-5"><a href="explore-sparsity.html#cb44-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb44-6"><a href="explore-sparsity.html#cb44-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb44-7"><a href="explore-sparsity.html#cb44-7"></a> <span class="dt">alpha=</span><span class="fl">0.5</span>,</span>
<span id="cb44-8"><a href="explore-sparsity.html#cb44-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>,</span>
<span id="cb44-9"><a href="explore-sparsity.html#cb44-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb44-10"><a href="explore-sparsity.html#cb44-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb44-11"><a href="explore-sparsity.html#cb44-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb44-12"><a href="explore-sparsity.html#cb44-12"></a>)</span>
<span id="cb44-13"><a href="explore-sparsity.html#cb44-13"></a></span>
<span id="cb44-14"><a href="explore-sparsity.html#cb44-14"></a><span class="kw">message</span>(<span class="st">&quot;enet time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## enet time: 12.44s</code></pre>
<p>The ridge regression model takes over 10 times longer to compute.</p>
<!-- do not show
Define plotting function.
Maybe show in appendix??
-->
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="explore-sparsity.html#cb46-1"></a>plot_cv_f &lt;-<span class="st"> </span><span class="cf">function</span>(cv_fit, <span class="dt">Nzero=</span>T, ...) {</span>
<span id="cb46-2"><a href="explore-sparsity.html#cb46-2"></a> </span>
<span id="cb46-3"><a href="explore-sparsity.html#cb46-3"></a> <span class="kw">library</span>(glmnet)</span>
<span id="cb46-4"><a href="explore-sparsity.html#cb46-4"></a></span>
<span id="cb46-5"><a href="explore-sparsity.html#cb46-5"></a> <span class="co"># No nonger used</span></span>
<span id="cb46-6"><a href="explore-sparsity.html#cb46-6"></a> <span class="co">#lambda.1se_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]</span></span>
<span id="cb46-7"><a href="explore-sparsity.html#cb46-7"></a> <span class="co">#lambda.min_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]</span></span>
<span id="cb46-8"><a href="explore-sparsity.html#cb46-8"></a> </span>
<span id="cb46-9"><a href="explore-sparsity.html#cb46-9"></a> <span class="co"># Get oof error</span></span>
<span id="cb46-10"><a href="explore-sparsity.html#cb46-10"></a> ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_fit<span class="op">$</span>lambda)</span>
<span id="cb46-11"><a href="explore-sparsity.html#cb46-11"></a> train_oofPred_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb46-12"><a href="explore-sparsity.html#cb46-12"></a>  cv_fit<span class="op">$</span>fit.preval[,ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb46-13"><a href="explore-sparsity.html#cb46-13"></a> train_oofPred_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb46-14"><a href="explore-sparsity.html#cb46-14"></a></span>
<span id="cb46-15"><a href="explore-sparsity.html#cb46-15"></a> ndx_min &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda.min,cv_fit<span class="op">$</span>lambda)</span>
<span id="cb46-16"><a href="explore-sparsity.html#cb46-16"></a> train_oofPred_min_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb46-17"><a href="explore-sparsity.html#cb46-17"></a>  cv_fit<span class="op">$</span>fit.preval[,ndx_min] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb46-18"><a href="explore-sparsity.html#cb46-18"></a> train_oofPred_min_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_min_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb46-19"><a href="explore-sparsity.html#cb46-19"></a></span>
<span id="cb46-20"><a href="explore-sparsity.html#cb46-20"></a> <span class="co"># Get test set error</span></span>
<span id="cb46-21"><a href="explore-sparsity.html#cb46-21"></a> test_pred_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb46-22"><a href="explore-sparsity.html#cb46-22"></a>  cv_fit, </span>
<span id="cb46-23"><a href="explore-sparsity.html#cb46-23"></a>  <span class="dt">newx=</span>test_lcpm_mtx, </span>
<span id="cb46-24"><a href="explore-sparsity.html#cb46-24"></a>  <span class="dt">s=</span><span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb46-25"><a href="explore-sparsity.html#cb46-25"></a>  <span class="dt">type=</span><span class="st">&quot;class&quot;</span></span>
<span id="cb46-26"><a href="explore-sparsity.html#cb46-26"></a> )</span>
<span id="cb46-27"><a href="explore-sparsity.html#cb46-27"></a> test_pred_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb46-28"><a href="explore-sparsity.html#cb46-28"></a> </span>
<span id="cb46-29"><a href="explore-sparsity.html#cb46-29"></a> test_pred_min_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb46-30"><a href="explore-sparsity.html#cb46-30"></a>  cv_fit, </span>
<span id="cb46-31"><a href="explore-sparsity.html#cb46-31"></a>  <span class="dt">newx=</span>test_lcpm_mtx, </span>
<span id="cb46-32"><a href="explore-sparsity.html#cb46-32"></a>  <span class="dt">s=</span><span class="st">&quot;lambda.min&quot;</span>,</span>
<span id="cb46-33"><a href="explore-sparsity.html#cb46-33"></a>  <span class="dt">type=</span><span class="st">&quot;class&quot;</span></span>
<span id="cb46-34"><a href="explore-sparsity.html#cb46-34"></a> )</span>
<span id="cb46-35"><a href="explore-sparsity.html#cb46-35"></a> test_pred_min_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_min_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb46-36"><a href="explore-sparsity.html#cb46-36"></a> </span>
<span id="cb46-37"><a href="explore-sparsity.html#cb46-37"></a>  </span>
<span id="cb46-38"><a href="explore-sparsity.html#cb46-38"></a> <span class="kw">plot</span>(</span>
<span id="cb46-39"><a href="explore-sparsity.html#cb46-39"></a>  <span class="kw">log</span>(cv_fit<span class="op">$</span>lambda),</span>
<span id="cb46-40"><a href="explore-sparsity.html#cb46-40"></a>  cv_fit<span class="op">$</span>cvm,</span>
<span id="cb46-41"><a href="explore-sparsity.html#cb46-41"></a>  <span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,</span>
<span id="cb46-42"><a href="explore-sparsity.html#cb46-42"></a>  <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>,<span class="dt">ylab=</span><span class="st">&#39;&#39;</span>,</span>
<span id="cb46-43"><a href="explore-sparsity.html#cb46-43"></a>  ...</span>
<span id="cb46-44"><a href="explore-sparsity.html#cb46-44"></a> )</span>
<span id="cb46-45"><a href="explore-sparsity.html#cb46-45"></a> <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">log</span>(<span class="kw">c</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_fit<span class="op">$</span>lambda.min)))</span>
<span id="cb46-46"><a href="explore-sparsity.html#cb46-46"></a> <span class="cf">if</span>(Nzero)</span>
<span id="cb46-47"><a href="explore-sparsity.html#cb46-47"></a> <span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">3</span>, <span class="dt">tick=</span>F, <span class="dt">at=</span><span class="kw">log</span>(cv_fit<span class="op">$</span>lambda), </span>
<span id="cb46-48"><a href="explore-sparsity.html#cb46-48"></a>  <span class="dt">labels=</span>cv_fit<span class="op">$</span>nzero, <span class="dt">line =</span> <span class="dv">-1</span></span>
<span id="cb46-49"><a href="explore-sparsity.html#cb46-49"></a> )</span>
<span id="cb46-50"><a href="explore-sparsity.html#cb46-50"></a> LL &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb46-51"><a href="explore-sparsity.html#cb46-51"></a> <span class="co">#mtext(side=1, outer=F, line = LL, &quot;log(Lambda)&quot;)</span></span>
<span id="cb46-52"><a href="explore-sparsity.html#cb46-52"></a> <span class="co">#LL &lt;- LL+1</span></span>
<span id="cb46-53"><a href="explore-sparsity.html#cb46-53"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>F, <span class="dt">line =</span> LL, <span class="kw">paste</span>(</span>
<span id="cb46-54"><a href="explore-sparsity.html#cb46-54"></a>  <span class="co">#ifelse(Nzero, paste(&quot;1se p =&quot;, lambda.1se_p),&#39;&#39;),</span></span>
<span id="cb46-55"><a href="explore-sparsity.html#cb46-55"></a>  <span class="st">&quot;1se: cv =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se], <span class="dv">1</span>),</span>
<span id="cb46-56"><a href="explore-sparsity.html#cb46-56"></a>  <span class="st">&quot;oof =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>train_oofPred_1se_error, <span class="dv">1</span>),</span>
<span id="cb46-57"><a href="explore-sparsity.html#cb46-57"></a>  <span class="st">&quot;test =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>test_pred_1se_error, <span class="dv">1</span>)</span>
<span id="cb46-58"><a href="explore-sparsity.html#cb46-58"></a> ))</span>
<span id="cb46-59"><a href="explore-sparsity.html#cb46-59"></a> LL &lt;-<span class="st"> </span>LL<span class="op">+</span><span class="dv">1</span></span>
<span id="cb46-60"><a href="explore-sparsity.html#cb46-60"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>F, <span class="dt">line =</span> LL, <span class="kw">paste</span>(</span>
<span id="cb46-61"><a href="explore-sparsity.html#cb46-61"></a>  <span class="co">#ifelse(Nzero, paste(&quot;min p =&quot;, lambda.min_p),&#39;&#39;),</span></span>
<span id="cb46-62"><a href="explore-sparsity.html#cb46-62"></a>  <span class="st">&quot;min: cv =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda.min], <span class="dv">1</span>),</span>
<span id="cb46-63"><a href="explore-sparsity.html#cb46-63"></a>  <span class="st">&quot;oof =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>train_oofPred_min_error, <span class="dv">1</span>),</span>
<span id="cb46-64"><a href="explore-sparsity.html#cb46-64"></a>  <span class="st">&quot;test =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>test_pred_min_error, <span class="dv">1</span>)</span>
<span id="cb46-65"><a href="explore-sparsity.html#cb46-65"></a> ))</span>
<span id="cb46-66"><a href="explore-sparsity.html#cb46-66"></a> </span>
<span id="cb46-67"><a href="explore-sparsity.html#cb46-67"></a> tmp &lt;-</span>
<span id="cb46-68"><a href="explore-sparsity.html#cb46-68"></a><span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb46-69"><a href="explore-sparsity.html#cb46-69"></a>  <span class="dt">error_1se =</span> <span class="kw">c</span>(</span>
<span id="cb46-70"><a href="explore-sparsity.html#cb46-70"></a>   <span class="dt">p =</span> cv_fit<span class="op">$</span>nzero[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se],</span>
<span id="cb46-71"><a href="explore-sparsity.html#cb46-71"></a>   <span class="dt">train_cv =</span> <span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se],</span>
<span id="cb46-72"><a href="explore-sparsity.html#cb46-72"></a>   <span class="dt">train_oof =</span> <span class="dv">100</span><span class="op">*</span>train_oofPred_1se_error,</span>
<span id="cb46-73"><a href="explore-sparsity.html#cb46-73"></a>   <span class="dt">test =</span> <span class="dv">100</span><span class="op">*</span>test_pred_1se_error),</span>
<span id="cb46-74"><a href="explore-sparsity.html#cb46-74"></a>  <span class="dt">error_min =</span> <span class="kw">c</span>(</span>
<span id="cb46-75"><a href="explore-sparsity.html#cb46-75"></a>   <span class="dt">p =</span> cv_fit<span class="op">$</span>nzero[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda.min],</span>
<span id="cb46-76"><a href="explore-sparsity.html#cb46-76"></a>   <span class="dt">train_cv =</span> <span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda.min],</span>
<span id="cb46-77"><a href="explore-sparsity.html#cb46-77"></a>   <span class="dt">train_oof =</span> <span class="dv">100</span><span class="op">*</span>train_oofPred_min_error,</span>
<span id="cb46-78"><a href="explore-sparsity.html#cb46-78"></a>   <span class="dt">test =</span> <span class="dv">100</span><span class="op">*</span>test_pred_min_error)</span>
<span id="cb46-79"><a href="explore-sparsity.html#cb46-79"></a>  )</span>
<span id="cb46-80"><a href="explore-sparsity.html#cb46-80"></a>  <span class="co"># Need to fix names  </span></span>
<span id="cb46-81"><a href="explore-sparsity.html#cb46-81"></a>  <span class="kw">rownames</span>(tmp) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;p&#39;</span>, <span class="st">&#39;train_cv&#39;</span>, <span class="st">&#39;train_oof&#39;</span>, <span class="st">&#39;test&#39;</span>)</span>
<span id="cb46-82"><a href="explore-sparsity.html#cb46-82"></a>  tmp </span>
<span id="cb46-83"><a href="explore-sparsity.html#cb46-83"></a>}</span></code></pre></div>
<p>Examine model performance.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="explore-sparsity.html#cb47-1"></a> <span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>)) </span>
<span id="cb47-2"><a href="explore-sparsity.html#cb47-2"></a></span>
<span id="cb47-3"><a href="explore-sparsity.html#cb47-3"></a> lasso_errors_mtx &lt;-<span class="st"> </span><span class="kw">plot_cv_f</span>(cv_lasso, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb47-4"><a href="explore-sparsity.html#cb47-4"></a> <span class="kw">title</span>(<span class="st">&#39;lasso&#39;</span>)</span>
<span id="cb47-5"><a href="explore-sparsity.html#cb47-5"></a></span>
<span id="cb47-6"><a href="explore-sparsity.html#cb47-6"></a> rifge_errors_mtx &lt;-<span class="st"> </span><span class="kw">plot_cv_f</span>(cv_ridge, <span class="dt">Nzero=</span>F, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb47-7"><a href="explore-sparsity.html#cb47-7"></a> <span class="kw">title</span>(<span class="st">&#39;ridge&#39;</span>)</span>
<span id="cb47-8"><a href="explore-sparsity.html#cb47-8"></a></span>
<span id="cb47-9"><a href="explore-sparsity.html#cb47-9"></a> enet_errors_mtx &lt;-<span class="st">  </span><span class="kw">plot_cv_f</span>(cv_enet, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb47-10"><a href="explore-sparsity.html#cb47-10"></a> <span class="kw">title</span>(<span class="st">&#39;enet&#39;</span>)</span>
<span id="cb47-11"><a href="explore-sparsity.html#cb47-11"></a></span>
<span id="cb47-12"><a href="explore-sparsity.html#cb47-12"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>T, <span class="dt">cex=</span><span class="fl">1.25</span>, <span class="st">&#39;log(Lambda)&#39;</span>)</span>
<span id="cb47-13"><a href="explore-sparsity.html#cb47-13"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">outer=</span>T, <span class="dt">cex=</span><span class="fl">1.25</span>, cv_lasso<span class="op">$</span>name)</span></code></pre></div>
<div class="figure"><span id="fig:lookFits"></span>
<img src="Static/figures/lookFits-1.png" alt="compare fits" width="1056" />
<p class="caption">
Figure 4.1: compare fits
</p>
</div>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="explore-sparsity.html#cb48-1"></a>errors_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb48-2"><a href="explore-sparsity.html#cb48-2"></a>  <span class="dt">lasso =</span> lasso_errors_mtx, <span class="dt">ridge =</span> rifge_errors_mtx, <span class="dt">enet =</span> enet_errors_mtx</span>
<span id="cb48-3"><a href="explore-sparsity.html#cb48-3"></a>)</span>
<span id="cb48-4"><a href="explore-sparsity.html#cb48-4"></a></span>
<span id="cb48-5"><a href="explore-sparsity.html#cb48-5"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">t</span>(errors_frm),</span>
<span id="cb48-6"><a href="explore-sparsity.html#cb48-6"></a> <span class="dt">caption =</span> <span class="st">&#39;Misclassifiaction error rates&#39;</span>,</span>
<span id="cb48-7"><a href="explore-sparsity.html#cb48-7"></a> <span class="dt">digits=</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb48-8"><a href="explore-sparsity.html#cb48-8"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:printErrors">Table 4.3: </span>Misclassifiaction error rates
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
p
</th>
<th style="text-align:right;">
train_cv
</th>
<th style="text-align:right;">
train_oof
</th>
<th style="text-align:right;">
test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
lasso.error_1se
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
9.7
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
lasso.error_min
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
9.7
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
ridge.error_1se
</td>
<td style="text-align:right;">
15752
</td>
<td style="text-align:right;">
13.5
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
18.4
</td>
</tr>
<tr>
<td style="text-align:left;">
ridge.error_min
</td>
<td style="text-align:right;">
15752
</td>
<td style="text-align:right;">
12.5
</td>
<td style="text-align:right;">
16.9
</td>
<td style="text-align:right;">
16.2
</td>
</tr>
<tr>
<td style="text-align:left;">
enet.error_1se
</td>
<td style="text-align:right;">
118
</td>
<td style="text-align:right;">
7.2
</td>
<td style="text-align:right;">
11.1
</td>
<td style="text-align:right;">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;">
enet.error_min
</td>
<td style="text-align:right;">
278
</td>
<td style="text-align:right;">
6.7
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
</tbody>
</table>
<p>We see that the lasso and enet models do better than the ridge model.
There is very little difference between the min lambda and the
the one standard error (1se) rule lambda models (the two are the same for the
lasso in this data set).</p>
<p>The <em>1se</em> lambda lasso fit is only slightly more parsimonious than the
<em>1se</em> elastic net fit, but its test set accuracy is better.
The <em>min</em> lambda elastic net fit performs as well as the lasso model,
but is much less parsimonious.</p>
<p>We also see that the training data out-of-fold
estimates of misclassification error rates are much closer to the
test set estimates than are the cv estimated rates. This has
been our experience with regularized regression models fitted to
genomic scale data. It should also be noted that the cv estimates of
misclassification rates become more biased as the sample size decreases,
as we will show in Section <a href="model-suite.html#model-suite">5</a>.</p>
</div>
</div>
<div id="relaxed-lasso-and-blended-mix" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Relaxed lasso and blended mix</h2>
<p>Next we look at the so-called <code>relaxed lasso</code> and
the <code>blended mix</code> which is an optimized shrinkage
between the relaxed lasso and the regular lasso.
See <a href="modeling-background.html#eq:blended">(2.3)</a> in Section <a href="modeling-background.html#modeling-background">2</a>.</p>
<!--
The relaxed fit takes quite a bit longer.  
-->
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="explore-sparsity.html#cb49-1"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb49-2"><a href="explore-sparsity.html#cb49-2"></a></span>
<span id="cb49-3"><a href="explore-sparsity.html#cb49-3"></a>cv_lassoR_sum &lt;-<span class="st"> </span><span class="kw">print</span>(cv_lassoR)</span></code></pre></div>
<pre><code>## 
## Call:  glmnet::cv.glmnet(x = train_lcpm_mtx, y = train_group_vec, type.measure = &quot;class&quot;,      foldid = train_foldid_vec, keep = T, relax = T, alpha = 1,      family = &quot;binomial&quot;, nlambda = 30) 
## 
## Measure: Misclassification Error 
## 
##     Gamma Lambda Measure       SE Nonzero
## min   0.5 0.0379 0.06748 0.005274      35
## 1se   0.5 0.0379 0.06748 0.005274      35</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="explore-sparsity.html#cb51-1"></a><span class="kw">plot</span>(cv_lassoR)</span></code></pre></div>
<div class="figure"><span id="fig:lookLassoR"></span>
<img src="Static/figures/lookLassoR-1.png" alt="Relaxed lasso fit" width="576" />
<p class="caption">
Figure 4.2: Relaxed lasso fit
</p>
</div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="explore-sparsity.html#cb52-1"></a><span class="co"># only report  1se</span></span>
<span id="cb52-2"><a href="explore-sparsity.html#cb52-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb52-3"><a href="explore-sparsity.html#cb52-3"></a>ndx_min &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda.min, cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb52-4"><a href="explore-sparsity.html#cb52-4"></a></span>
<span id="cb52-5"><a href="explore-sparsity.html#cb52-5"></a><span class="co"># only show 1se anyway</span></span>
<span id="cb52-6"><a href="explore-sparsity.html#cb52-6"></a><span class="co"># if(ndx_1se != ndx_min) stop(&quot;lambda.1se != lambda.min&quot;)</span></span>
<span id="cb52-7"><a href="explore-sparsity.html#cb52-7"></a></span>
<span id="cb52-8"><a href="explore-sparsity.html#cb52-8"></a></span>
<span id="cb52-9"><a href="explore-sparsity.html#cb52-9"></a><span class="co"># train oof data</span></span>
<span id="cb52-10"><a href="explore-sparsity.html#cb52-10"></a><span class="co"># Get relaxed lasso (gamma=0) oof error</span></span>
<span id="cb52-11"><a href="explore-sparsity.html#cb52-11"></a>train_oofPred_relaxed_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb52-12"><a href="explore-sparsity.html#cb52-12"></a>  cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&quot;g:0&quot;</span>]][, ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;HCC&quot;</span>, <span class="st">&quot;Control&quot;</span></span>
<span id="cb52-13"><a href="explore-sparsity.html#cb52-13"></a>)</span>
<span id="cb52-14"><a href="explore-sparsity.html#cb52-14"></a>train_oofPred_relaxed_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_relaxed_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb52-15"><a href="explore-sparsity.html#cb52-15"></a></span>
<span id="cb52-16"><a href="explore-sparsity.html#cb52-16"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb52-17"><a href="explore-sparsity.html#cb52-17"></a>train_oofPred_blended_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb52-18"><a href="explore-sparsity.html#cb52-18"></a>  cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&quot;g:0.5&quot;</span>]][, ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;HCC&quot;</span>, <span class="st">&quot;Control&quot;</span></span>
<span id="cb52-19"><a href="explore-sparsity.html#cb52-19"></a>)</span>
<span id="cb52-20"><a href="explore-sparsity.html#cb52-20"></a>train_oofPred_blended_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_blended_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb52-21"><a href="explore-sparsity.html#cb52-21"></a></span>
<span id="cb52-22"><a href="explore-sparsity.html#cb52-22"></a></span>
<span id="cb52-23"><a href="explore-sparsity.html#cb52-23"></a><span class="co"># Test set error - relaxed</span></span>
<span id="cb52-24"><a href="explore-sparsity.html#cb52-24"></a>test_pred_relaxed_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb52-25"><a href="explore-sparsity.html#cb52-25"></a>  cv_lassoR,</span>
<span id="cb52-26"><a href="explore-sparsity.html#cb52-26"></a>  <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb52-27"><a href="explore-sparsity.html#cb52-27"></a>  <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb52-28"><a href="explore-sparsity.html#cb52-28"></a>  <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb52-29"><a href="explore-sparsity.html#cb52-29"></a>  <span class="dt">gamma =</span> <span class="dv">0</span></span>
<span id="cb52-30"><a href="explore-sparsity.html#cb52-30"></a>)</span>
<span id="cb52-31"><a href="explore-sparsity.html#cb52-31"></a>test_pred_relaxed_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_relaxed_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb52-32"><a href="explore-sparsity.html#cb52-32"></a></span>
<span id="cb52-33"><a href="explore-sparsity.html#cb52-33"></a><span class="co"># Test set error - blended</span></span>
<span id="cb52-34"><a href="explore-sparsity.html#cb52-34"></a>test_pred_blended_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb52-35"><a href="explore-sparsity.html#cb52-35"></a>  cv_lassoR,</span>
<span id="cb52-36"><a href="explore-sparsity.html#cb52-36"></a>  <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb52-37"><a href="explore-sparsity.html#cb52-37"></a>  <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb52-38"><a href="explore-sparsity.html#cb52-38"></a>  <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb52-39"><a href="explore-sparsity.html#cb52-39"></a>  <span class="dt">gamma =</span> <span class="fl">0.5</span></span>
<span id="cb52-40"><a href="explore-sparsity.html#cb52-40"></a>)</span>
<span id="cb52-41"><a href="explore-sparsity.html#cb52-41"></a>test_pred_blended_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_blended_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb52-42"><a href="explore-sparsity.html#cb52-42"></a></span>
<span id="cb52-43"><a href="explore-sparsity.html#cb52-43"></a></span>
<span id="cb52-44"><a href="explore-sparsity.html#cb52-44"></a></span>
<span id="cb52-45"><a href="explore-sparsity.html#cb52-45"></a>cv_lassoR_1se_error &lt;-<span class="st"> </span>cv_lassoR<span class="op">$</span>cvm[cv_lassoR<span class="op">$</span>lambda<span class="op">==</span>cv_lassoR<span class="op">$</span>lambda.min]</span>
<span id="cb52-46"><a href="explore-sparsity.html#cb52-46"></a></span>
<span id="cb52-47"><a href="explore-sparsity.html#cb52-47"></a>cv_blended_statlist &lt;-<span class="st"> </span>cv_lassoR<span class="op">$</span>relaxed<span class="op">$</span>statlist[[<span class="st">&#39;g:0.5&#39;</span>]]</span>
<span id="cb52-48"><a href="explore-sparsity.html#cb52-48"></a>cv_blended_1se_error &lt;-<span class="st"> </span>cv_blended_statlist<span class="op">$</span>cvm[cv_blended_statlist<span class="op">$</span>lambda<span class="op">==</span></span>
<span id="cb52-49"><a href="explore-sparsity.html#cb52-49"></a><span class="st">   </span>cv_lassoR<span class="op">$</span>relaxed<span class="op">$</span>lambda<span class="fl">.1</span>se]</span>
<span id="cb52-50"><a href="explore-sparsity.html#cb52-50"></a> </span>
<span id="cb52-51"><a href="explore-sparsity.html#cb52-51"></a></span>
<span id="cb52-52"><a href="explore-sparsity.html#cb52-52"></a></span>
<span id="cb52-53"><a href="explore-sparsity.html#cb52-53"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">t</span>(<span class="kw">data.frame</span>(</span>
<span id="cb52-54"><a href="explore-sparsity.html#cb52-54"></a>  <span class="dt">train_relaxed_cv =</span> cv_lassoR_1se_error,</span>
<span id="cb52-55"><a href="explore-sparsity.html#cb52-55"></a>  <span class="dt">train_blended_cv =</span> cv_blended_1se_error,</span>
<span id="cb52-56"><a href="explore-sparsity.html#cb52-56"></a>  <span class="dt">train_relaxed_oof =</span> train_oofPred_relaxed_1se_error,</span>
<span id="cb52-57"><a href="explore-sparsity.html#cb52-57"></a>  <span class="dt">train_blended_oof =</span> train_oofPred_blended_1se_error,</span>
<span id="cb52-58"><a href="explore-sparsity.html#cb52-58"></a>  <span class="dt">test_relaxed_oof =</span> test_pred_relaxed_1se_error,</span>
<span id="cb52-59"><a href="explore-sparsity.html#cb52-59"></a>  <span class="dt">test_blended_oof =</span> test_pred_blended_1se_error</span>
<span id="cb52-60"><a href="explore-sparsity.html#cb52-60"></a>)) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>,</span>
<span id="cb52-61"><a href="explore-sparsity.html#cb52-61"></a><span class="dt">digits =</span> <span class="dv">1</span>,</span>
<span id="cb52-62"><a href="explore-sparsity.html#cb52-62"></a><span class="dt">caption =</span> <span class="st">&quot;Relaxed lasso and blended mix error rates&quot;</span></span>
<span id="cb52-63"><a href="explore-sparsity.html#cb52-63"></a>) <span class="op">%&gt;%</span></span>
<span id="cb52-64"><a href="explore-sparsity.html#cb52-64"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:lookLassoR2">Table 4.4: </span>Relaxed lasso and blended mix error rates
</caption>
<tbody>
<tr>
<td style="text-align:left;">
train_relaxed_cv
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_cv
</td>
<td style="text-align:right;">
6.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_relaxed_oof
</td>
<td style="text-align:right;">
11.1
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_oof
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
test_relaxed_oof
</td>
<td style="text-align:right;">
10.9
</td>
</tr>
<tr>
<td style="text-align:left;">
test_blended_oof
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
</tbody>
</table>
<p>The relaxed lasso and blended mix error rates are comparable to the
regular lasso fit error rate. We see here too that the reported cv
error rates are quite optimistic, while out-of-fold error rates
continue to be good indicators of unseen data error rates, as captured
by the test set.</p>
<p>The <em>1se</em> lambda rule applied to the relaxed lasso fit selected a model with
<span class="math inline">\(99\)</span> features,
while for the blended mix model
(See <a href="modeling-background.html#eq:blended">(2.3)</a> in Section <a href="modeling-background.html#modeling-background">2</a>)
the <em>1se</em> lambda rule selected
<span class="math inline">\(35\)</span> features (vertical
dotted reference line in Figure <a href="explore-sparsity.html#fig:lookLassoR">4.2</a>).</p>
</div>
<div id="examination-of-sensitivity-vs-specificity" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Examination of sensitivity vs specificity</h2>
<p>In the results above we reported error rates without inspecting the
sensitivity versus specificity trade-off. ROC curves can be examined
to get a sense of the trade-off.</p>
<div id="training-data-out-of-fold-roc-curves" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Training data out-of-fold ROC curves</h3>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="explore-sparsity.html#cb53-1"></a><span class="co"># train</span></span>
<span id="cb53-2"><a href="explore-sparsity.html#cb53-2"></a><span class="co"># lasso</span></span>
<span id="cb53-3"><a href="explore-sparsity.html#cb53-3"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lasso<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lasso<span class="op">$</span>lambda)</span>
<span id="cb53-4"><a href="explore-sparsity.html#cb53-4"></a>train_lasso_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lasso<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb53-5"><a href="explore-sparsity.html#cb53-5"></a>train_lasso_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb53-6"><a href="explore-sparsity.html#cb53-6"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb53-7"><a href="explore-sparsity.html#cb53-7"></a> <span class="dt">predictor =</span> train_lasso_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="explore-sparsity.html#cb56-1"></a><span class="co"># enet</span></span>
<span id="cb56-2"><a href="explore-sparsity.html#cb56-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_enet<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_enet<span class="op">$</span>lambda)</span>
<span id="cb56-3"><a href="explore-sparsity.html#cb56-3"></a>train_enet_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_enet<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb56-4"><a href="explore-sparsity.html#cb56-4"></a>train_enet_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb56-5"><a href="explore-sparsity.html#cb56-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb56-6"><a href="explore-sparsity.html#cb56-6"></a> <span class="dt">predictor =</span> train_enet_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="explore-sparsity.html#cb58-1"></a><span class="co"># lasso - relaxed</span></span>
<span id="cb58-2"><a href="explore-sparsity.html#cb58-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb58-3"><a href="explore-sparsity.html#cb58-3"></a>train_relaxed_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0&#39;</span>]][,ndx_1se])</span>
<span id="cb58-4"><a href="explore-sparsity.html#cb58-4"></a>train_relaxed_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb58-5"><a href="explore-sparsity.html#cb58-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb58-6"><a href="explore-sparsity.html#cb58-6"></a> <span class="dt">predictor =</span> train_relaxed_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="explore-sparsity.html#cb60-1"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb60-2"><a href="explore-sparsity.html#cb60-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb60-3"><a href="explore-sparsity.html#cb60-3"></a>train_blended_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0.5&#39;</span>]][,ndx_1se])</span>
<span id="cb60-4"><a href="explore-sparsity.html#cb60-4"></a>train_blended_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb60-5"><a href="explore-sparsity.html#cb60-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb60-6"><a href="explore-sparsity.html#cb60-6"></a> <span class="dt">predictor =</span> train_blended_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="explore-sparsity.html#cb62-1"></a><span class="kw">plot</span>(train_lasso_roc, <span class="dt">col=</span>col_vec[<span class="dv">1</span>])</span>
<span id="cb62-2"><a href="explore-sparsity.html#cb62-2"></a><span class="kw">lines</span>(train_enet_roc, <span class="dt">col=</span>col_vec[<span class="dv">2</span>])</span>
<span id="cb62-3"><a href="explore-sparsity.html#cb62-3"></a><span class="kw">lines</span>(train_relaxed_roc, <span class="dt">col=</span>col_vec[<span class="dv">3</span>])</span>
<span id="cb62-4"><a href="explore-sparsity.html#cb62-4"></a><span class="kw">lines</span>(train_blended_roc, <span class="dt">col=</span>col_vec[<span class="dv">4</span>])</span>
<span id="cb62-5"><a href="explore-sparsity.html#cb62-5"></a></span>
<span id="cb62-6"><a href="explore-sparsity.html#cb62-6"></a><span class="kw">legend</span>(<span class="st">&#39;bottomright&#39;</span>, <span class="dt">title=</span><span class="st">&#39;AUC&#39;</span>,</span>
<span id="cb62-7"><a href="explore-sparsity.html#cb62-7"></a> <span class="dt">legend=</span><span class="kw">c</span>(</span>
<span id="cb62-8"><a href="explore-sparsity.html#cb62-8"></a>  <span class="kw">paste</span>(<span class="st">&#39;lasso =&#39;</span>, <span class="kw">round</span>(train_lasso_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb62-9"><a href="explore-sparsity.html#cb62-9"></a>  <span class="kw">paste</span>(<span class="st">&#39;enet =&#39;</span>, <span class="kw">round</span>(train_enet_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb62-10"><a href="explore-sparsity.html#cb62-10"></a>  <span class="kw">paste</span>(<span class="st">&#39;relaxed =&#39;</span>, <span class="kw">round</span>(train_relaxed_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb62-11"><a href="explore-sparsity.html#cb62-11"></a>  <span class="kw">paste</span>(<span class="st">&#39;blended =&#39;</span>, <span class="kw">round</span>(train_blended_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>))</span>
<span id="cb62-12"><a href="explore-sparsity.html#cb62-12"></a> ),</span>
<span id="cb62-13"><a href="explore-sparsity.html#cb62-13"></a> <span class="dt">text.col =</span> col_vec[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>],</span>
<span id="cb62-14"><a href="explore-sparsity.html#cb62-14"></a> <span class="dt">bty=</span><span class="st">&#39;n&#39;</span></span>
<span id="cb62-15"><a href="explore-sparsity.html#cb62-15"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:trainROC"></span>
<img src="Static/figures/trainROC-1.png" alt="Train data out-of-sample ROCs" width="480" />
<p class="caption">
Figure 4.3: Train data out-of-sample ROCs
</p>
</div>
<p>Compare thresholds for 90% Specificity:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="explore-sparsity.html#cb63-1"></a> lasso_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lasso_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb63-2"><a href="explore-sparsity.html#cb63-2"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb63-3"><a href="explore-sparsity.html#cb63-3"></a></span>
<span id="cb63-4"><a href="explore-sparsity.html#cb63-4"></a> enet_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_enet_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb63-5"><a href="explore-sparsity.html#cb63-5"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb63-6"><a href="explore-sparsity.html#cb63-6"></a></span>
<span id="cb63-7"><a href="explore-sparsity.html#cb63-7"></a> lassoR_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_relaxed_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb63-8"><a href="explore-sparsity.html#cb63-8"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb63-9"><a href="explore-sparsity.html#cb63-9"></a></span>
<span id="cb63-10"><a href="explore-sparsity.html#cb63-10"></a> blended_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_blended_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb63-11"><a href="explore-sparsity.html#cb63-11"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb63-12"><a href="explore-sparsity.html#cb63-12"></a></span>
<span id="cb63-13"><a href="explore-sparsity.html#cb63-13"></a>  spec90_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">rbind</span>(</span>
<span id="cb63-14"><a href="explore-sparsity.html#cb63-14"></a>  <span class="dt">lasso=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lasso_roc, <span class="dt">transpose=</span>F))[lasso_ndx,],</span>
<span id="cb63-15"><a href="explore-sparsity.html#cb63-15"></a>  <span class="dt">enet=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_enet_roc, <span class="dt">transpose=</span>F))[enet_ndx,],</span>
<span id="cb63-16"><a href="explore-sparsity.html#cb63-16"></a>  <span class="dt">relaxed=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_relaxed_roc, <span class="dt">transpose=</span>F))[lassoR_ndx,],</span>
<span id="cb63-17"><a href="explore-sparsity.html#cb63-17"></a>  <span class="dt">blended=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_blended_roc, <span class="dt">transpose=</span>F))[blended_ndx,]</span>
<span id="cb63-18"><a href="explore-sparsity.html#cb63-18"></a> ))</span>
<span id="cb63-19"><a href="explore-sparsity.html#cb63-19"></a></span>
<span id="cb63-20"><a href="explore-sparsity.html#cb63-20"></a></span>
<span id="cb63-21"><a href="explore-sparsity.html#cb63-21"></a>knitr<span class="op">::</span><span class="kw">kable</span>(spec90_frm,</span>
<span id="cb63-22"><a href="explore-sparsity.html#cb63-22"></a>  <span class="dt">digits=</span><span class="dv">3</span>,</span>
<span id="cb63-23"><a href="explore-sparsity.html#cb63-23"></a>  <span class="dt">caption=</span><span class="st">&quot;Specificity = .90 Coordinates&quot;</span></span>
<span id="cb63-24"><a href="explore-sparsity.html#cb63-24"></a>) <span class="op">%&gt;%</span></span>
<span id="cb63-25"><a href="explore-sparsity.html#cb63-25"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:thresh90">Table 4.5: </span>Specificity = .90 Coordinates
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
threshold
</th>
<th style="text-align:right;">
specificity
</th>
<th style="text-align:right;">
sensitivity
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
lasso
</td>
<td style="text-align:right;">
0.337
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.932
</td>
</tr>
<tr>
<td style="text-align:left;">
enet
</td>
<td style="text-align:right;">
0.347
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.937
</td>
</tr>
<tr>
<td style="text-align:left;">
relaxed
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.894
</td>
</tr>
<tr>
<td style="text-align:left;">
blended
</td>
<td style="text-align:right;">
0.031
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.899
</td>
</tr>
</tbody>
</table>
<p>This is strange.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="explore-sparsity.html#cb64-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb64-2"><a href="explore-sparsity.html#cb64-2"></a></span>
<span id="cb64-3"><a href="explore-sparsity.html#cb64-3"></a><span class="co"># lasso</span></span>
<span id="cb64-4"><a href="explore-sparsity.html#cb64-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_lasso_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb64-5"><a href="explore-sparsity.html#cb64-5"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb64-6"><a href="explore-sparsity.html#cb64-6"></a>)</span>
<span id="cb64-7"><a href="explore-sparsity.html#cb64-7"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_lasso_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb64-8"><a href="explore-sparsity.html#cb64-8"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb64-9"><a href="explore-sparsity.html#cb64-9"></a>)</span>
<span id="cb64-10"><a href="explore-sparsity.html#cb64-10"></a><span class="kw">title</span>(<span class="st">&quot;lasso&quot;</span>)</span>
<span id="cb64-11"><a href="explore-sparsity.html#cb64-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Control&quot;</span>, <span class="st">&quot;HCC&quot;</span>), <span class="dt">text.col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb64-12"><a href="explore-sparsity.html#cb64-12"></a></span>
<span id="cb64-13"><a href="explore-sparsity.html#cb64-13"></a><span class="co"># enet</span></span>
<span id="cb64-14"><a href="explore-sparsity.html#cb64-14"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_enet_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb64-15"><a href="explore-sparsity.html#cb64-15"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb64-16"><a href="explore-sparsity.html#cb64-16"></a>)</span>
<span id="cb64-17"><a href="explore-sparsity.html#cb64-17"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_enet_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb64-18"><a href="explore-sparsity.html#cb64-18"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb64-19"><a href="explore-sparsity.html#cb64-19"></a>)</span>
<span id="cb64-20"><a href="explore-sparsity.html#cb64-20"></a><span class="kw">title</span>(<span class="st">&quot;enet&quot;</span>)</span>
<span id="cb64-21"><a href="explore-sparsity.html#cb64-21"></a></span>
<span id="cb64-22"><a href="explore-sparsity.html#cb64-22"></a><span class="co"># lassoR</span></span>
<span id="cb64-23"><a href="explore-sparsity.html#cb64-23"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_relaxed_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb64-24"><a href="explore-sparsity.html#cb64-24"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb64-25"><a href="explore-sparsity.html#cb64-25"></a>)</span>
<span id="cb64-26"><a href="explore-sparsity.html#cb64-26"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_relaxed_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb64-27"><a href="explore-sparsity.html#cb64-27"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb64-28"><a href="explore-sparsity.html#cb64-28"></a>)</span>
<span id="cb64-29"><a href="explore-sparsity.html#cb64-29"></a><span class="kw">title</span>(<span class="st">&quot;lassoR&quot;</span>)</span>
<span id="cb64-30"><a href="explore-sparsity.html#cb64-30"></a></span>
<span id="cb64-31"><a href="explore-sparsity.html#cb64-31"></a><span class="co"># blended</span></span>
<span id="cb64-32"><a href="explore-sparsity.html#cb64-32"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_blended_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb64-33"><a href="explore-sparsity.html#cb64-33"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb64-34"><a href="explore-sparsity.html#cb64-34"></a>)</span>
<span id="cb64-35"><a href="explore-sparsity.html#cb64-35"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_blended_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb64-36"><a href="explore-sparsity.html#cb64-36"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb64-37"><a href="explore-sparsity.html#cb64-37"></a>)</span>
<span id="cb64-38"><a href="explore-sparsity.html#cb64-38"></a><span class="kw">title</span>(<span class="st">&quot;blended&quot;</span>)</span>
<span id="cb64-39"><a href="explore-sparsity.html#cb64-39"></a></span>
<span id="cb64-40"><a href="explore-sparsity.html#cb64-40"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">outer =</span> T, <span class="st">&quot;out-of-fold predicted probability&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span>
<span id="cb64-41"><a href="explore-sparsity.html#cb64-41"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dt">outer =</span> T, <span class="st">&quot;density&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span></code></pre></div>
<div class="figure"><span id="fig:trainOOFprops"></span>
<img src="Static/figures/trainOOFprops-1.png" alt="Train data out-of-fold predicted probabilities" width="960" />
<p class="caption">
Figure 4.4: Train data out-of-fold predicted probabilities
</p>
</div>
<p>The relaxed lasso fit results in essentially dichotomized predicted probability
distribution - predicted probabilities are very close to 0 or 1.</p>
<p>Look at test data ROC curves.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="explore-sparsity.html#cb65-1"></a><span class="co"># plot all</span></span>
<span id="cb65-2"><a href="explore-sparsity.html#cb65-2"></a><span class="kw">plot</span>(test_lasso_roc, <span class="dt">col =</span> col_vec[<span class="dv">1</span>])</span>
<span id="cb65-3"><a href="explore-sparsity.html#cb65-3"></a><span class="kw">lines</span>(test_enet_roc, <span class="dt">col =</span> col_vec[<span class="dv">2</span>])</span>
<span id="cb65-4"><a href="explore-sparsity.html#cb65-4"></a><span class="kw">lines</span>(test_relaxed_roc, <span class="dt">col =</span> col_vec[<span class="dv">3</span>])</span>
<span id="cb65-5"><a href="explore-sparsity.html#cb65-5"></a><span class="kw">lines</span>(test_blended_roc, <span class="dt">col =</span> col_vec[<span class="dv">4</span>])</span>
<span id="cb65-6"><a href="explore-sparsity.html#cb65-6"></a></span>
<span id="cb65-7"><a href="explore-sparsity.html#cb65-7"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb65-8"><a href="explore-sparsity.html#cb65-8"></a>  <span class="dt">title =</span> <span class="st">&quot;AUC&quot;</span>,</span>
<span id="cb65-9"><a href="explore-sparsity.html#cb65-9"></a>  <span class="dt">legend =</span> <span class="kw">c</span>(</span>
<span id="cb65-10"><a href="explore-sparsity.html#cb65-10"></a>    <span class="kw">paste</span>(<span class="st">&quot;lasso =&quot;</span>, <span class="kw">round</span>(test_lasso_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb65-11"><a href="explore-sparsity.html#cb65-11"></a>    <span class="kw">paste</span>(<span class="st">&quot;enet =&quot;</span>, <span class="kw">round</span>(test_enet_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb65-12"><a href="explore-sparsity.html#cb65-12"></a>    <span class="kw">paste</span>(<span class="st">&quot;relaxed =&quot;</span>, <span class="kw">round</span>(test_relaxed_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb65-13"><a href="explore-sparsity.html#cb65-13"></a>    <span class="kw">paste</span>(<span class="st">&quot;blended =&quot;</span>, <span class="kw">round</span>(test_blended_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>))</span>
<span id="cb65-14"><a href="explore-sparsity.html#cb65-14"></a>  ),</span>
<span id="cb65-15"><a href="explore-sparsity.html#cb65-15"></a>  <span class="dt">text.col =</span> col_vec[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>],</span>
<span id="cb65-16"><a href="explore-sparsity.html#cb65-16"></a>  <span class="dt">bty=</span><span class="st">&#39;n&#39;</span></span>
<span id="cb65-17"><a href="explore-sparsity.html#cb65-17"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:testROC2"></span>
<img src="Static/figures/testROC2-1.png" alt="Test data out-of-sample ROCs" width="480" />
<p class="caption">
Figure 4.5: Test data out-of-sample ROCs
</p>
</div>
<p>Look at densities of predicted probabilities.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="explore-sparsity.html#cb66-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb66-2"><a href="explore-sparsity.html#cb66-2"></a></span>
<span id="cb66-3"><a href="explore-sparsity.html#cb66-3"></a><span class="co"># lasso</span></span>
<span id="cb66-4"><a href="explore-sparsity.html#cb66-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_lasso_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb66-5"><a href="explore-sparsity.html#cb66-5"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb66-6"><a href="explore-sparsity.html#cb66-6"></a>)</span>
<span id="cb66-7"><a href="explore-sparsity.html#cb66-7"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_lasso_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb66-8"><a href="explore-sparsity.html#cb66-8"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb66-9"><a href="explore-sparsity.html#cb66-9"></a>)</span>
<span id="cb66-10"><a href="explore-sparsity.html#cb66-10"></a><span class="kw">title</span>(<span class="st">&quot;lasso&quot;</span>)</span>
<span id="cb66-11"><a href="explore-sparsity.html#cb66-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Control&quot;</span>, <span class="st">&quot;HCC&quot;</span>), <span class="dt">text.col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb66-12"><a href="explore-sparsity.html#cb66-12"></a></span>
<span id="cb66-13"><a href="explore-sparsity.html#cb66-13"></a><span class="co"># enet</span></span>
<span id="cb66-14"><a href="explore-sparsity.html#cb66-14"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_enet_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb66-15"><a href="explore-sparsity.html#cb66-15"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb66-16"><a href="explore-sparsity.html#cb66-16"></a>)</span>
<span id="cb66-17"><a href="explore-sparsity.html#cb66-17"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_enet_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb66-18"><a href="explore-sparsity.html#cb66-18"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb66-19"><a href="explore-sparsity.html#cb66-19"></a>)</span>
<span id="cb66-20"><a href="explore-sparsity.html#cb66-20"></a><span class="kw">title</span>(<span class="st">&quot;enet&quot;</span>)</span>
<span id="cb66-21"><a href="explore-sparsity.html#cb66-21"></a></span>
<span id="cb66-22"><a href="explore-sparsity.html#cb66-22"></a><span class="co"># relaxed</span></span>
<span id="cb66-23"><a href="explore-sparsity.html#cb66-23"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_relaxed_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb66-24"><a href="explore-sparsity.html#cb66-24"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb66-25"><a href="explore-sparsity.html#cb66-25"></a>)</span>
<span id="cb66-26"><a href="explore-sparsity.html#cb66-26"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_relaxed_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb66-27"><a href="explore-sparsity.html#cb66-27"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb66-28"><a href="explore-sparsity.html#cb66-28"></a>)</span>
<span id="cb66-29"><a href="explore-sparsity.html#cb66-29"></a><span class="kw">title</span>(<span class="st">&quot;relaxed&quot;</span>)</span>
<span id="cb66-30"><a href="explore-sparsity.html#cb66-30"></a></span>
<span id="cb66-31"><a href="explore-sparsity.html#cb66-31"></a><span class="co">#sapply(split(test_relaxed_predProb_vec, test_group_vec), summary)</span></span>
<span id="cb66-32"><a href="explore-sparsity.html#cb66-32"></a></span>
<span id="cb66-33"><a href="explore-sparsity.html#cb66-33"></a></span>
<span id="cb66-34"><a href="explore-sparsity.html#cb66-34"></a><span class="co"># blended</span></span>
<span id="cb66-35"><a href="explore-sparsity.html#cb66-35"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_blended_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb66-36"><a href="explore-sparsity.html#cb66-36"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb66-37"><a href="explore-sparsity.html#cb66-37"></a>)</span>
<span id="cb66-38"><a href="explore-sparsity.html#cb66-38"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_blended_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb66-39"><a href="explore-sparsity.html#cb66-39"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb66-40"><a href="explore-sparsity.html#cb66-40"></a>)</span>
<span id="cb66-41"><a href="explore-sparsity.html#cb66-41"></a><span class="kw">title</span>(<span class="st">&quot;blended&quot;</span>)</span>
<span id="cb66-42"><a href="explore-sparsity.html#cb66-42"></a></span>
<span id="cb66-43"><a href="explore-sparsity.html#cb66-43"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">outer =</span> T, <span class="st">&quot;test set predicted probability&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span>
<span id="cb66-44"><a href="explore-sparsity.html#cb66-44"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dt">outer =</span> T, <span class="st">&quot;density&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span></code></pre></div>
<div class="figure"><span id="fig:testOOFprobs"></span>
<img src="Static/figures/testOOFprobs-1.png" alt="Test data out-of-fold predicted probabilities" width="960" />
<p class="caption">
Figure 4.6: Test data out-of-fold predicted probabilities
</p>
</div>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="explore-sparsity.html#cb67-1"></a><span class="co"># Define plotting function</span></span>
<span id="cb67-2"><a href="explore-sparsity.html#cb67-2"></a>bxpPredProb_f &lt;-<span class="st"> </span><span class="cf">function</span>(cv_fit, <span class="dt">Gamma=</span><span class="ot">NULL</span>) {</span>
<span id="cb67-3"><a href="explore-sparsity.html#cb67-3"></a>  <span class="co"># Train - preval is out-of-fold linear predictor for training design points</span></span>
<span id="cb67-4"><a href="explore-sparsity.html#cb67-4"></a>  onese_ndx &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_fit<span class="op">$</span>lambda)</span>
<span id="cb67-5"><a href="explore-sparsity.html#cb67-5"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(Gamma)) </span>
<span id="cb67-6"><a href="explore-sparsity.html#cb67-6"></a>   train_1se_preval_vec &lt;-<span class="st"> </span>cv_fit<span class="op">$</span>fit.preval[, onese_ndx] <span class="cf">else</span></span>
<span id="cb67-7"><a href="explore-sparsity.html#cb67-7"></a>   train_1se_preval_vec &lt;-<span class="st"> </span>cv_fit<span class="op">$</span>fit.preval[[Gamma]][, onese_ndx] </span>
<span id="cb67-8"><a href="explore-sparsity.html#cb67-8"></a></span>
<span id="cb67-9"><a href="explore-sparsity.html#cb67-9"></a>  train_1se_predProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(train_1se_preval_vec)</span>
<span id="cb67-10"><a href="explore-sparsity.html#cb67-10"></a></span>
<span id="cb67-11"><a href="explore-sparsity.html#cb67-11"></a>  <span class="co"># Test</span></span>
<span id="cb67-12"><a href="explore-sparsity.html#cb67-12"></a>  test_1se_predProb_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb67-13"><a href="explore-sparsity.html#cb67-13"></a>    cv_fit,</span>
<span id="cb67-14"><a href="explore-sparsity.html#cb67-14"></a>    <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb67-15"><a href="explore-sparsity.html#cb67-15"></a>    <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb67-16"><a href="explore-sparsity.html#cb67-16"></a>    <span class="dt">type =</span> <span class="st">&quot;resp&quot;</span></span>
<span id="cb67-17"><a href="explore-sparsity.html#cb67-17"></a>  )</span>
<span id="cb67-18"><a href="explore-sparsity.html#cb67-18"></a></span>
<span id="cb67-19"><a href="explore-sparsity.html#cb67-19"></a>  tmp &lt;-<span class="st"> </span><span class="kw">c</span>(</span>
<span id="cb67-20"><a href="explore-sparsity.html#cb67-20"></a>    <span class="dt">train =</span> <span class="kw">split</span>(train_1se_predProb_vec, train_group_vec),</span>
<span id="cb67-21"><a href="explore-sparsity.html#cb67-21"></a>    <span class="dt">test =</span> <span class="kw">split</span>(test_1se_predProb_vec, test_group_vec)</span>
<span id="cb67-22"><a href="explore-sparsity.html#cb67-22"></a>  )</span>
<span id="cb67-23"><a href="explore-sparsity.html#cb67-23"></a>  <span class="kw">names</span>(tmp) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">sub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">names</span>(tmp)))</span>
<span id="cb67-24"><a href="explore-sparsity.html#cb67-24"></a></span>
<span id="cb67-25"><a href="explore-sparsity.html#cb67-25"></a>  <span class="kw">boxplot</span>(tmp)</span>
<span id="cb67-26"><a href="explore-sparsity.html#cb67-26"></a>}</span>
<span id="cb67-27"><a href="explore-sparsity.html#cb67-27"></a></span>
<span id="cb67-28"><a href="explore-sparsity.html#cb67-28"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb67-29"><a href="explore-sparsity.html#cb67-29"></a></span>
<span id="cb67-30"><a href="explore-sparsity.html#cb67-30"></a><span class="kw">bxpPredProb_f</span>(cv_lasso)</span>
<span id="cb67-31"><a href="explore-sparsity.html#cb67-31"></a><span class="kw">title</span>(<span class="st">&#39;lasso&#39;</span>)</span>
<span id="cb67-32"><a href="explore-sparsity.html#cb67-32"></a></span>
<span id="cb67-33"><a href="explore-sparsity.html#cb67-33"></a><span class="kw">bxpPredProb_f</span>(cv_enet)</span>
<span id="cb67-34"><a href="explore-sparsity.html#cb67-34"></a><span class="kw">title</span>(<span class="st">&#39;enet&#39;</span>)</span>
<span id="cb67-35"><a href="explore-sparsity.html#cb67-35"></a></span>
<span id="cb67-36"><a href="explore-sparsity.html#cb67-36"></a><span class="kw">bxpPredProb_f</span>(cv_lassoR, <span class="dt">Gamma=</span><span class="st">&#39;g:0&#39;</span>)</span>
<span id="cb67-37"><a href="explore-sparsity.html#cb67-37"></a><span class="kw">title</span>(<span class="st">&#39;relaxed&#39;</span>)</span>
<span id="cb67-38"><a href="explore-sparsity.html#cb67-38"></a></span>
<span id="cb67-39"><a href="explore-sparsity.html#cb67-39"></a><span class="kw">bxpPredProb_f</span>(cv_lassoR, <span class="dt">Gamma=</span><span class="st">&#39;g:0.5&#39;</span>)</span>
<span id="cb67-40"><a href="explore-sparsity.html#cb67-40"></a><span class="kw">title</span>(<span class="st">&#39;blended&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fitPrevalByGroup"></span>
<img src="Static/figures/fitPrevalByGroup-1.png" alt="Predicted Probabilities - Train and Test" width="768" />
<p class="caption">
Figure 4.7: Predicted Probabilities - Train and Test
</p>
</div>
<!--
Another look - plot train and test set logistic curves with annotation.

The following shows that predicted classes come from fitted
probabilities - not out of sample probabilities.

Also shows that threshold is at 0.5 

SKIP
-->
<p>We have seen above that assessments of model performance based on the out-of-fold
predicted values are close to the test set assessments, and that
assessments based on prediction extracted from glmnet object are optimistic.
Here we look at confusion matrices to see how this affects the
classification results.</p>
<p>Here we us a threshold of 0.5 to dichotomize the predicted
probabilities into a class prediction, as is done in the
glmnet predictions.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="explore-sparsity.html#cb68-1"></a><span class="co"># lasso </span></span>
<span id="cb68-2"><a href="explore-sparsity.html#cb68-2"></a><span class="co">##########################</span></span>
<span id="cb68-3"><a href="explore-sparsity.html#cb68-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb68-4"><a href="explore-sparsity.html#cb68-4"></a>train_lasso_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-5"><a href="explore-sparsity.html#cb68-5"></a> cv_lasso,</span>
<span id="cb68-6"><a href="explore-sparsity.html#cb68-6"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb68-7"><a href="explore-sparsity.html#cb68-7"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-8"><a href="explore-sparsity.html#cb68-8"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-9"><a href="explore-sparsity.html#cb68-9"></a>)</span>
<span id="cb68-10"><a href="explore-sparsity.html#cb68-10"></a></span>
<span id="cb68-11"><a href="explore-sparsity.html#cb68-11"></a><span class="co"># train - oof</span></span>
<span id="cb68-12"><a href="explore-sparsity.html#cb68-12"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lasso<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lasso<span class="op">$</span>lambda)</span>
<span id="cb68-13"><a href="explore-sparsity.html#cb68-13"></a>train_lasso_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lasso<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb68-14"><a href="explore-sparsity.html#cb68-14"></a>train_lasso_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb68-15"><a href="explore-sparsity.html#cb68-15"></a>   train_lasso_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb68-16"><a href="explore-sparsity.html#cb68-16"></a></span>
<span id="cb68-17"><a href="explore-sparsity.html#cb68-17"></a><span class="co"># test </span></span>
<span id="cb68-18"><a href="explore-sparsity.html#cb68-18"></a>test_lasso_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-19"><a href="explore-sparsity.html#cb68-19"></a> cv_lasso,</span>
<span id="cb68-20"><a href="explore-sparsity.html#cb68-20"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb68-21"><a href="explore-sparsity.html#cb68-21"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-22"><a href="explore-sparsity.html#cb68-22"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-23"><a href="explore-sparsity.html#cb68-23"></a>)</span>
<span id="cb68-24"><a href="explore-sparsity.html#cb68-24"></a></span>
<span id="cb68-25"><a href="explore-sparsity.html#cb68-25"></a><span class="co"># enet</span></span>
<span id="cb68-26"><a href="explore-sparsity.html#cb68-26"></a><span class="co">##########################</span></span>
<span id="cb68-27"><a href="explore-sparsity.html#cb68-27"></a><span class="co"># train - cv predicted</span></span>
<span id="cb68-28"><a href="explore-sparsity.html#cb68-28"></a>train_enet_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-29"><a href="explore-sparsity.html#cb68-29"></a> cv_enet,</span>
<span id="cb68-30"><a href="explore-sparsity.html#cb68-30"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb68-31"><a href="explore-sparsity.html#cb68-31"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-32"><a href="explore-sparsity.html#cb68-32"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-33"><a href="explore-sparsity.html#cb68-33"></a>)</span>
<span id="cb68-34"><a href="explore-sparsity.html#cb68-34"></a></span>
<span id="cb68-35"><a href="explore-sparsity.html#cb68-35"></a><span class="co"># train - oof</span></span>
<span id="cb68-36"><a href="explore-sparsity.html#cb68-36"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_enet<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_enet<span class="op">$</span>lambda)</span>
<span id="cb68-37"><a href="explore-sparsity.html#cb68-37"></a>train_enet_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_enet<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb68-38"><a href="explore-sparsity.html#cb68-38"></a>train_enet_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb68-39"><a href="explore-sparsity.html#cb68-39"></a>   train_enet_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb68-40"><a href="explore-sparsity.html#cb68-40"></a></span>
<span id="cb68-41"><a href="explore-sparsity.html#cb68-41"></a><span class="co"># test</span></span>
<span id="cb68-42"><a href="explore-sparsity.html#cb68-42"></a>test_enet_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-43"><a href="explore-sparsity.html#cb68-43"></a> cv_enet,</span>
<span id="cb68-44"><a href="explore-sparsity.html#cb68-44"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb68-45"><a href="explore-sparsity.html#cb68-45"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-46"><a href="explore-sparsity.html#cb68-46"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-47"><a href="explore-sparsity.html#cb68-47"></a>)</span>
<span id="cb68-48"><a href="explore-sparsity.html#cb68-48"></a></span>
<span id="cb68-49"><a href="explore-sparsity.html#cb68-49"></a></span>
<span id="cb68-50"><a href="explore-sparsity.html#cb68-50"></a></span>
<span id="cb68-51"><a href="explore-sparsity.html#cb68-51"></a><span class="co"># relaxed lasso (gamma=0)</span></span>
<span id="cb68-52"><a href="explore-sparsity.html#cb68-52"></a><span class="co">##########################</span></span>
<span id="cb68-53"><a href="explore-sparsity.html#cb68-53"></a><span class="co"># train - cv predicted</span></span>
<span id="cb68-54"><a href="explore-sparsity.html#cb68-54"></a>train_relaxed_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-55"><a href="explore-sparsity.html#cb68-55"></a> cv_lassoR,</span>
<span id="cb68-56"><a href="explore-sparsity.html#cb68-56"></a> <span class="dt">g=</span><span class="dv">0</span>,</span>
<span id="cb68-57"><a href="explore-sparsity.html#cb68-57"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb68-58"><a href="explore-sparsity.html#cb68-58"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-59"><a href="explore-sparsity.html#cb68-59"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-60"><a href="explore-sparsity.html#cb68-60"></a>)</span>
<span id="cb68-61"><a href="explore-sparsity.html#cb68-61"></a></span>
<span id="cb68-62"><a href="explore-sparsity.html#cb68-62"></a><span class="co"># RECALL: cv_lassoR$nzero[cv_lassoR$lambda==cv_lassoR$lambda.1se]</span></span>
<span id="cb68-63"><a href="explore-sparsity.html#cb68-63"></a><span class="co"># train - oof</span></span>
<span id="cb68-64"><a href="explore-sparsity.html#cb68-64"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb68-65"><a href="explore-sparsity.html#cb68-65"></a>train_relaxed_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0&#39;</span>]][,ndx_1se])</span>
<span id="cb68-66"><a href="explore-sparsity.html#cb68-66"></a>train_relaxed_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb68-67"><a href="explore-sparsity.html#cb68-67"></a>   train_relaxed_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb68-68"><a href="explore-sparsity.html#cb68-68"></a></span>
<span id="cb68-69"><a href="explore-sparsity.html#cb68-69"></a><span class="co"># test </span></span>
<span id="cb68-70"><a href="explore-sparsity.html#cb68-70"></a>test_relaxed_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-71"><a href="explore-sparsity.html#cb68-71"></a> cv_lassoR,</span>
<span id="cb68-72"><a href="explore-sparsity.html#cb68-72"></a> <span class="dt">g=</span><span class="dv">0</span>,</span>
<span id="cb68-73"><a href="explore-sparsity.html#cb68-73"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb68-74"><a href="explore-sparsity.html#cb68-74"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-75"><a href="explore-sparsity.html#cb68-75"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-76"><a href="explore-sparsity.html#cb68-76"></a>)</span>
<span id="cb68-77"><a href="explore-sparsity.html#cb68-77"></a></span>
<span id="cb68-78"><a href="explore-sparsity.html#cb68-78"></a></span>
<span id="cb68-79"><a href="explore-sparsity.html#cb68-79"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb68-80"><a href="explore-sparsity.html#cb68-80"></a><span class="co">###############################</span></span>
<span id="cb68-81"><a href="explore-sparsity.html#cb68-81"></a><span class="co"># train - cv predicted</span></span>
<span id="cb68-82"><a href="explore-sparsity.html#cb68-82"></a>train_blended_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-83"><a href="explore-sparsity.html#cb68-83"></a> cv_lassoR,</span>
<span id="cb68-84"><a href="explore-sparsity.html#cb68-84"></a> <span class="dt">g=</span><span class="fl">0.5</span>,</span>
<span id="cb68-85"><a href="explore-sparsity.html#cb68-85"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb68-86"><a href="explore-sparsity.html#cb68-86"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-87"><a href="explore-sparsity.html#cb68-87"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb68-88"><a href="explore-sparsity.html#cb68-88"></a>)</span>
<span id="cb68-89"><a href="explore-sparsity.html#cb68-89"></a></span>
<span id="cb68-90"><a href="explore-sparsity.html#cb68-90"></a><span class="co"># RECALL $`r cv_lassoR$relaxed$nzero.1se`$ features (vertical </span></span>
<span id="cb68-91"><a href="explore-sparsity.html#cb68-91"></a><span class="co">#  cv_blended_statlist &lt;- cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]]</span></span>
<span id="cb68-92"><a href="explore-sparsity.html#cb68-92"></a><span class="co">#  cv_blended_1se_error &lt;- cv_blended_statlist$cvm[cv_blended_statlist$lambda==</span></span>
<span id="cb68-93"><a href="explore-sparsity.html#cb68-93"></a>      <span class="co">#cv_lassoR$relaxed$lambda.1se]</span></span>
<span id="cb68-94"><a href="explore-sparsity.html#cb68-94"></a></span>
<span id="cb68-95"><a href="explore-sparsity.html#cb68-95"></a><span class="co"># train - oof</span></span>
<span id="cb68-96"><a href="explore-sparsity.html#cb68-96"></a>cv_blended_statlist &lt;-<span class="st"> </span>cv_lassoR<span class="op">$</span>relaxed<span class="op">$</span>statlist[[<span class="st">&#39;g:0.5&#39;</span>]]</span>
<span id="cb68-97"><a href="explore-sparsity.html#cb68-97"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>relaxed<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_blended_statlist<span class="op">$</span>lambda)</span>
<span id="cb68-98"><a href="explore-sparsity.html#cb68-98"></a>train_blended_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0.5&#39;</span>]][,ndx_1se])</span>
<span id="cb68-99"><a href="explore-sparsity.html#cb68-99"></a>train_blended_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb68-100"><a href="explore-sparsity.html#cb68-100"></a>   train_blended_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb68-101"><a href="explore-sparsity.html#cb68-101"></a></span>
<span id="cb68-102"><a href="explore-sparsity.html#cb68-102"></a><span class="co"># test</span></span>
<span id="cb68-103"><a href="explore-sparsity.html#cb68-103"></a>test_blended_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb68-104"><a href="explore-sparsity.html#cb68-104"></a> cv_lassoR,</span>
<span id="cb68-105"><a href="explore-sparsity.html#cb68-105"></a> <span class="dt">g=</span><span class="fl">0.5</span>,</span>
<span id="cb68-106"><a href="explore-sparsity.html#cb68-106"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb68-107"><a href="explore-sparsity.html#cb68-107"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb68-108"><a href="explore-sparsity.html#cb68-108"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span> </span>
<span id="cb68-109"><a href="explore-sparsity.html#cb68-109"></a>)</span>
<span id="cb68-110"><a href="explore-sparsity.html#cb68-110"></a></span>
<span id="cb68-111"><a href="explore-sparsity.html#cb68-111"></a><span class="co"># put it all together</span></span>
<span id="cb68-112"><a href="explore-sparsity.html#cb68-112"></a><span class="co">########################</span></span>
<span id="cb68-113"><a href="explore-sparsity.html#cb68-113"></a>all_models_confustion_mtx &lt;-<span class="st"> </span><span class="kw">rbind</span>(</span>
<span id="cb68-114"><a href="explore-sparsity.html#cb68-114"></a> <span class="dt">train_lasso_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lasso_predClass_vec, train_group_vec)),</span>
<span id="cb68-115"><a href="explore-sparsity.html#cb68-115"></a> <span class="dt">train_lasso_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lasso_oofClass_vec, train_group_vec)),</span>
<span id="cb68-116"><a href="explore-sparsity.html#cb68-116"></a> <span class="dt">test_lasso =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_lasso_predClass_vec, test_group_vec)),</span>
<span id="cb68-117"><a href="explore-sparsity.html#cb68-117"></a></span>
<span id="cb68-118"><a href="explore-sparsity.html#cb68-118"></a> <span class="dt">train_enet_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_enet_predClass_vec, train_group_vec)),</span>
<span id="cb68-119"><a href="explore-sparsity.html#cb68-119"></a> <span class="dt">train_enet_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_enet_oofClass_vec, train_group_vec)),</span>
<span id="cb68-120"><a href="explore-sparsity.html#cb68-120"></a> <span class="dt">test_enet =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_enet_predClass_vec, test_group_vec)),</span>
<span id="cb68-121"><a href="explore-sparsity.html#cb68-121"></a></span>
<span id="cb68-122"><a href="explore-sparsity.html#cb68-122"></a> <span class="dt">train_relaxed_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_relaxed_predClass_vec, train_group_vec)),</span>
<span id="cb68-123"><a href="explore-sparsity.html#cb68-123"></a> <span class="dt">train_relaxed_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_relaxed_oofClass_vec, train_group_vec)),</span>
<span id="cb68-124"><a href="explore-sparsity.html#cb68-124"></a> <span class="dt">test_relaxed =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_relaxed_predClass_vec, test_group_vec)),</span>
<span id="cb68-125"><a href="explore-sparsity.html#cb68-125"></a></span>
<span id="cb68-126"><a href="explore-sparsity.html#cb68-126"></a> <span class="dt">train_blended_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_blended_predClass_vec, train_group_vec)),</span>
<span id="cb68-127"><a href="explore-sparsity.html#cb68-127"></a> <span class="dt">train_blended_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_blended_oofClass_vec, train_group_vec)),</span>
<span id="cb68-128"><a href="explore-sparsity.html#cb68-128"></a> <span class="dt">test_blended =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_blended_predClass_vec, test_group_vec))</span>
<span id="cb68-129"><a href="explore-sparsity.html#cb68-129"></a>)</span>
<span id="cb68-130"><a href="explore-sparsity.html#cb68-130"></a><span class="kw">colnames</span>(all_models_confustion_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;C:C&#39;</span>,<span class="st">&#39;C:H&#39;</span>,<span class="st">&#39;H:C&#39;</span>, <span class="st">&#39;H:H&#39;</span>)</span>
<span id="cb68-131"><a href="explore-sparsity.html#cb68-131"></a></span>
<span id="cb68-132"><a href="explore-sparsity.html#cb68-132"></a></span>
<span id="cb68-133"><a href="explore-sparsity.html#cb68-133"></a>all_models_confustionRates_mtx &lt;-<span class="st"> </span><span class="kw">sweep</span>(</span>
<span id="cb68-134"><a href="explore-sparsity.html#cb68-134"></a> all_models_confustion_mtx, <span class="dv">1</span>, <span class="kw">rowSums</span>(all_models_confustion_mtx), <span class="st">&#39;/&#39;</span>)</span>
<span id="cb68-135"><a href="explore-sparsity.html#cb68-135"></a></span>
<span id="cb68-136"><a href="explore-sparsity.html#cb68-136"></a>all_models_confustionRates_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(all_models_confustionRates_mtx,</span>
<span id="cb68-137"><a href="explore-sparsity.html#cb68-137"></a>  <span class="dt">error =</span> <span class="kw">rowSums</span>(all_models_confustionRates_mtx[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]))</span>
<span id="cb68-138"><a href="explore-sparsity.html#cb68-138"></a></span>
<span id="cb68-139"><a href="explore-sparsity.html#cb68-139"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dv">100</span><span class="op">*</span>all_models_confustionRates_mtx, </span>
<span id="cb68-140"><a href="explore-sparsity.html#cb68-140"></a>  <span class="dt">caption=</span><span class="st">&quot;confusion: Columns are Truth:Predicted&quot;</span>,</span>
<span id="cb68-141"><a href="explore-sparsity.html#cb68-141"></a>  <span class="dt">digits=</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb68-142"><a href="explore-sparsity.html#cb68-142"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:confMtxTrainLasso">Table 4.6: </span>confusion: Columns are Truth:Predicted
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
C:C
</th>
<th style="text-align:right;">
C:H
</th>
<th style="text-align:right;">
H:C
</th>
<th style="text-align:right;">
H:H
</th>
<th style="text-align:right;">
error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
train_lasso_cv
</td>
<td style="text-align:right;">
57.6
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
2.9
</td>
<td style="text-align:right;">
38.7
</td>
<td style="text-align:right;">
3.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_lasso_oof
</td>
<td style="text-align:right;">
56.7
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
5.3
</td>
<td style="text-align:right;">
36.3
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
test_lasso
</td>
<td style="text-align:right;">
55.6
</td>
<td style="text-align:right;">
2.6
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
34.2
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
train_enet_cv
</td>
<td style="text-align:right;">
57.6
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
3.8
</td>
<td style="text-align:right;">
37.8
</td>
<td style="text-align:right;">
4.6
</td>
</tr>
<tr>
<td style="text-align:left;">
train_enet_oof
</td>
<td style="text-align:right;">
57.1
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
5.9
</td>
<td style="text-align:right;">
35.7
</td>
<td style="text-align:right;">
7.2
</td>
</tr>
<tr>
<td style="text-align:left;">
test_enet
</td>
<td style="text-align:right;">
55.3
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
32.7
</td>
<td style="text-align:right;">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;">
train_relaxed_cv
</td>
<td style="text-align:right;">
56.7
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
38.6
</td>
<td style="text-align:right;">
4.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_relaxed_oof
</td>
<td style="text-align:right;">
53.8
</td>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
6.4
</td>
<td style="text-align:right;">
35.2
</td>
<td style="text-align:right;">
11.0
</td>
</tr>
<tr>
<td style="text-align:left;">
test_relaxed
</td>
<td style="text-align:right;">
54.1
</td>
<td style="text-align:right;">
4.1
</td>
<td style="text-align:right;">
6.8
</td>
<td style="text-align:right;">
35.0
</td>
<td style="text-align:right;">
10.9
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_cv
</td>
<td style="text-align:right;">
57.1
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
38.2
</td>
<td style="text-align:right;">
4.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_oof
</td>
<td style="text-align:right;">
56.0
</td>
<td style="text-align:right;">
2.4
</td>
<td style="text-align:right;">
4.3
</td>
<td style="text-align:right;">
37.3
</td>
<td style="text-align:right;">
6.7
</td>
</tr>
<tr>
<td style="text-align:left;">
test_blended
</td>
<td style="text-align:right;">
54.9
</td>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
6.8
</td>
<td style="text-align:right;">
35.0
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
</tbody>
</table>
<p>The out-of-fold error rates are larger for the relaxed lasso and blended fit models.
On the test set, errors are slightly higher for the elastic net model.</p>
</div>
</div>
<div id="compare-predictions-at-misclassified-samples" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Compare predictions at misclassified samples</h2>
<p>It is useful to examine classification errors more carefully.
If models have different failure modes, one might get improved
performance by combining model predictions. Note that the models
considered here are not expected to compliment each other usefully
as they are too similar in nature.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="explore-sparsity.html#cb69-1"></a>misclass_id_vec &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">c</span>(</span>
<span id="cb69-2"><a href="explore-sparsity.html#cb69-2"></a> <span class="kw">names</span>(train_lasso_oofClass_vec)[train_lasso_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb69-3"><a href="explore-sparsity.html#cb69-3"></a> <span class="kw">names</span>(train_enet_oofClass_vec)[train_enet_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb69-4"><a href="explore-sparsity.html#cb69-4"></a> <span class="kw">names</span>(train_relaxed_oofClass_vec)[train_relaxed_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb69-5"><a href="explore-sparsity.html#cb69-5"></a> <span class="kw">names</span>(train_blended_oofClass_vec)[train_blended_oofClass_vec<span class="op">!=</span>train_group_vec]</span>
<span id="cb69-6"><a href="explore-sparsity.html#cb69-6"></a> )</span>
<span id="cb69-7"><a href="explore-sparsity.html#cb69-7"></a>)</span>
<span id="cb69-8"><a href="explore-sparsity.html#cb69-8"></a></span>
<span id="cb69-9"><a href="explore-sparsity.html#cb69-9"></a></span>
<span id="cb69-10"><a href="explore-sparsity.html#cb69-10"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb69-11"><a href="explore-sparsity.html#cb69-11"></a> train_lasso_oofProb_vec[misclass_id_vec],</span>
<span id="cb69-12"><a href="explore-sparsity.html#cb69-12"></a> train_enet_oofProb_vec[misclass_id_vec],</span>
<span id="cb69-13"><a href="explore-sparsity.html#cb69-13"></a> train_relaxed_oofProb_vec[misclass_id_vec],</span>
<span id="cb69-14"><a href="explore-sparsity.html#cb69-14"></a> train_blended_oofProb_vec[misclass_id_vec]</span>
<span id="cb69-15"><a href="explore-sparsity.html#cb69-15"></a>)</span>
<span id="cb69-16"><a href="explore-sparsity.html#cb69-16"></a><span class="kw">colnames</span>(missclass_oofProb_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;lasso&#39;</span>,<span class="st">&#39;enet&#39;</span>, <span class="st">&#39;lassoR&#39;</span>, <span class="st">&#39;blended&#39;</span>)</span>
<span id="cb69-17"><a href="explore-sparsity.html#cb69-17"></a></span>
<span id="cb69-18"><a href="explore-sparsity.html#cb69-18"></a>row_med_vec &lt;-<span class="st"> </span><span class="kw">apply</span>(missclass_oofProb_mtx, <span class="dv">1</span>, median)</span>
<span id="cb69-19"><a href="explore-sparsity.html#cb69-19"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span>missclass_oofProb_mtx[</span>
<span id="cb69-20"><a href="explore-sparsity.html#cb69-20"></a>  <span class="kw">order</span>(train_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)], row_med_vec),]</span>
<span id="cb69-21"><a href="explore-sparsity.html#cb69-21"></a></span>
<span id="cb69-22"><a href="explore-sparsity.html#cb69-22"></a><span class="kw">plot</span>(</span>
<span id="cb69-23"><a href="explore-sparsity.html#cb69-23"></a> <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(missclass_oofProb_mtx)), <span class="dt">xlab=</span><span class="st">&#39;samples&#39;</span>,</span>
<span id="cb69-24"><a href="explore-sparsity.html#cb69-24"></a> <span class="dt">y=</span><span class="kw">range</span>(missclass_oofProb_mtx), <span class="dt">ylab=</span><span class="st">&#39;out-of-fold predicted probability&#39;</span>,</span>
<span id="cb69-25"><a href="explore-sparsity.html#cb69-25"></a> <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">type=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb69-26"><a href="explore-sparsity.html#cb69-26"></a></span>
<span id="cb69-27"><a href="explore-sparsity.html#cb69-27"></a><span class="cf">for</span>(RR <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(missclass_oofProb_mtx))</span>
<span id="cb69-28"><a href="explore-sparsity.html#cb69-28"></a><span class="kw">points</span>(</span>
<span id="cb69-29"><a href="explore-sparsity.html#cb69-29"></a> <span class="kw">rep</span>(RR, <span class="kw">ncol</span>(missclass_oofProb_mtx)), </span>
<span id="cb69-30"><a href="explore-sparsity.html#cb69-30"></a> missclass_oofProb_mtx[RR,],</span>
<span id="cb69-31"><a href="explore-sparsity.html#cb69-31"></a> <span class="dt">col=</span><span class="kw">ifelse</span>(train_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)[RR]] <span class="op">==</span><span class="st"> &#39;Control&#39;</span>,</span>
<span id="cb69-32"><a href="explore-sparsity.html#cb69-32"></a>  <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>),</span>
<span id="cb69-33"><a href="explore-sparsity.html#cb69-33"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(missclass_oofProb_mtx))</span>
<span id="cb69-34"><a href="explore-sparsity.html#cb69-34"></a></span>
<span id="cb69-35"><a href="explore-sparsity.html#cb69-35"></a><span class="kw">legend</span>(<span class="st">&#39;top&#39;</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(missclass_oofProb_mtx), </span>
<span id="cb69-36"><a href="explore-sparsity.html#cb69-36"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb69-37"><a href="explore-sparsity.html#cb69-37"></a></span>
<span id="cb69-38"><a href="explore-sparsity.html#cb69-38"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:misclassTrain"></span>
<img src="Static/figures/misclassTrain-1.png" alt="out-of-fold predicted probabilities at miscassified samples" width="768" />
<p class="caption">
Figure 4.8: out-of-fold predicted probabilities at miscassified samples
</p>
</div>
<p>As we’ve seen above, predictions from lassoR and the blended mix model
are basically dichotomous; 0 or 1. Samples have been order by group, and
median P(HCC) within group. For the Controls (green), predicted probabilities
less than 0.5 are considered correct here. For the HCC (red) samples,
predicted probabilities greater than 0.5 are considered correct here.</p>
<p>Now look at the same plot on the test data set.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="explore-sparsity.html#cb70-1"></a>misclass_id_vec &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">c</span>(</span>
<span id="cb70-2"><a href="explore-sparsity.html#cb70-2"></a> <span class="kw">names</span>(test_lasso_predClass_vec[,<span class="dv">1</span>])[test_lasso_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb70-3"><a href="explore-sparsity.html#cb70-3"></a> <span class="kw">names</span>(test_enet_predClass_vec[,<span class="dv">1</span>])[test_enet_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb70-4"><a href="explore-sparsity.html#cb70-4"></a> <span class="kw">names</span>(test_relaxed_predClass_vec[,<span class="dv">1</span>])[test_relaxed_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb70-5"><a href="explore-sparsity.html#cb70-5"></a> <span class="kw">names</span>(test_blended_predClass_vec[,<span class="dv">1</span>])[test_blended_predClass_vec<span class="op">!=</span>test_group_vec]</span>
<span id="cb70-6"><a href="explore-sparsity.html#cb70-6"></a> )</span>
<span id="cb70-7"><a href="explore-sparsity.html#cb70-7"></a>)</span>
<span id="cb70-8"><a href="explore-sparsity.html#cb70-8"></a></span>
<span id="cb70-9"><a href="explore-sparsity.html#cb70-9"></a></span>
<span id="cb70-10"><a href="explore-sparsity.html#cb70-10"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb70-11"><a href="explore-sparsity.html#cb70-11"></a> test_lasso_predProb_vec[misclass_id_vec,],</span>
<span id="cb70-12"><a href="explore-sparsity.html#cb70-12"></a> test_enet_predProb_vec[misclass_id_vec,],</span>
<span id="cb70-13"><a href="explore-sparsity.html#cb70-13"></a> test_relaxed_predProb_vec[misclass_id_vec,],</span>
<span id="cb70-14"><a href="explore-sparsity.html#cb70-14"></a> test_blended_predProb_vec[misclass_id_vec,]</span>
<span id="cb70-15"><a href="explore-sparsity.html#cb70-15"></a>)</span>
<span id="cb70-16"><a href="explore-sparsity.html#cb70-16"></a><span class="kw">colnames</span>(missclass_oofProb_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;lasso&#39;</span>,<span class="st">&#39;enet&#39;</span>, <span class="st">&#39;lassoR&#39;</span>, <span class="st">&#39;blended&#39;</span>)</span>
<span id="cb70-17"><a href="explore-sparsity.html#cb70-17"></a></span>
<span id="cb70-18"><a href="explore-sparsity.html#cb70-18"></a>row_med_vec &lt;-<span class="st"> </span><span class="kw">apply</span>(missclass_oofProb_mtx, <span class="dv">1</span>, median)</span>
<span id="cb70-19"><a href="explore-sparsity.html#cb70-19"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span>missclass_oofProb_mtx[</span>
<span id="cb70-20"><a href="explore-sparsity.html#cb70-20"></a>  <span class="kw">order</span>(test_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)], row_med_vec),]</span>
<span id="cb70-21"><a href="explore-sparsity.html#cb70-21"></a></span>
<span id="cb70-22"><a href="explore-sparsity.html#cb70-22"></a><span class="kw">plot</span>(</span>
<span id="cb70-23"><a href="explore-sparsity.html#cb70-23"></a> <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(missclass_oofProb_mtx)), <span class="dt">xlab=</span><span class="st">&#39;samples&#39;</span>,</span>
<span id="cb70-24"><a href="explore-sparsity.html#cb70-24"></a> <span class="dt">y=</span><span class="kw">range</span>(missclass_oofProb_mtx), <span class="dt">ylab=</span><span class="st">&#39;out-of-fold predicted probability&#39;</span>,</span>
<span id="cb70-25"><a href="explore-sparsity.html#cb70-25"></a> <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">type=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb70-26"><a href="explore-sparsity.html#cb70-26"></a></span>
<span id="cb70-27"><a href="explore-sparsity.html#cb70-27"></a><span class="cf">for</span>(RR <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(missclass_oofProb_mtx))</span>
<span id="cb70-28"><a href="explore-sparsity.html#cb70-28"></a><span class="kw">points</span>(</span>
<span id="cb70-29"><a href="explore-sparsity.html#cb70-29"></a> <span class="kw">rep</span>(RR, <span class="kw">ncol</span>(missclass_oofProb_mtx)), </span>
<span id="cb70-30"><a href="explore-sparsity.html#cb70-30"></a> missclass_oofProb_mtx[RR,],</span>
<span id="cb70-31"><a href="explore-sparsity.html#cb70-31"></a> <span class="dt">col=</span><span class="kw">ifelse</span>(test_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)[RR]] <span class="op">==</span><span class="st"> &#39;Control&#39;</span>,</span>
<span id="cb70-32"><a href="explore-sparsity.html#cb70-32"></a>  <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>),</span>
<span id="cb70-33"><a href="explore-sparsity.html#cb70-33"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(missclass_oofProb_mtx))</span>
<span id="cb70-34"><a href="explore-sparsity.html#cb70-34"></a></span>
<span id="cb70-35"><a href="explore-sparsity.html#cb70-35"></a><span class="kw">legend</span>(<span class="st">&#39;top&#39;</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(missclass_oofProb_mtx), </span>
<span id="cb70-36"><a href="explore-sparsity.html#cb70-36"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb70-37"><a href="explore-sparsity.html#cb70-37"></a></span>
<span id="cb70-38"><a href="explore-sparsity.html#cb70-38"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:misclassTest"></span>
<img src="Static/figures/misclassTest-1.png" alt="Test data predicted probabilities at miscassified samples" width="768" />
<p class="caption">
Figure 4.9: Test data predicted probabilities at miscassified samples
</p>
</div>
</div>
<div id="compare-coefficient-profiles" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Compare coefficient profiles</h2>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="explore-sparsity.html#cb71-1"></a><span class="co"># lasso </span></span>
<span id="cb71-2"><a href="explore-sparsity.html#cb71-2"></a><span class="co">##########################</span></span>
<span id="cb71-3"><a href="explore-sparsity.html#cb71-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb71-4"><a href="explore-sparsity.html#cb71-4"></a>lasso_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb71-5"><a href="explore-sparsity.html#cb71-5"></a> cv_lasso,</span>
<span id="cb71-6"><a href="explore-sparsity.html#cb71-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb71-7"><a href="explore-sparsity.html#cb71-7"></a>)</span>
<span id="cb71-8"><a href="explore-sparsity.html#cb71-8"></a>lasso_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb71-9"><a href="explore-sparsity.html#cb71-9"></a> <span class="dt">gene=</span>lasso_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lasso_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb71-10"><a href="explore-sparsity.html#cb71-10"></a> <span class="dt">lasso=</span>lasso_coef<span class="op">@</span>x)</span>
<span id="cb71-11"><a href="explore-sparsity.html#cb71-11"></a></span>
<span id="cb71-12"><a href="explore-sparsity.html#cb71-12"></a></span>
<span id="cb71-13"><a href="explore-sparsity.html#cb71-13"></a><span class="co"># enet</span></span>
<span id="cb71-14"><a href="explore-sparsity.html#cb71-14"></a><span class="co">##########################</span></span>
<span id="cb71-15"><a href="explore-sparsity.html#cb71-15"></a>enet_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb71-16"><a href="explore-sparsity.html#cb71-16"></a> cv_enet,</span>
<span id="cb71-17"><a href="explore-sparsity.html#cb71-17"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb71-18"><a href="explore-sparsity.html#cb71-18"></a>)</span>
<span id="cb71-19"><a href="explore-sparsity.html#cb71-19"></a>enet_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb71-20"><a href="explore-sparsity.html#cb71-20"></a> <span class="dt">gene=</span>enet_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, enet_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb71-21"><a href="explore-sparsity.html#cb71-21"></a> <span class="dt">enet=</span>enet_coef<span class="op">@</span>x)</span>
<span id="cb71-22"><a href="explore-sparsity.html#cb71-22"></a></span>
<span id="cb71-23"><a href="explore-sparsity.html#cb71-23"></a><span class="co"># THESE ARE NOT CORRECT - SKIP</span></span>
<span id="cb71-24"><a href="explore-sparsity.html#cb71-24"></a><span class="co"># relaxed lasso (gamma=0)</span></span>
<span id="cb71-25"><a href="explore-sparsity.html#cb71-25"></a><span class="co">##########################</span></span>
<span id="cb71-26"><a href="explore-sparsity.html#cb71-26"></a>SKIP &lt;-<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb71-27"><a href="explore-sparsity.html#cb71-27"></a>lassoR_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb71-28"><a href="explore-sparsity.html#cb71-28"></a> cv_lassoR,</span>
<span id="cb71-29"><a href="explore-sparsity.html#cb71-29"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb71-30"><a href="explore-sparsity.html#cb71-30"></a> <span class="dt">g=</span><span class="dv">0</span></span>
<span id="cb71-31"><a href="explore-sparsity.html#cb71-31"></a>)</span>
<span id="cb71-32"><a href="explore-sparsity.html#cb71-32"></a>lassoR_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb71-33"><a href="explore-sparsity.html#cb71-33"></a> <span class="dt">gene=</span>lassoR_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lassoR_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb71-34"><a href="explore-sparsity.html#cb71-34"></a> <span class="dt">lassoR=</span>lassoR_coef<span class="op">@</span>x)</span>
<span id="cb71-35"><a href="explore-sparsity.html#cb71-35"></a>}</span>
<span id="cb71-36"><a href="explore-sparsity.html#cb71-36"></a></span>
<span id="cb71-37"><a href="explore-sparsity.html#cb71-37"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb71-38"><a href="explore-sparsity.html#cb71-38"></a><span class="co">###############################</span></span>
<span id="cb71-39"><a href="explore-sparsity.html#cb71-39"></a>blended_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb71-40"><a href="explore-sparsity.html#cb71-40"></a> cv_lassoR,</span>
<span id="cb71-41"><a href="explore-sparsity.html#cb71-41"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb71-42"><a href="explore-sparsity.html#cb71-42"></a> <span class="dt">g=</span><span class="fl">0.5</span></span>
<span id="cb71-43"><a href="explore-sparsity.html#cb71-43"></a>)</span>
<span id="cb71-44"><a href="explore-sparsity.html#cb71-44"></a>blended_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb71-45"><a href="explore-sparsity.html#cb71-45"></a> <span class="dt">gene=</span>blended_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, blended_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb71-46"><a href="explore-sparsity.html#cb71-46"></a> <span class="dt">blended=</span>blended_coef<span class="op">@</span>x)</span>
<span id="cb71-47"><a href="explore-sparsity.html#cb71-47"></a></span>
<span id="cb71-48"><a href="explore-sparsity.html#cb71-48"></a></span>
<span id="cb71-49"><a href="explore-sparsity.html#cb71-49"></a><span class="co"># put it all together</span></span>
<span id="cb71-50"><a href="explore-sparsity.html#cb71-50"></a>all_coef_frm &lt;-<span class="st"> </span></span>
<span id="cb71-51"><a href="explore-sparsity.html#cb71-51"></a><span class="st"> </span>base<span class="op">::</span><span class="kw">merge</span>(</span>
<span id="cb71-52"><a href="explore-sparsity.html#cb71-52"></a> <span class="dt">x =</span> lasso_coef_frm, </span>
<span id="cb71-53"><a href="explore-sparsity.html#cb71-53"></a> <span class="dt">y =</span> base<span class="op">::</span><span class="kw">merge</span>(</span>
<span id="cb71-54"><a href="explore-sparsity.html#cb71-54"></a>     <span class="dt">x =</span> enet_coef_frm,</span>
<span id="cb71-55"><a href="explore-sparsity.html#cb71-55"></a>     <span class="dt">y =</span> blended_coef_frm,</span>
<span id="cb71-56"><a href="explore-sparsity.html#cb71-56"></a>         <span class="dt">by=</span><span class="st">&#39;gene&#39;</span>, <span class="dt">all=</span>T),</span>
<span id="cb71-57"><a href="explore-sparsity.html#cb71-57"></a> <span class="dt">by=</span><span class="st">&#39;gene&#39;</span>, <span class="dt">all=</span>T)</span>
<span id="cb71-58"><a href="explore-sparsity.html#cb71-58"></a></span>
<span id="cb71-59"><a href="explore-sparsity.html#cb71-59"></a><span class="co"># SKIPPED</span></span>
<span id="cb71-60"><a href="explore-sparsity.html#cb71-60"></a><span class="co">#base::merge(</span></span>
<span id="cb71-61"><a href="explore-sparsity.html#cb71-61"></a>         <span class="co">#x = lassoR_coef_frm,</span></span>
<span id="cb71-62"><a href="explore-sparsity.html#cb71-62"></a>         <span class="co">#y = blended_coef_frm,</span></span>
<span id="cb71-63"><a href="explore-sparsity.html#cb71-63"></a>         <span class="co">#by=&#39;gene&#39;, all=T),</span></span>
<span id="cb71-64"><a href="explore-sparsity.html#cb71-64"></a></span>
<span id="cb71-65"><a href="explore-sparsity.html#cb71-65"></a>all_coef_frm[,<span class="op">-</span><span class="dv">1</span>][<span class="kw">is.na</span>(all_coef_frm[,<span class="op">-</span><span class="dv">1</span>])] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb71-66"><a href="explore-sparsity.html#cb71-66"></a></span>
<span id="cb71-67"><a href="explore-sparsity.html#cb71-67"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="kw">ncol</span>(all_coef_frm)<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb71-68"><a href="explore-sparsity.html#cb71-68"></a></span>
<span id="cb71-69"><a href="explore-sparsity.html#cb71-69"></a><span class="cf">for</span>(CC <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(all_coef_frm)) {</span>
<span id="cb71-70"><a href="explore-sparsity.html#cb71-70"></a> <span class="kw">plot</span>(</span>
<span id="cb71-71"><a href="explore-sparsity.html#cb71-71"></a>  <span class="dt">x=</span><span class="dv">1</span><span class="op">:</span>(<span class="kw">nrow</span>(all_coef_frm)<span class="op">-</span><span class="dv">1</span>), <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>, </span>
<span id="cb71-72"><a href="explore-sparsity.html#cb71-72"></a>  <span class="dt">y=</span>all_coef_frm[<span class="op">-</span><span class="dv">1</span>, CC], <span class="dt">ylab=</span><span class="kw">colnames</span>(all_coef_frm)[CC],</span>
<span id="cb71-73"><a href="explore-sparsity.html#cb71-73"></a>  <span class="dt">type=</span><span class="st">&#39;h&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb71-74"><a href="explore-sparsity.html#cb71-74"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:compCoeffProf"></span>
<img src="Static/figures/compCoeffProf-1.png" alt="Coefficient Profiles" width="768" />
<p class="caption">
Figure 4.10: Coefficient Profiles
</p>
</div>
<p>Note that there is little difference between the elastic net and the lasso
in the selected features, and when the coefficient is zero in one set, it
is smaell in the other. By contrast, the blended fit produces more shrinkage.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="explore-sparsity.html#cb72-1"></a>knitr<span class="op">::</span><span class="kw">kable</span>(</span>
<span id="cb72-2"><a href="explore-sparsity.html#cb72-2"></a><span class="kw">with</span>(all_coef_frm[,<span class="op">-</span><span class="dv">1</span>], <span class="kw">table</span>(<span class="dt">lassoZero=</span>lasso<span class="op">==</span><span class="dv">0</span>, <span class="dt">enetZero=</span>enet<span class="op">==</span><span class="dv">0</span>)),</span>
<span id="cb72-3"><a href="explore-sparsity.html#cb72-3"></a> <span class="dt">caption=</span><span class="st">&#39;Zero Ceofficient: rows are lasso, columns enet&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb72-4"><a href="explore-sparsity.html#cb72-4"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:zreros">Table 4.7: </span>Zero Ceofficient: rows are lasso, columns enet
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
FALSE
</th>
<th style="text-align:right;">
TRUE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
88
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<!--
Coefficients in the relaxed lasso fit are much larger than those in the
lasso fit, or zero.  As a consequence, the blended fit coefficients look 
like a shrunken version of the relaxed lasso fit coefficients.  
-->
<p>Coefficients in the blended fit are larger than those in the
lasso fit, or zero.</p>
<p>We can also examine these with a scatter plot matrix.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="explore-sparsity.html#cb73-1"></a><span class="kw">pairs</span>(all_coef_frm[<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb73-2"><a href="explore-sparsity.html#cb73-2"></a>  <span class="dt">lower.panel =</span> <span class="ot">NULL</span>,</span>
<span id="cb73-3"><a href="explore-sparsity.html#cb73-3"></a>  <span class="dt">panel =</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb73-4"><a href="explore-sparsity.html#cb73-4"></a>    <span class="kw">points</span>(x, y, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb73-5"><a href="explore-sparsity.html#cb73-5"></a>  }</span>
<span id="cb73-6"><a href="explore-sparsity.html#cb73-6"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:pairsCoeffProf"></span>
<img src="Static/figures/pairsCoeffProf-1.png" alt="Coefficients from fits" width="768" />
<p class="caption">
Figure 4.11: Coefficients from fits
</p>
</div>
</div>
<div id="examine-feature-selection" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Examine feature selection</h2>
<p>Recall from <code>glmnet</code> vignette:</p>
<pre><code>It is known that the ridge penalty shrinks the coefficients of correlated predictors
towards each other while the lasso tends to pick one of them and discard the others.
The elastic-net penalty mixes these two; if predictors are correlated in groups,
an $\alpha$=0.5 tends to select the groups in or out together.
This is a higher level parameter, and users might pick a value upfront,
else experiment with a few different values. One use of $\alpha$ is for numerical stability;
for example, the *elastic net with $\alpha = 1 - \epsilon$ for some small $\epsilon$&gt;0
performs much like the lasso, but removes any degeneracies and wild behavior caused
by extreme correlations*.</code></pre>
<p>To see how this plays out in this dataset, we can look at feature expression
heat maps.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="explore-sparsity.html#cb75-1"></a> <span class="kw">suppressPackageStartupMessages</span>(<span class="kw">require</span>(gplots))</span>
<span id="cb75-2"><a href="explore-sparsity.html#cb75-2"></a></span>
<span id="cb75-3"><a href="explore-sparsity.html#cb75-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb75-4"><a href="explore-sparsity.html#cb75-4"></a>lasso_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb75-5"><a href="explore-sparsity.html#cb75-5"></a> cv_lasso,</span>
<span id="cb75-6"><a href="explore-sparsity.html#cb75-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb75-7"><a href="explore-sparsity.html#cb75-7"></a>)</span>
<span id="cb75-8"><a href="explore-sparsity.html#cb75-8"></a>lasso_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb75-9"><a href="explore-sparsity.html#cb75-9"></a> <span class="dt">gene=</span>lasso_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lasso_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb75-10"><a href="explore-sparsity.html#cb75-10"></a> <span class="dt">lasso=</span>lasso_coef<span class="op">@</span>x)</span>
<span id="cb75-11"><a href="explore-sparsity.html#cb75-11"></a></span>
<span id="cb75-12"><a href="explore-sparsity.html#cb75-12"></a> </span>
<span id="cb75-13"><a href="explore-sparsity.html#cb75-13"></a>  Mycol &lt;-<span class="st"> </span><span class="kw">colorpanel</span>(<span class="dv">1000</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb75-14"><a href="explore-sparsity.html#cb75-14"></a>  <span class="kw">heatmap.2</span>(</span>
<span id="cb75-15"><a href="explore-sparsity.html#cb75-15"></a>    <span class="dt">x=</span><span class="kw">t</span>(train_lcpm_mtx[,lasso_coef_frm<span class="op">$</span>gene[<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb75-16"><a href="explore-sparsity.html#cb75-16"></a>    <span class="dt">scale=</span><span class="st">&quot;row&quot;</span>,</span>
<span id="cb75-17"><a href="explore-sparsity.html#cb75-17"></a>    <span class="dt">labRow=</span>lasso_coef_frm<span class="op">$</span>gene,</span>
<span id="cb75-18"><a href="explore-sparsity.html#cb75-18"></a>    <span class="dt">labCol=</span>train_group_vec,</span>
<span id="cb75-19"><a href="explore-sparsity.html#cb75-19"></a>    <span class="dt">col=</span>Mycol, </span>
<span id="cb75-20"><a href="explore-sparsity.html#cb75-20"></a>    <span class="dt">trace=</span><span class="st">&quot;none&quot;</span>, <span class="dt">density.info=</span><span class="st">&quot;none&quot;</span>, </span>
<span id="cb75-21"><a href="explore-sparsity.html#cb75-21"></a>    <span class="co">#margin=c(8,6), lhei=c(2,10), </span></span>
<span id="cb75-22"><a href="explore-sparsity.html#cb75-22"></a>    <span class="co">#lwid=c(0.1,4), #lhei=c(0.1,4)</span></span>
<span id="cb75-23"><a href="explore-sparsity.html#cb75-23"></a>    <span class="dt">key=</span>F,</span>
<span id="cb75-24"><a href="explore-sparsity.html#cb75-24"></a>    <span class="dt">ColSideColors=</span><span class="kw">ifelse</span>(train_group_vec<span class="op">==</span><span class="st">&#39;Control&#39;</span>, <span class="st">&#39;green&#39;</span>,<span class="st">&#39;red&#39;</span>),</span>
<span id="cb75-25"><a href="explore-sparsity.html#cb75-25"></a>    <span class="dt">dendrogram=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb75-26"><a href="explore-sparsity.html#cb75-26"></a>    <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;lasso genes - N =&#39;</span>, <span class="kw">nrow</span>(lasso_coef_frm)<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:heatmapLasso"></span>
<img src="Static/figures/heatmapLasso-1.png" alt="Lasso Model Genes" width="768" />
<p class="caption">
Figure 4.12: Lasso Model Genes
</p>
</div>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="explore-sparsity.html#cb76-1"></a> <span class="kw">suppressPackageStartupMessages</span>(<span class="kw">require</span>(gplots))</span>
<span id="cb76-2"><a href="explore-sparsity.html#cb76-2"></a></span>
<span id="cb76-3"><a href="explore-sparsity.html#cb76-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb76-4"><a href="explore-sparsity.html#cb76-4"></a>enet_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb76-5"><a href="explore-sparsity.html#cb76-5"></a> cv_enet,</span>
<span id="cb76-6"><a href="explore-sparsity.html#cb76-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb76-7"><a href="explore-sparsity.html#cb76-7"></a>)</span>
<span id="cb76-8"><a href="explore-sparsity.html#cb76-8"></a>enet_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb76-9"><a href="explore-sparsity.html#cb76-9"></a> <span class="dt">gene=</span>enet_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, enet_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb76-10"><a href="explore-sparsity.html#cb76-10"></a> <span class="dt">enet=</span>enet_coef<span class="op">@</span>x)</span>
<span id="cb76-11"><a href="explore-sparsity.html#cb76-11"></a></span>
<span id="cb76-12"><a href="explore-sparsity.html#cb76-12"></a> </span>
<span id="cb76-13"><a href="explore-sparsity.html#cb76-13"></a>  Mycol &lt;-<span class="st"> </span><span class="kw">colorpanel</span>(<span class="dv">1000</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb76-14"><a href="explore-sparsity.html#cb76-14"></a>  <span class="kw">heatmap.2</span>(</span>
<span id="cb76-15"><a href="explore-sparsity.html#cb76-15"></a>    <span class="dt">x=</span><span class="kw">t</span>(train_lcpm_mtx[,enet_coef_frm<span class="op">$</span>gene[<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb76-16"><a href="explore-sparsity.html#cb76-16"></a>    <span class="dt">scale=</span><span class="st">&quot;row&quot;</span>,</span>
<span id="cb76-17"><a href="explore-sparsity.html#cb76-17"></a>    <span class="dt">labRow=</span>enet_coef_frm<span class="op">$</span>gene,</span>
<span id="cb76-18"><a href="explore-sparsity.html#cb76-18"></a>    <span class="dt">labCol=</span>train_group_vec,</span>
<span id="cb76-19"><a href="explore-sparsity.html#cb76-19"></a>    <span class="dt">col=</span>Mycol, </span>
<span id="cb76-20"><a href="explore-sparsity.html#cb76-20"></a>    <span class="dt">trace=</span><span class="st">&quot;none&quot;</span>, <span class="dt">density.info=</span><span class="st">&quot;none&quot;</span>, </span>
<span id="cb76-21"><a href="explore-sparsity.html#cb76-21"></a>    <span class="co">#margin=c(8,6), lhei=c(2,10), </span></span>
<span id="cb76-22"><a href="explore-sparsity.html#cb76-22"></a>    <span class="co">#lwid=c(0.1,4), #lhei=c(0.1,4)</span></span>
<span id="cb76-23"><a href="explore-sparsity.html#cb76-23"></a>    <span class="dt">key=</span>F,</span>
<span id="cb76-24"><a href="explore-sparsity.html#cb76-24"></a>    <span class="dt">ColSideColors=</span><span class="kw">ifelse</span>(train_group_vec<span class="op">==</span><span class="st">&#39;Control&#39;</span>, <span class="st">&#39;green&#39;</span>,<span class="st">&#39;red&#39;</span>),</span>
<span id="cb76-25"><a href="explore-sparsity.html#cb76-25"></a>    <span class="dt">dendrogram=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb76-26"><a href="explore-sparsity.html#cb76-26"></a>    <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;enet genes - N =&#39;</span>, <span class="kw">nrow</span>(enet_coef_frm)<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:heatmapEnet"></span>
<img src="Static/figures/heatmapEnet-1.png" alt="Enet Model Genes" width="768" />
<p class="caption">
Figure 4.13: Enet Model Genes
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preproc.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-suite.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["HCC5hmCExplore.pdf", "HCC5hmCExplore.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
