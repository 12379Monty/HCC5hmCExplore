<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma</title>
  <meta name="description" content="Data from Cai et al. (2019) paper are explored" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Data from Cai et al. (2019) paper are explored" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 4 The bet on sparsity | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  
  <meta name="twitter:description" content="Data from Cai et al. (2019) paper are explored" />
  

<meta name="author" content="Francois Collin" />


<meta name="date" content="2020-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modeling-background.html"/>
<link rel="next" href="model-suite.html"/>
<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script type="text/javascript">
// source:https://stackoverflow.com/questions/45360998/code-folding-in-bookdown
// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preamble</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="preproc.html"><a href="preproc.html"><i class="fa fa-check"></i><b>2</b> Preprocessing</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preproc.html"><a href="preproc.html#load-the-data"><i class="fa fa-check"></i><b>2.1</b> Load the data</a></li>
<li class="chapter" data-level="2.2" data-path="preproc.html"><a href="preproc.html#dra"><i class="fa fa-check"></i><b>2.2</b> Differential representation analysis</a>
<ul>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#remove-lowly-expressed-genes"><i class="fa fa-check"></i>Remove lowly expressed genes</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#creating-a-design-matrix-and-contrasts"><i class="fa fa-check"></i>Creating a design matrix and contrasts</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#removing-heteroscedasticity-from-the-count-data"><i class="fa fa-check"></i>Removing heteroscedasticity from the count data</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#fit-linear-models-and-examine-the-results"><i class="fa fa-check"></i>Fit linear models and examine the results</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#graphical-representations-of-de-results-md-plots"><i class="fa fa-check"></i>Graphical representations of DE results: MD Plots</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#de-genes-at-10-fold-change"><i class="fa fa-check"></i>DE genes at 10% fold change</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="preproc.html"><a href="preproc.html#snr-regime"><i class="fa fa-check"></i><b>2.3</b> Signal-to-noise ratio regime</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modeling-background.html"><a href="modeling-background.html"><i class="fa fa-check"></i><b>3</b> Modeling - Background</a>
<ul>
<li class="chapter" data-level="3.1" data-path="modeling-background.html"><a href="modeling-background.html#predictive-modeling-for-genomic-data"><i class="fa fa-check"></i><b>3.1</b> Predictive modeling for genomic data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="modeling-background.html"><a href="modeling-background.html#caret-for-model-evaluation"><i class="fa fa-check"></i><b>3.1.1</b> caret for model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="modeling-background.html"><a href="modeling-background.html#glmnet"><i class="fa fa-check"></i><b>3.2</b> glmnet</a>
<ul>
<li><a href="modeling-background.html#alpha-hyper-parameter"><strong>alpha</strong> hyper-parameter</a></li>
<li class="chapter" data-level="" data-path="modeling-background.html"><a href="modeling-background.html#lasso-vs-best-subset"><i class="fa fa-check"></i>Lasso vs Best Subset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="explore-sparsity.html"><a href="explore-sparsity.html"><i class="fa fa-check"></i><b>4</b> The bet on sparsity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#cv-analysis-setup"><i class="fa fa-check"></i><b>4.1</b> CV analysis setup</a></li>
<li class="chapter" data-level="4.2" data-path="explore-sparsity.html"><a href="explore-sparsity.html#fit-and-compare-models"><i class="fa fa-check"></i><b>4.2</b> Fit and compare models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#logistic-regression-in-glmnet"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression in <code>glmnet</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="explore-sparsity.html"><a href="explore-sparsity.html#relaxed-lasso-and-blended-mix"><i class="fa fa-check"></i><b>4.3</b> Relaxed lasso and blended mix</a></li>
<li class="chapter" data-level="4.4" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examination-of-sensitivity-vs-specificity"><i class="fa fa-check"></i><b>4.4</b> Examination of sensitivity vs specificity</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#training-data-out-of-fold-roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> Training data out-of-fold ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-predictions-at-misclassified-samples"><i class="fa fa-check"></i><b>4.5</b> Compare predictions at misclassified samples</a></li>
<li class="chapter" data-level="4.6" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-coefficient-profiles"><i class="fa fa-check"></i><b>4.6</b> Compare coefficient profiles</a></li>
<li class="chapter" data-level="4.7" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examine-feature-selection"><i class="fa fa-check"></i><b>4.7</b> Examine feature selection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-suite.html"><a href="model-suite.html"><i class="fa fa-check"></i><b>5</b> Fitted Model Suite</a>
<ul>
<li class="chapter" data-level="5.1" data-path="model-suite.html"><a href="model-suite.html#sample-quality-scores"><i class="fa fa-check"></i><b>5.1</b> Sample quality scores</a></li>
<li class="chapter" data-level="5.2" data-path="model-suite.html"><a href="model-suite.html#model-suite-1"><i class="fa fa-check"></i><b>5.2</b> Model suite</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="model-suite.html"><a href="model-suite.html#simulation-setup"><i class="fa fa-check"></i><b>5.2.1</b> Simulation setup</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>6</b> Variable importance</a></li>
<li class="chapter" data-level="7" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>7</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="LICENSE.md" style="font:em;" target="blank">Copyright (c) 2020 Francois Collin</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="explore-sparsity" class="section level1" number="4">
<h1><span class="header-section-number">Section 4</span> The bet on sparsity</h1>
<p>In this section we explore various fits that can be computed
and analyzed with tools provided in the <code>glmnet</code> package.
Refer to the <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">Glmnet Vignette</a>
for a quick reference guide.</p>
<div id="cv-analysis-setup" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> CV analysis setup</h2>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="explore-sparsity.html#cb34-1"></a>K_FOLD &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb34-2"><a href="explore-sparsity.html#cb34-2"></a>trainP &lt;-<span class="st"> </span><span class="fl">0.8</span></span>
<span id="cb34-3"><a href="explore-sparsity.html#cb34-3"></a>EPS &lt;-<span class="st"> </span><span class="fl">0.05</span>    <span class="co"># Have no idea what &quot;small&quot; epsilon means</span></span></code></pre></div>
<p>First we divide the analysis dataset into <code>train</code> and <code>test</code> in a 4:1 ratio.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="explore-sparsity.html#cb35-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb35-2"><a href="explore-sparsity.html#cb35-2"></a>train_sampID_vec &lt;-<span class="st"> </span><span class="kw">with</span>(AF_dgel<span class="op">$</span>samples,</span>
<span id="cb35-3"><a href="explore-sparsity.html#cb35-3"></a>AF_dgel<span class="op">$</span>samples<span class="op">$</span>sampID[caret<span class="op">::</span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>group, <span class="dt">p=</span>trainP, <span class="dt">list=</span>F)]</span>
<span id="cb35-4"><a href="explore-sparsity.html#cb35-4"></a>)</span>
<span id="cb35-5"><a href="explore-sparsity.html#cb35-5"></a></span>
<span id="cb35-6"><a href="explore-sparsity.html#cb35-6"></a>test_sampID_vec &lt;-<span class="st"> </span><span class="kw">with</span>(AF_dgel<span class="op">$</span>samples,</span>
<span id="cb35-7"><a href="explore-sparsity.html#cb35-7"></a><span class="kw">setdiff</span>(sampID, train_sampID_vec)</span>
<span id="cb35-8"><a href="explore-sparsity.html#cb35-8"></a>)</span>
<span id="cb35-9"><a href="explore-sparsity.html#cb35-9"></a></span>
<span id="cb35-10"><a href="explore-sparsity.html#cb35-10"></a>train_group_vec &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[train_sampID_vec, <span class="st">&#39;group&#39;</span>]</span>
<span id="cb35-11"><a href="explore-sparsity.html#cb35-11"></a><span class="kw">names</span>(train_group_vec) &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[train_sampID_vec, <span class="st">&#39;sampID&#39;</span>]</span>
<span id="cb35-12"><a href="explore-sparsity.html#cb35-12"></a></span>
<span id="cb35-13"><a href="explore-sparsity.html#cb35-13"></a>test_group_vec &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[test_sampID_vec, <span class="st">&#39;group&#39;</span>]</span>
<span id="cb35-14"><a href="explore-sparsity.html#cb35-14"></a><span class="kw">names</span>(test_group_vec) &lt;-<span class="st"> </span>AF_dgel<span class="op">$</span>samples[test_sampID_vec, <span class="st">&#39;sampID&#39;</span>]</span>
<span id="cb35-15"><a href="explore-sparsity.html#cb35-15"></a></span>
<span id="cb35-16"><a href="explore-sparsity.html#cb35-16"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">table</span>(train_group_vec),</span>
<span id="cb35-17"><a href="explore-sparsity.html#cb35-17"></a>  <span class="dt">caption=</span><span class="st">&quot;Train set&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-18"><a href="explore-sparsity.html#cb35-18"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainVal">Table 4.1: </span>Train set
</caption>
<thead>
<tr>
<th style="text-align:left;">
train_group_vec
</th>
<th style="text-align:right;">
Freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
623
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
444
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="explore-sparsity.html#cb36-1"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">table</span>(test_group_vec),</span>
<span id="cb36-2"><a href="explore-sparsity.html#cb36-2"></a>  <span class="dt">caption=</span><span class="st">&quot;Test set&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb36-3"><a href="explore-sparsity.html#cb36-3"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainVal">Table 4.1: </span>Test set
</caption>
<thead>
<tr>
<th style="text-align:left;">
test_group_vec
</th>
<th style="text-align:right;">
Freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
155
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
111
</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="explore-sparsity.html#cb37-1"></a>train_lcpm_mtx &lt;-<span class="st"> </span><span class="kw">t</span>(lcpm_mtx[,train_sampID_vec])</span>
<span id="cb37-2"><a href="explore-sparsity.html#cb37-2"></a>test_lcpm_mtx &lt;-<span class="st"> </span><span class="kw">t</span>(lcpm_mtx[,test_sampID_vec])</span></code></pre></div>
<p>We explore some glmnet fits and the “bet on sparsity”</p>
<ul>
<li>Consider models:
<ul>
<li>lasso: <span class="math inline">\(\alpha = 1.0\)</span> - sparse model<br />
</li>
<li>ridge <span class="math inline">\(\alpha = 0\)</span> - shrunken coefficients model</li>
<li>elastic net: <span class="math inline">\(\alpha = 0.5\)</span> - semi sparse model
<!-- - lassoC: $\alpha = 1-\epsilon =$ 0.95 - lasso for correlated predictors  --></li>
</ul></li>
<li>Does the relaxed lasso improve performance?<br />
</li>
<li>Does the shrunken relaxed lasso (aka the blended mix) improve performance<br />
</li>
<li>How sparse is the model undelying best 5hmC classifier for Early HCC vs Control?<br />
</li>
<li>Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set?</li>
</ul>
<p>In this analysis, we will only evaluate models in terms of
model size, stability and performance. We leave the question
of significance testing of hypotheses about model parameters
completely out. See Lockhart et al. (2014) <span class="citation">[<a href="references.html#ref-Lockhart:2014aa" role="doc-biblioref">24</a>]</span>
and Wassermam (2014) <span class="citation">[<a href="references.html#ref-Wasserman:2014aa" role="doc-biblioref">25</a>]</span> for a discussion of this topic.</p>
<p>Next we create folds for 10-fold cross-validation of models fitted to
training data. We’ll use caret::createFolds to assign samples
to folds while keeping the outcome ratios constant across folds.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="explore-sparsity.html#cb38-1"></a><span class="co"># This is too variable, both in terms of fold size And composition</span></span>
<span id="cb38-2"><a href="explore-sparsity.html#cb38-2"></a><span class="co">#foldid_vec &lt;- sample(1:10, size=length(train_group_vec), replace=T)</span></span>
<span id="cb38-3"><a href="explore-sparsity.html#cb38-3"></a></span>
<span id="cb38-4"><a href="explore-sparsity.html#cb38-4"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb38-5"><a href="explore-sparsity.html#cb38-5"></a>train_foldid_vec &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createFolds</span>(</span>
<span id="cb38-6"><a href="explore-sparsity.html#cb38-6"></a> <span class="kw">factor</span>(train_group_vec), </span>
<span id="cb38-7"><a href="explore-sparsity.html#cb38-7"></a> <span class="dt">k=</span>K_FOLD,</span>
<span id="cb38-8"><a href="explore-sparsity.html#cb38-8"></a> <span class="dt">list=</span>F)</span>
<span id="cb38-9"><a href="explore-sparsity.html#cb38-9"></a></span>
<span id="cb38-10"><a href="explore-sparsity.html#cb38-10"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">sapply</span>(<span class="kw">split</span>(train_group_vec, train_foldid_vec), </span>
<span id="cb38-11"><a href="explore-sparsity.html#cb38-11"></a>  table), <span class="dt">caption=</span><span class="st">&quot;training samples fold composition&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb38-12"><a href="explore-sparsity.html#cb38-12"></a><span class="st">   </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:getTrainFolds">Table 4.2: </span>training samples fold composition
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
<th style="text-align:right;">
7
</th>
<th style="text-align:right;">
8
</th>
<th style="text-align:right;">
9
</th>
<th style="text-align:right;">
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
63
</td>
<td style="text-align:right;">
62
</td>
</tr>
<tr>
<td style="text-align:left;">
HCC
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
44
</td>
</tr>
</tbody>
</table>
<p>Note that the folds identify samples that are left-out of the training
data for each fold fit.</p>
</div>
<div id="fit-and-compare-models" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Fit and compare models</h2>
<ul>
<li>cross-validated accuracy</li>
<li>test set accuracy<br />
</li>
<li>sparsity
<ul>
<li>for lasso, enet <!--and lassoC-->, examine number of selected variables</li>
</ul></li>
</ul>
<p>Although “the one standard error rule” can produce a model with fewer predictors, it usually results in increased MSE and more biased parameter estimates
(see Engebretsen et al. (2019) <span class="citation">[<a href="references.html#ref-Engebretsen:2019aa" role="doc-biblioref">26</a>]</span> for example).
We will look at both the minimum cv error and the one standard error rule model
preformance.</p>
<div id="logistic-regression-in-glmnet" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Logistic regression in <code>glmnet</code></h3>
<p><code>glmnet</code> provides functionality to extract various predicted of fitted values
from calibrated models. Note in passing that some folks make a distinction between
<strong>fitted</strong> or <strong>estimated</strong> values for sample points in the training data
versus <strong>predicted</strong> values for sample points that
are not in the training dataset. <code>glmnet</code> makes no such distinction and the
<code>predict</code> function is used to produce both fitted as well as predicted values.
For logistic regressions, which is the model fitted in a regularized fashion
when models are fitted by glmnet with the parameter <code>family='binomial'</code>, three
fitted or predicted values can be extracted at a given design point.<br />
Suppose our response variable Y is either 0 or 1 (Control or HCC in our case).
These are specified by the <code>type</code> parameter. <code>type='resp'</code> returns
the fitted or predicted probability of <span class="math inline">\(Y=1\)</span>. <code>type='class'</code> returns the fitted or
predicted class for the design point, which is simply dichotomizing the
response: class = 1 if the fitted or predicted probability is greater than 0.5
(check to make sure class is no the Bayes estimate). <code>type='link'</code> returns
the fitted or predicted value of the linear predictor <span class="math inline">\(\beta&#39;x\)</span>. The relationship
between the linear predictor and the response can be derided from the
logistic regression model:</p>
<p><span class="math display">\[P(Y=1|x,\beta) = g^{-1}(\beta&#39;x) = h(\beta&#39;x) = \frac{e^{\beta&#39;x}}{1+e^{\beta&#39;x}}\]</span></p>
<p>where <span class="math inline">\(g\)</span> is the link function, <span class="math inline">\(g^{-1}\)</span> the mean function.
The link function is given by:</p>
<p><span class="math display">\[g(y) = h^{-1}(y) = ln(\frac{y}{1-y})\]</span></p>
<p>This link function is called the logit function, and its inverse the logistic
function.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="explore-sparsity.html#cb39-1"></a>logistic_f &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))</span></code></pre></div>
<p>It is important to note that all <em>predicted</em> values extracted from
<code>glmnet</code> fitted models by the <strong>predict()</strong> extraction method
yield <strong>fitted</strong> values for design points that are part of the
training data set. This includes the predicted class for training data
which ae used to estimate misclassification error rates. As a result, the cv error
rates quoted in various <code>glmnet</code> summaries are generally optimistic.
<code>glmnet</code> fitting functions have a
parameter, <em>keep</em>, which instructs the fitting function to keep the
<code>out-of-fold</code>, or <code>prevalidated</code>, predictions as part of the returned object. The
<code>out-of-fold</code> predictions are predicted values for the samples in the
left-out folds, pooled across all cv folds. For each hyper-parameter
specification, we get one full set of <code>out-of-fold</code> predictions for
the training set samples. Performance assessments based on these
values are usually more generalizable - ie. predictive of
performance in unseen data - than assessments based on values
produced from the full fit, which by default is what <code>glmnet</code> extraction
methods provide. See Höfling and Tibshirani (2008) <span class="citation">[<a href="references.html#ref-Hofling:2008aa" role="doc-biblioref">27</a>]</span>
for a description of the use of pre-validation in model assessment.</p>
<p>Because the <code>keep=T</code> option will store predicted values for
all models evaluated in the cross-validation process, we will
limit the number of models tested by setting <strong>nlambda=30</strong>
when calling the fitting functions. This has no effect on
performance in this data set.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="explore-sparsity.html#cb40-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb40-2"><a href="explore-sparsity.html#cb40-2"></a></span>
<span id="cb40-3"><a href="explore-sparsity.html#cb40-3"></a>cv_lasso &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb40-4"><a href="explore-sparsity.html#cb40-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb40-5"><a href="explore-sparsity.html#cb40-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb40-6"><a href="explore-sparsity.html#cb40-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb40-7"><a href="explore-sparsity.html#cb40-7"></a> <span class="dt">alpha=</span><span class="dv">1</span>,</span>
<span id="cb40-8"><a href="explore-sparsity.html#cb40-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>, </span>
<span id="cb40-9"><a href="explore-sparsity.html#cb40-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb40-10"><a href="explore-sparsity.html#cb40-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb40-11"><a href="explore-sparsity.html#cb40-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb40-12"><a href="explore-sparsity.html#cb40-12"></a>)</span>
<span id="cb40-13"><a href="explore-sparsity.html#cb40-13"></a></span>
<span id="cb40-14"><a href="explore-sparsity.html#cb40-14"></a><span class="kw">message</span>(<span class="st">&quot;lasso time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## lasso time: 13.22s</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="explore-sparsity.html#cb42-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb42-2"><a href="explore-sparsity.html#cb42-2"></a></span>
<span id="cb42-3"><a href="explore-sparsity.html#cb42-3"></a>cv_ridge &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb42-4"><a href="explore-sparsity.html#cb42-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb42-5"><a href="explore-sparsity.html#cb42-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb42-6"><a href="explore-sparsity.html#cb42-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb42-7"><a href="explore-sparsity.html#cb42-7"></a> <span class="dt">alpha=</span><span class="dv">0</span>,</span>
<span id="cb42-8"><a href="explore-sparsity.html#cb42-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>, </span>
<span id="cb42-9"><a href="explore-sparsity.html#cb42-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb42-10"><a href="explore-sparsity.html#cb42-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb42-11"><a href="explore-sparsity.html#cb42-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb42-12"><a href="explore-sparsity.html#cb42-12"></a>)</span>
<span id="cb42-13"><a href="explore-sparsity.html#cb42-13"></a></span>
<span id="cb42-14"><a href="explore-sparsity.html#cb42-14"></a><span class="kw">message</span>(<span class="st">&quot;ridge time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## ridge time: 103.96s</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="explore-sparsity.html#cb44-1"></a>start_time &lt;-<span class="st">  </span><span class="kw">proc.time</span>()</span>
<span id="cb44-2"><a href="explore-sparsity.html#cb44-2"></a></span>
<span id="cb44-3"><a href="explore-sparsity.html#cb44-3"></a>cv_enet &lt;-<span class="st"> </span>glmnet<span class="op">::</span><span class="kw">cv.glmnet</span>(</span>
<span id="cb44-4"><a href="explore-sparsity.html#cb44-4"></a> <span class="dt">x=</span>train_lcpm_mtx,</span>
<span id="cb44-5"><a href="explore-sparsity.html#cb44-5"></a> <span class="dt">y=</span>train_group_vec,</span>
<span id="cb44-6"><a href="explore-sparsity.html#cb44-6"></a> <span class="dt">foldid=</span>train_foldid_vec,</span>
<span id="cb44-7"><a href="explore-sparsity.html#cb44-7"></a> <span class="dt">alpha=</span><span class="fl">0.5</span>,</span>
<span id="cb44-8"><a href="explore-sparsity.html#cb44-8"></a> <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>,</span>
<span id="cb44-9"><a href="explore-sparsity.html#cb44-9"></a> <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb44-10"><a href="explore-sparsity.html#cb44-10"></a> <span class="dt">keep=</span>T,</span>
<span id="cb44-11"><a href="explore-sparsity.html#cb44-11"></a> <span class="dt">nlambda=</span><span class="dv">30</span></span>
<span id="cb44-12"><a href="explore-sparsity.html#cb44-12"></a>)</span>
<span id="cb44-13"><a href="explore-sparsity.html#cb44-13"></a></span>
<span id="cb44-14"><a href="explore-sparsity.html#cb44-14"></a><span class="kw">message</span>(<span class="st">&quot;enet time: &quot;</span>, <span class="kw">round</span>((<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>start_time)[<span class="dv">3</span>],<span class="dv">2</span>),<span class="st">&quot;s&quot;</span>)</span></code></pre></div>
<pre><code>## enet time: 12.18s</code></pre>
<p>The ridge regression model takes over 10 times longer to compute.</p>
<!-- do not show
Define plotting function.
Maybe show in appendix??
-->
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="explore-sparsity.html#cb46-1"></a>plot_cv_f &lt;-<span class="st"> </span><span class="cf">function</span>(cv_fit, <span class="dt">Nzero=</span>T, ...) {</span>
<span id="cb46-2"><a href="explore-sparsity.html#cb46-2"></a> </span>
<span id="cb46-3"><a href="explore-sparsity.html#cb46-3"></a> <span class="kw">library</span>(glmnet)</span>
<span id="cb46-4"><a href="explore-sparsity.html#cb46-4"></a></span>
<span id="cb46-5"><a href="explore-sparsity.html#cb46-5"></a> <span class="co"># No nonger used</span></span>
<span id="cb46-6"><a href="explore-sparsity.html#cb46-6"></a> <span class="co">#lambda.1se_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]</span></span>
<span id="cb46-7"><a href="explore-sparsity.html#cb46-7"></a> <span class="co">#lambda.min_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]</span></span>
<span id="cb46-8"><a href="explore-sparsity.html#cb46-8"></a> </span>
<span id="cb46-9"><a href="explore-sparsity.html#cb46-9"></a> <span class="co"># Get oof error</span></span>
<span id="cb46-10"><a href="explore-sparsity.html#cb46-10"></a> ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_fit<span class="op">$</span>lambda)</span>
<span id="cb46-11"><a href="explore-sparsity.html#cb46-11"></a> train_oofPred_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb46-12"><a href="explore-sparsity.html#cb46-12"></a>  cv_fit<span class="op">$</span>fit.preval[,ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb46-13"><a href="explore-sparsity.html#cb46-13"></a> train_oofPred_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb46-14"><a href="explore-sparsity.html#cb46-14"></a></span>
<span id="cb46-15"><a href="explore-sparsity.html#cb46-15"></a> ndx_min &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda.min,cv_fit<span class="op">$</span>lambda)</span>
<span id="cb46-16"><a href="explore-sparsity.html#cb46-16"></a> train_oofPred_min_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb46-17"><a href="explore-sparsity.html#cb46-17"></a>  cv_fit<span class="op">$</span>fit.preval[,ndx_min] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb46-18"><a href="explore-sparsity.html#cb46-18"></a> train_oofPred_min_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_min_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb46-19"><a href="explore-sparsity.html#cb46-19"></a></span>
<span id="cb46-20"><a href="explore-sparsity.html#cb46-20"></a> <span class="co"># Get test set error</span></span>
<span id="cb46-21"><a href="explore-sparsity.html#cb46-21"></a> test_pred_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb46-22"><a href="explore-sparsity.html#cb46-22"></a>  cv_fit, </span>
<span id="cb46-23"><a href="explore-sparsity.html#cb46-23"></a>  <span class="dt">newx=</span>test_lcpm_mtx, </span>
<span id="cb46-24"><a href="explore-sparsity.html#cb46-24"></a>  <span class="dt">s=</span><span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb46-25"><a href="explore-sparsity.html#cb46-25"></a>  <span class="dt">type=</span><span class="st">&quot;class&quot;</span></span>
<span id="cb46-26"><a href="explore-sparsity.html#cb46-26"></a> )</span>
<span id="cb46-27"><a href="explore-sparsity.html#cb46-27"></a> test_pred_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb46-28"><a href="explore-sparsity.html#cb46-28"></a> </span>
<span id="cb46-29"><a href="explore-sparsity.html#cb46-29"></a> test_pred_min_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb46-30"><a href="explore-sparsity.html#cb46-30"></a>  cv_fit, </span>
<span id="cb46-31"><a href="explore-sparsity.html#cb46-31"></a>  <span class="dt">newx=</span>test_lcpm_mtx, </span>
<span id="cb46-32"><a href="explore-sparsity.html#cb46-32"></a>  <span class="dt">s=</span><span class="st">&quot;lambda.min&quot;</span>,</span>
<span id="cb46-33"><a href="explore-sparsity.html#cb46-33"></a>  <span class="dt">type=</span><span class="st">&quot;class&quot;</span></span>
<span id="cb46-34"><a href="explore-sparsity.html#cb46-34"></a> )</span>
<span id="cb46-35"><a href="explore-sparsity.html#cb46-35"></a> test_pred_min_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_min_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb46-36"><a href="explore-sparsity.html#cb46-36"></a> </span>
<span id="cb46-37"><a href="explore-sparsity.html#cb46-37"></a>  </span>
<span id="cb46-38"><a href="explore-sparsity.html#cb46-38"></a> <span class="kw">plot</span>(</span>
<span id="cb46-39"><a href="explore-sparsity.html#cb46-39"></a>  <span class="kw">log</span>(cv_fit<span class="op">$</span>lambda),</span>
<span id="cb46-40"><a href="explore-sparsity.html#cb46-40"></a>  cv_fit<span class="op">$</span>cvm,</span>
<span id="cb46-41"><a href="explore-sparsity.html#cb46-41"></a>  <span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,</span>
<span id="cb46-42"><a href="explore-sparsity.html#cb46-42"></a>  <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>,<span class="dt">ylab=</span><span class="st">&#39;&#39;</span>,</span>
<span id="cb46-43"><a href="explore-sparsity.html#cb46-43"></a>  ...</span>
<span id="cb46-44"><a href="explore-sparsity.html#cb46-44"></a> )</span>
<span id="cb46-45"><a href="explore-sparsity.html#cb46-45"></a> <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">log</span>(<span class="kw">c</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_fit<span class="op">$</span>lambda.min)))</span>
<span id="cb46-46"><a href="explore-sparsity.html#cb46-46"></a> <span class="cf">if</span>(Nzero)</span>
<span id="cb46-47"><a href="explore-sparsity.html#cb46-47"></a> <span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">3</span>, <span class="dt">tick=</span>F, <span class="dt">at=</span><span class="kw">log</span>(cv_fit<span class="op">$</span>lambda), </span>
<span id="cb46-48"><a href="explore-sparsity.html#cb46-48"></a>  <span class="dt">labels=</span>cv_fit<span class="op">$</span>nzero, <span class="dt">line =</span> <span class="dv">-1</span></span>
<span id="cb46-49"><a href="explore-sparsity.html#cb46-49"></a> )</span>
<span id="cb46-50"><a href="explore-sparsity.html#cb46-50"></a> LL &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb46-51"><a href="explore-sparsity.html#cb46-51"></a> <span class="co">#mtext(side=1, outer=F, line = LL, &quot;log(Lambda)&quot;)</span></span>
<span id="cb46-52"><a href="explore-sparsity.html#cb46-52"></a> <span class="co">#LL &lt;- LL+1</span></span>
<span id="cb46-53"><a href="explore-sparsity.html#cb46-53"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>F, <span class="dt">line =</span> LL, <span class="kw">paste</span>(</span>
<span id="cb46-54"><a href="explore-sparsity.html#cb46-54"></a>  <span class="co">#ifelse(Nzero, paste(&quot;1se p =&quot;, lambda.1se_p),&#39;&#39;),</span></span>
<span id="cb46-55"><a href="explore-sparsity.html#cb46-55"></a>  <span class="st">&quot;1se: cv =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se], <span class="dv">1</span>),</span>
<span id="cb46-56"><a href="explore-sparsity.html#cb46-56"></a>  <span class="st">&quot;oof =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>train_oofPred_1se_error, <span class="dv">1</span>),</span>
<span id="cb46-57"><a href="explore-sparsity.html#cb46-57"></a>  <span class="st">&quot;test =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>test_pred_1se_error, <span class="dv">1</span>)</span>
<span id="cb46-58"><a href="explore-sparsity.html#cb46-58"></a> ))</span>
<span id="cb46-59"><a href="explore-sparsity.html#cb46-59"></a> LL &lt;-<span class="st"> </span>LL<span class="op">+</span><span class="dv">1</span></span>
<span id="cb46-60"><a href="explore-sparsity.html#cb46-60"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>F, <span class="dt">line =</span> LL, <span class="kw">paste</span>(</span>
<span id="cb46-61"><a href="explore-sparsity.html#cb46-61"></a>  <span class="co">#ifelse(Nzero, paste(&quot;min p =&quot;, lambda.min_p),&#39;&#39;),</span></span>
<span id="cb46-62"><a href="explore-sparsity.html#cb46-62"></a>  <span class="st">&quot;min: cv =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda.min], <span class="dv">1</span>),</span>
<span id="cb46-63"><a href="explore-sparsity.html#cb46-63"></a>  <span class="st">&quot;oof =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>train_oofPred_min_error, <span class="dv">1</span>),</span>
<span id="cb46-64"><a href="explore-sparsity.html#cb46-64"></a>  <span class="st">&quot;test =&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span>test_pred_min_error, <span class="dv">1</span>)</span>
<span id="cb46-65"><a href="explore-sparsity.html#cb46-65"></a> ))</span>
<span id="cb46-66"><a href="explore-sparsity.html#cb46-66"></a></span>
<span id="cb46-67"><a href="explore-sparsity.html#cb46-67"></a> <span class="kw">cbind</span>(</span>
<span id="cb46-68"><a href="explore-sparsity.html#cb46-68"></a>  <span class="dt">error_1se =</span> <span class="kw">c</span>(</span>
<span id="cb46-69"><a href="explore-sparsity.html#cb46-69"></a>   <span class="dt">train_cv =</span> cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se],</span>
<span id="cb46-70"><a href="explore-sparsity.html#cb46-70"></a>   <span class="dt">train_oof =</span> train_oofPred_1se_error,</span>
<span id="cb46-71"><a href="explore-sparsity.html#cb46-71"></a>   <span class="dt">test =</span> test_pred_1se_error),</span>
<span id="cb46-72"><a href="explore-sparsity.html#cb46-72"></a>  <span class="dt">error_min =</span> <span class="kw">c</span>(</span>
<span id="cb46-73"><a href="explore-sparsity.html#cb46-73"></a>   <span class="dt">train_cv =</span> cv_fit<span class="op">$</span>cvm[cv_fit<span class="op">$</span>lambda <span class="op">==</span><span class="st"> </span>cv_fit<span class="op">$</span>lambda.min],</span>
<span id="cb46-74"><a href="explore-sparsity.html#cb46-74"></a>   <span class="dt">train_oof =</span> train_oofPred_min_error,</span>
<span id="cb46-75"><a href="explore-sparsity.html#cb46-75"></a>   <span class="dt">test =</span> test_pred_min_error)</span>
<span id="cb46-76"><a href="explore-sparsity.html#cb46-76"></a>  )</span>
<span id="cb46-77"><a href="explore-sparsity.html#cb46-77"></a>                  </span>
<span id="cb46-78"><a href="explore-sparsity.html#cb46-78"></a>}</span></code></pre></div>
<p>Examine model performance.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="explore-sparsity.html#cb47-1"></a> <span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>)) </span>
<span id="cb47-2"><a href="explore-sparsity.html#cb47-2"></a></span>
<span id="cb47-3"><a href="explore-sparsity.html#cb47-3"></a> lasso_errors_mtx &lt;-<span class="st"> </span><span class="kw">plot_cv_f</span>(cv_lasso, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span></code></pre></div>
<pre><code>## Warning: package &#39;glmnet&#39; was built under R version 4.0.2</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loaded glmnet 4.0-2</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="explore-sparsity.html#cb51-1"></a> <span class="kw">title</span>(<span class="st">&#39;lasso&#39;</span>)</span>
<span id="cb51-2"><a href="explore-sparsity.html#cb51-2"></a></span>
<span id="cb51-3"><a href="explore-sparsity.html#cb51-3"></a> rifge_errors_mtx &lt;-<span class="st"> </span><span class="kw">plot_cv_f</span>(cv_ridge, <span class="dt">Nzero=</span>F, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb51-4"><a href="explore-sparsity.html#cb51-4"></a> <span class="kw">title</span>(<span class="st">&#39;ridge&#39;</span>)</span>
<span id="cb51-5"><a href="explore-sparsity.html#cb51-5"></a></span>
<span id="cb51-6"><a href="explore-sparsity.html#cb51-6"></a> enet_errors_mtx &lt;-<span class="st">  </span><span class="kw">plot_cv_f</span>(cv_enet, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>))</span>
<span id="cb51-7"><a href="explore-sparsity.html#cb51-7"></a> <span class="kw">title</span>(<span class="st">&#39;enet&#39;</span>)</span>
<span id="cb51-8"><a href="explore-sparsity.html#cb51-8"></a></span>
<span id="cb51-9"><a href="explore-sparsity.html#cb51-9"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">outer=</span>T, <span class="dt">cex=</span><span class="fl">1.25</span>, <span class="st">&#39;log(Lambda)&#39;</span>)</span>
<span id="cb51-10"><a href="explore-sparsity.html#cb51-10"></a> <span class="kw">mtext</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">outer=</span>T, <span class="dt">cex=</span><span class="fl">1.25</span>, cv_lasso<span class="op">$</span>name)</span></code></pre></div>
<div class="figure"><span id="fig:lookFits"></span>
<img src="Static/figures/lookFits-1.png" alt="compare fits" width="1056" />
<p class="caption">
Figure 4.1: compare fits
</p>
</div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="explore-sparsity.html#cb52-1"></a>errors_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb52-2"><a href="explore-sparsity.html#cb52-2"></a>  <span class="dt">lasso =</span> lasso_errors_mtx, <span class="dt">ridge =</span> rifge_errors_mtx, <span class="dt">enet =</span> enet_errors_mtx</span>
<span id="cb52-3"><a href="explore-sparsity.html#cb52-3"></a>)</span>
<span id="cb52-4"><a href="explore-sparsity.html#cb52-4"></a></span>
<span id="cb52-5"><a href="explore-sparsity.html#cb52-5"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">t</span>(errors_frm)<span class="op">*</span><span class="dv">100</span>,</span>
<span id="cb52-6"><a href="explore-sparsity.html#cb52-6"></a> <span class="dt">caption =</span> <span class="st">&#39;Misclassifiaction error rates&#39;</span>,</span>
<span id="cb52-7"><a href="explore-sparsity.html#cb52-7"></a> <span class="dt">digits=</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb52-8"><a href="explore-sparsity.html#cb52-8"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:printErrors">Table 4.3: </span>Misclassifiaction error rates
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
train_cv
</th>
<th style="text-align:right;">
train_oof
</th>
<th style="text-align:right;">
test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
lasso.error_1se
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
9.7
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
lasso.error_min
</td>
<td style="text-align:right;">
7.0
</td>
<td style="text-align:right;">
9.7
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
ridge.error_1se
</td>
<td style="text-align:right;">
13.5
</td>
<td style="text-align:right;">
19.0
</td>
<td style="text-align:right;">
18.4
</td>
</tr>
<tr>
<td style="text-align:left;">
ridge.error_min
</td>
<td style="text-align:right;">
12.5
</td>
<td style="text-align:right;">
16.9
</td>
<td style="text-align:right;">
16.2
</td>
</tr>
<tr>
<td style="text-align:left;">
enet.error_1se
</td>
<td style="text-align:right;">
7.2
</td>
<td style="text-align:right;">
11.1
</td>
<td style="text-align:right;">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;">
enet.error_min
</td>
<td style="text-align:right;">
6.7
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
</tbody>
</table>
<p>We see that the lasso and enet models do better than the ridge model.
These is very little difference between the min lambda and the
the one standard error rule lambda models (the two are the same for the
lasso in this data set). We also see that the training data out-of-fold
estimates of misclassification error rates are much closer to the
test set estimates than are the cv estimated rates. This has
been our experience with regularized regression models fitted to
genomic scale data. It should also be noted that the cv estimates of
misclassification rates become more biased as the sample size decreases,
as we will show in Section <a href="model-suite.html#model-suite">5</a>.</p>
</div>
</div>
<div id="relaxed-lasso-and-blended-mix" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Relaxed lasso and blended mix</h2>
<p>Next we look at the so-called <code>relaxed lasso</code> and
the <code>blended mix</code> which is an optimized shrinkage
between the relaxed lasso and the regular lasso.</p>
<!--
The relaxed fit takes quite a bit longer.  
-->
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="explore-sparsity.html#cb53-1"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb53-2"><a href="explore-sparsity.html#cb53-2"></a></span>
<span id="cb53-3"><a href="explore-sparsity.html#cb53-3"></a>cv_lassoR_sum &lt;-<span class="st"> </span><span class="kw">print</span>(cv_lassoR)</span></code></pre></div>
<pre><code>## 
## Call:  glmnet::cv.glmnet(x = train_lcpm_mtx, y = train_group_vec, type.measure = &quot;class&quot;,      foldid = train_foldid_vec, keep = T, relax = T, alpha = 1,      family = &quot;binomial&quot;, nlambda = 30) 
## 
## Measure: Misclassification Error 
## 
##     Gamma Lambda Measure       SE Nonzero
## min   0.5 0.0379 0.06748 0.005274      35
## 1se   0.5 0.0379 0.06748 0.005274      35</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="explore-sparsity.html#cb55-1"></a><span class="kw">plot</span>(cv_lassoR)</span></code></pre></div>
<div class="figure"><span id="fig:lookLassoR"></span>
<img src="Static/figures/lookLassoR-1.png" alt="lassoR fit" width="480" />
<p class="caption">
Figure 4.2: lassoR fit
</p>
</div>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="explore-sparsity.html#cb56-1"></a><span class="co"># only report  1se</span></span>
<span id="cb56-2"><a href="explore-sparsity.html#cb56-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb56-3"><a href="explore-sparsity.html#cb56-3"></a>ndx_min &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda.min, cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb56-4"><a href="explore-sparsity.html#cb56-4"></a></span>
<span id="cb56-5"><a href="explore-sparsity.html#cb56-5"></a><span class="co"># only show 1se anyway</span></span>
<span id="cb56-6"><a href="explore-sparsity.html#cb56-6"></a><span class="co"># if(ndx_1se != ndx_min) stop(&quot;lambda.1se != lambda.min&quot;)</span></span>
<span id="cb56-7"><a href="explore-sparsity.html#cb56-7"></a></span>
<span id="cb56-8"><a href="explore-sparsity.html#cb56-8"></a></span>
<span id="cb56-9"><a href="explore-sparsity.html#cb56-9"></a><span class="co"># train oof data</span></span>
<span id="cb56-10"><a href="explore-sparsity.html#cb56-10"></a><span class="co"># Get relaxed lasso (gamma=0) oof error</span></span>
<span id="cb56-11"><a href="explore-sparsity.html#cb56-11"></a>train_oofPred_relaxed_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb56-12"><a href="explore-sparsity.html#cb56-12"></a>  cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&quot;g:0&quot;</span>]][, ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;HCC&quot;</span>, <span class="st">&quot;Control&quot;</span></span>
<span id="cb56-13"><a href="explore-sparsity.html#cb56-13"></a>)</span>
<span id="cb56-14"><a href="explore-sparsity.html#cb56-14"></a>train_oofPred_relaxed_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_relaxed_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb56-15"><a href="explore-sparsity.html#cb56-15"></a></span>
<span id="cb56-16"><a href="explore-sparsity.html#cb56-16"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb56-17"><a href="explore-sparsity.html#cb56-17"></a>train_oofPred_blended_1se_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb56-18"><a href="explore-sparsity.html#cb56-18"></a>  cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&quot;g:0.5&quot;</span>]][, ndx_1se] <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;HCC&quot;</span>, <span class="st">&quot;Control&quot;</span></span>
<span id="cb56-19"><a href="explore-sparsity.html#cb56-19"></a>)</span>
<span id="cb56-20"><a href="explore-sparsity.html#cb56-20"></a>train_oofPred_blended_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(train_oofPred_blended_1se_vec <span class="op">!=</span><span class="st"> </span>train_group_vec)</span>
<span id="cb56-21"><a href="explore-sparsity.html#cb56-21"></a></span>
<span id="cb56-22"><a href="explore-sparsity.html#cb56-22"></a></span>
<span id="cb56-23"><a href="explore-sparsity.html#cb56-23"></a><span class="co"># Test set error - relaxed</span></span>
<span id="cb56-24"><a href="explore-sparsity.html#cb56-24"></a>test_pred_relaxed_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb56-25"><a href="explore-sparsity.html#cb56-25"></a>  cv_lassoR,</span>
<span id="cb56-26"><a href="explore-sparsity.html#cb56-26"></a>  <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb56-27"><a href="explore-sparsity.html#cb56-27"></a>  <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb56-28"><a href="explore-sparsity.html#cb56-28"></a>  <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb56-29"><a href="explore-sparsity.html#cb56-29"></a>  <span class="dt">gamma =</span> <span class="dv">0</span></span>
<span id="cb56-30"><a href="explore-sparsity.html#cb56-30"></a>)</span>
<span id="cb56-31"><a href="explore-sparsity.html#cb56-31"></a>test_pred_relaxed_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_relaxed_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb56-32"><a href="explore-sparsity.html#cb56-32"></a></span>
<span id="cb56-33"><a href="explore-sparsity.html#cb56-33"></a><span class="co"># Test set error - blended</span></span>
<span id="cb56-34"><a href="explore-sparsity.html#cb56-34"></a>test_pred_blended_1se_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb56-35"><a href="explore-sparsity.html#cb56-35"></a>  cv_lassoR,</span>
<span id="cb56-36"><a href="explore-sparsity.html#cb56-36"></a>  <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb56-37"><a href="explore-sparsity.html#cb56-37"></a>  <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb56-38"><a href="explore-sparsity.html#cb56-38"></a>  <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb56-39"><a href="explore-sparsity.html#cb56-39"></a>  <span class="dt">gamma =</span> <span class="fl">0.5</span></span>
<span id="cb56-40"><a href="explore-sparsity.html#cb56-40"></a>)</span>
<span id="cb56-41"><a href="explore-sparsity.html#cb56-41"></a>test_pred_blended_1se_error &lt;-<span class="st"> </span><span class="kw">mean</span>(test_pred_blended_1se_vec <span class="op">!=</span><span class="st"> </span>test_group_vec)</span>
<span id="cb56-42"><a href="explore-sparsity.html#cb56-42"></a></span>
<span id="cb56-43"><a href="explore-sparsity.html#cb56-43"></a>cv_lassoR_1se_error &lt;-<span class="st"> </span>cv_lassoR_sum[<span class="st">&quot;1se&quot;</span>, <span class="st">&quot;Measure&quot;</span>]</span>
<span id="cb56-44"><a href="explore-sparsity.html#cb56-44"></a>cv_lassoR_min_error &lt;-<span class="st"> </span>cv_lassoR_sum[<span class="st">&quot;min&quot;</span>, <span class="st">&quot;Measure&quot;</span>]</span>
<span id="cb56-45"><a href="explore-sparsity.html#cb56-45"></a></span>
<span id="cb56-46"><a href="explore-sparsity.html#cb56-46"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">t</span>(<span class="kw">data.frame</span>(</span>
<span id="cb56-47"><a href="explore-sparsity.html#cb56-47"></a>  <span class="dt">train_lassoR_cv =</span> cv_lassoR_1se_error,</span>
<span id="cb56-48"><a href="explore-sparsity.html#cb56-48"></a>  <span class="dt">train_blended_oof =</span> train_oofPred_blended_1se_error,</span>
<span id="cb56-49"><a href="explore-sparsity.html#cb56-49"></a>  <span class="dt">train_relaxed_oof =</span> train_oofPred_relaxed_1se_error,</span>
<span id="cb56-50"><a href="explore-sparsity.html#cb56-50"></a>  <span class="dt">test_blended_oof =</span> test_pred_blended_1se_error,</span>
<span id="cb56-51"><a href="explore-sparsity.html#cb56-51"></a>  <span class="dt">test_relaxed_oof =</span> test_pred_relaxed_1se_error</span>
<span id="cb56-52"><a href="explore-sparsity.html#cb56-52"></a>)) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>,</span>
<span id="cb56-53"><a href="explore-sparsity.html#cb56-53"></a><span class="dt">digits =</span> <span class="dv">1</span>,</span>
<span id="cb56-54"><a href="explore-sparsity.html#cb56-54"></a><span class="dt">caption =</span> <span class="st">&quot;Relaxed lasso and blended mix error rates&quot;</span></span>
<span id="cb56-55"><a href="explore-sparsity.html#cb56-55"></a>) <span class="op">%&gt;%</span></span>
<span id="cb56-56"><a href="explore-sparsity.html#cb56-56"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:lookLassoR">Table 4.4: </span>Relaxed lasso and blended mix error rates
</caption>
<tbody>
<tr>
<td style="text-align:left;">
train_lassoR_cv
</td>
<td style="text-align:right;">
6.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_oof
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
train_relaxed_oof
</td>
<td style="text-align:right;">
11.1
</td>
</tr>
<tr>
<td style="text-align:left;">
test_blended_oof
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
test_relaxed_oof
</td>
<td style="text-align:right;">
10.9
</td>
</tr>
</tbody>
</table>
<p>The relaxed lasso and blended mix error rates are comparable to the
regular lasso fit error rate. We see here too that the reported cv
error rates are quite optimistic, while out-of-fold error rates
continue to be good indicators of unseen data error rates.</p>
</div>
<div id="examination-of-sensitivity-vs-specificity" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Examination of sensitivity vs specificity</h2>
<p>In the results above we reported error rates without inspecting the
sensitivity versus specificity trade off. Here we look at this
question with the help of ROC curves.</p>
<div id="training-data-out-of-fold-roc-curves" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Training data out-of-fold ROC curves</h3>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="explore-sparsity.html#cb57-1"></a><span class="co"># train</span></span>
<span id="cb57-2"><a href="explore-sparsity.html#cb57-2"></a><span class="co"># lasso</span></span>
<span id="cb57-3"><a href="explore-sparsity.html#cb57-3"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lasso<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lasso<span class="op">$</span>lambda)</span>
<span id="cb57-4"><a href="explore-sparsity.html#cb57-4"></a>train_lasso_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lasso<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb57-5"><a href="explore-sparsity.html#cb57-5"></a>train_lasso_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb57-6"><a href="explore-sparsity.html#cb57-6"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb57-7"><a href="explore-sparsity.html#cb57-7"></a> <span class="dt">predictor =</span> train_lasso_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="explore-sparsity.html#cb60-1"></a><span class="co"># enet</span></span>
<span id="cb60-2"><a href="explore-sparsity.html#cb60-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_enet<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_enet<span class="op">$</span>lambda)</span>
<span id="cb60-3"><a href="explore-sparsity.html#cb60-3"></a>train_enet_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_enet<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb60-4"><a href="explore-sparsity.html#cb60-4"></a>train_enet_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb60-5"><a href="explore-sparsity.html#cb60-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb60-6"><a href="explore-sparsity.html#cb60-6"></a> <span class="dt">predictor =</span> train_enet_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="explore-sparsity.html#cb62-1"></a><span class="co"># lasso - relaxed</span></span>
<span id="cb62-2"><a href="explore-sparsity.html#cb62-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb62-3"><a href="explore-sparsity.html#cb62-3"></a>train_lassoR_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0&#39;</span>]][,ndx_1se])</span>
<span id="cb62-4"><a href="explore-sparsity.html#cb62-4"></a>train_lassoR_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb62-5"><a href="explore-sparsity.html#cb62-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb62-6"><a href="explore-sparsity.html#cb62-6"></a> <span class="dt">predictor =</span> train_lassoR_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="explore-sparsity.html#cb64-1"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb64-2"><a href="explore-sparsity.html#cb64-2"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb64-3"><a href="explore-sparsity.html#cb64-3"></a>train_blended_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0.5&#39;</span>]][,ndx_1se])</span>
<span id="cb64-4"><a href="explore-sparsity.html#cb64-4"></a>train_blended_roc &lt;-<span class="st"> </span>pROC<span class="op">::</span><span class="kw">roc</span>(</span>
<span id="cb64-5"><a href="explore-sparsity.html#cb64-5"></a> <span class="dt">response =</span> <span class="kw">as.numeric</span>(train_group_vec<span class="op">==</span><span class="st">&#39;HCC&#39;</span>),</span>
<span id="cb64-6"><a href="explore-sparsity.html#cb64-6"></a> <span class="dt">predictor =</span> train_blended_oofProb_vec)</span></code></pre></div>
<pre><code>## Setting levels: control = 0, case = 1
## Setting direction: controls &lt; cases</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="explore-sparsity.html#cb66-1"></a><span class="kw">plot</span>(train_lasso_roc, <span class="dt">col=</span>col_vec[<span class="dv">1</span>])</span>
<span id="cb66-2"><a href="explore-sparsity.html#cb66-2"></a><span class="kw">lines</span>(train_enet_roc, <span class="dt">col=</span>col_vec[<span class="dv">2</span>])</span>
<span id="cb66-3"><a href="explore-sparsity.html#cb66-3"></a><span class="kw">lines</span>(train_lassoR_roc, <span class="dt">col=</span>col_vec[<span class="dv">3</span>])</span>
<span id="cb66-4"><a href="explore-sparsity.html#cb66-4"></a><span class="kw">lines</span>(train_blended_roc, <span class="dt">col=</span>col_vec[<span class="dv">4</span>])</span>
<span id="cb66-5"><a href="explore-sparsity.html#cb66-5"></a></span>
<span id="cb66-6"><a href="explore-sparsity.html#cb66-6"></a><span class="kw">legend</span>(<span class="st">&#39;bottomright&#39;</span>, <span class="dt">title=</span><span class="st">&#39;AUC&#39;</span>,</span>
<span id="cb66-7"><a href="explore-sparsity.html#cb66-7"></a> <span class="dt">legend=</span><span class="kw">c</span>(</span>
<span id="cb66-8"><a href="explore-sparsity.html#cb66-8"></a>  <span class="kw">paste</span>(<span class="st">&#39;lasso =&#39;</span>, <span class="kw">round</span>(train_lasso_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb66-9"><a href="explore-sparsity.html#cb66-9"></a>  <span class="kw">paste</span>(<span class="st">&#39;enet =&#39;</span>, <span class="kw">round</span>(train_enet_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb66-10"><a href="explore-sparsity.html#cb66-10"></a>  <span class="kw">paste</span>(<span class="st">&#39;lassoR =&#39;</span>, <span class="kw">round</span>(train_lassoR_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>)),</span>
<span id="cb66-11"><a href="explore-sparsity.html#cb66-11"></a>  <span class="kw">paste</span>(<span class="st">&#39;blended =&#39;</span>, <span class="kw">round</span>(train_blended_roc[[<span class="st">&#39;auc&#39;</span>]],<span class="dv">3</span>))</span>
<span id="cb66-12"><a href="explore-sparsity.html#cb66-12"></a> ),</span>
<span id="cb66-13"><a href="explore-sparsity.html#cb66-13"></a> <span class="dt">text.col =</span> col_vec[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>],</span>
<span id="cb66-14"><a href="explore-sparsity.html#cb66-14"></a> <span class="dt">bty=</span><span class="st">&#39;n&#39;</span></span>
<span id="cb66-15"><a href="explore-sparsity.html#cb66-15"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:trainROC"></span>
<img src="Static/figures/trainROC-1.png" alt="Train data out-of-sample ROCs" width="480" />
<p class="caption">
Figure 4.3: Train data out-of-sample ROCs
</p>
</div>
<p>Compare thresholds for 90% Specificity:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="explore-sparsity.html#cb67-1"></a> lasso_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lasso_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb67-2"><a href="explore-sparsity.html#cb67-2"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb67-3"><a href="explore-sparsity.html#cb67-3"></a></span>
<span id="cb67-4"><a href="explore-sparsity.html#cb67-4"></a> enet_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_enet_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb67-5"><a href="explore-sparsity.html#cb67-5"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb67-6"><a href="explore-sparsity.html#cb67-6"></a></span>
<span id="cb67-7"><a href="explore-sparsity.html#cb67-7"></a> lassoR_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lassoR_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb67-8"><a href="explore-sparsity.html#cb67-8"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb67-9"><a href="explore-sparsity.html#cb67-9"></a></span>
<span id="cb67-10"><a href="explore-sparsity.html#cb67-10"></a> blended_ndx &lt;-<span class="st"> </span><span class="kw">with</span>(<span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_blended_roc, <span class="dt">transpose=</span>F)), </span>
<span id="cb67-11"><a href="explore-sparsity.html#cb67-11"></a>   <span class="kw">min</span>(<span class="kw">which</span>(specificity <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.9</span>)))</span>
<span id="cb67-12"><a href="explore-sparsity.html#cb67-12"></a></span>
<span id="cb67-13"><a href="explore-sparsity.html#cb67-13"></a>  spec90_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">rbind</span>(</span>
<span id="cb67-14"><a href="explore-sparsity.html#cb67-14"></a>  <span class="dt">lasso=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lasso_roc, <span class="dt">transpose=</span>F))[lasso_ndx,],</span>
<span id="cb67-15"><a href="explore-sparsity.html#cb67-15"></a>  <span class="dt">enet=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_enet_roc, <span class="dt">transpose=</span>F))[enet_ndx,],</span>
<span id="cb67-16"><a href="explore-sparsity.html#cb67-16"></a>  <span class="dt">lassoR=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_lassoR_roc, <span class="dt">transpose=</span>F))[lassoR_ndx,],</span>
<span id="cb67-17"><a href="explore-sparsity.html#cb67-17"></a>  <span class="dt">blended=</span><span class="kw">as.data.frame</span>(pROC<span class="op">::</span><span class="kw">coords</span>(train_blended_roc, <span class="dt">transpose=</span>F))[blended_ndx,]</span>
<span id="cb67-18"><a href="explore-sparsity.html#cb67-18"></a> ))</span>
<span id="cb67-19"><a href="explore-sparsity.html#cb67-19"></a></span>
<span id="cb67-20"><a href="explore-sparsity.html#cb67-20"></a></span>
<span id="cb67-21"><a href="explore-sparsity.html#cb67-21"></a>knitr<span class="op">::</span><span class="kw">kable</span>(spec90_frm,</span>
<span id="cb67-22"><a href="explore-sparsity.html#cb67-22"></a>  <span class="dt">digits=</span><span class="dv">3</span>,</span>
<span id="cb67-23"><a href="explore-sparsity.html#cb67-23"></a>  <span class="dt">caption=</span><span class="st">&quot;Specificity = .90 Coordinates&quot;</span></span>
<span id="cb67-24"><a href="explore-sparsity.html#cb67-24"></a>) <span class="op">%&gt;%</span></span>
<span id="cb67-25"><a href="explore-sparsity.html#cb67-25"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:thresh90">Table 4.5: </span>Specificity = .90 Coordinates
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
threshold
</th>
<th style="text-align:right;">
specificity
</th>
<th style="text-align:right;">
sensitivity
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
lasso
</td>
<td style="text-align:right;">
0.337
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.932
</td>
</tr>
<tr>
<td style="text-align:left;">
enet
</td>
<td style="text-align:right;">
0.347
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.937
</td>
</tr>
<tr>
<td style="text-align:left;">
lassoR
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.894
</td>
</tr>
<tr>
<td style="text-align:left;">
blended
</td>
<td style="text-align:right;">
0.031
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
0.899
</td>
</tr>
</tbody>
</table>
<p>This is strange.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="explore-sparsity.html#cb68-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb68-2"><a href="explore-sparsity.html#cb68-2"></a></span>
<span id="cb68-3"><a href="explore-sparsity.html#cb68-3"></a><span class="co"># lasso</span></span>
<span id="cb68-4"><a href="explore-sparsity.html#cb68-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_lasso_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb68-5"><a href="explore-sparsity.html#cb68-5"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb68-6"><a href="explore-sparsity.html#cb68-6"></a>)</span>
<span id="cb68-7"><a href="explore-sparsity.html#cb68-7"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_lasso_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb68-8"><a href="explore-sparsity.html#cb68-8"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb68-9"><a href="explore-sparsity.html#cb68-9"></a>)</span>
<span id="cb68-10"><a href="explore-sparsity.html#cb68-10"></a><span class="kw">title</span>(<span class="st">&quot;lasso&quot;</span>)</span>
<span id="cb68-11"><a href="explore-sparsity.html#cb68-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Control&quot;</span>, <span class="st">&quot;HCC&quot;</span>), <span class="dt">text.col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb68-12"><a href="explore-sparsity.html#cb68-12"></a></span>
<span id="cb68-13"><a href="explore-sparsity.html#cb68-13"></a><span class="co"># enet</span></span>
<span id="cb68-14"><a href="explore-sparsity.html#cb68-14"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_enet_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb68-15"><a href="explore-sparsity.html#cb68-15"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb68-16"><a href="explore-sparsity.html#cb68-16"></a>)</span>
<span id="cb68-17"><a href="explore-sparsity.html#cb68-17"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_enet_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb68-18"><a href="explore-sparsity.html#cb68-18"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb68-19"><a href="explore-sparsity.html#cb68-19"></a>)</span>
<span id="cb68-20"><a href="explore-sparsity.html#cb68-20"></a><span class="kw">title</span>(<span class="st">&quot;enet&quot;</span>)</span>
<span id="cb68-21"><a href="explore-sparsity.html#cb68-21"></a></span>
<span id="cb68-22"><a href="explore-sparsity.html#cb68-22"></a><span class="co"># lassoR</span></span>
<span id="cb68-23"><a href="explore-sparsity.html#cb68-23"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_lassoR_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb68-24"><a href="explore-sparsity.html#cb68-24"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb68-25"><a href="explore-sparsity.html#cb68-25"></a>)</span>
<span id="cb68-26"><a href="explore-sparsity.html#cb68-26"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_lassoR_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb68-27"><a href="explore-sparsity.html#cb68-27"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb68-28"><a href="explore-sparsity.html#cb68-28"></a>)</span>
<span id="cb68-29"><a href="explore-sparsity.html#cb68-29"></a><span class="kw">title</span>(<span class="st">&quot;lassoR&quot;</span>)</span>
<span id="cb68-30"><a href="explore-sparsity.html#cb68-30"></a></span>
<span id="cb68-31"><a href="explore-sparsity.html#cb68-31"></a><span class="co"># blended</span></span>
<span id="cb68-32"><a href="explore-sparsity.html#cb68-32"></a><span class="kw">plot</span>(<span class="kw">density</span>(train_blended_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb68-33"><a href="explore-sparsity.html#cb68-33"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb68-34"><a href="explore-sparsity.html#cb68-34"></a>)</span>
<span id="cb68-35"><a href="explore-sparsity.html#cb68-35"></a><span class="kw">lines</span>(<span class="kw">density</span>(train_blended_oofProb_vec[train_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb68-36"><a href="explore-sparsity.html#cb68-36"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb68-37"><a href="explore-sparsity.html#cb68-37"></a>)</span>
<span id="cb68-38"><a href="explore-sparsity.html#cb68-38"></a><span class="kw">title</span>(<span class="st">&quot;blended&quot;</span>)</span>
<span id="cb68-39"><a href="explore-sparsity.html#cb68-39"></a></span>
<span id="cb68-40"><a href="explore-sparsity.html#cb68-40"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">outer =</span> T, <span class="st">&quot;out-of-fold predicted probability&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span>
<span id="cb68-41"><a href="explore-sparsity.html#cb68-41"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dt">outer =</span> T, <span class="st">&quot;density&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span></code></pre></div>
<div class="figure"><span id="fig:trainOOFprops"></span>
<img src="Static/figures/trainOOFprops-1.png" alt="Train data out-of-fold predicted probabilities" width="960" />
<p class="caption">
Figure 4.4: Train data out-of-fold predicted probabilities
</p>
</div>
<p>The relaxed lasso fit results in essentially dichotomized predicted probability
distribution - predicted probabilities arr very close to 0 or 1.</p>
<p>Look at test data ROC curves.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="explore-sparsity.html#cb69-1"></a><span class="co"># plot all</span></span>
<span id="cb69-2"><a href="explore-sparsity.html#cb69-2"></a><span class="kw">plot</span>(test_lasso_roc, <span class="dt">col =</span> col_vec[<span class="dv">1</span>])</span>
<span id="cb69-3"><a href="explore-sparsity.html#cb69-3"></a><span class="kw">lines</span>(test_enet_roc, <span class="dt">col =</span> col_vec[<span class="dv">2</span>])</span>
<span id="cb69-4"><a href="explore-sparsity.html#cb69-4"></a><span class="kw">lines</span>(test_lassoR_roc, <span class="dt">col =</span> col_vec[<span class="dv">3</span>])</span>
<span id="cb69-5"><a href="explore-sparsity.html#cb69-5"></a><span class="kw">lines</span>(test_blended_roc, <span class="dt">col =</span> col_vec[<span class="dv">4</span>])</span>
<span id="cb69-6"><a href="explore-sparsity.html#cb69-6"></a></span>
<span id="cb69-7"><a href="explore-sparsity.html#cb69-7"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb69-8"><a href="explore-sparsity.html#cb69-8"></a>  <span class="dt">title =</span> <span class="st">&quot;AUC&quot;</span>,</span>
<span id="cb69-9"><a href="explore-sparsity.html#cb69-9"></a>  <span class="dt">legend =</span> <span class="kw">c</span>(</span>
<span id="cb69-10"><a href="explore-sparsity.html#cb69-10"></a>    <span class="kw">paste</span>(<span class="st">&quot;lasso =&quot;</span>, <span class="kw">round</span>(test_lasso_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb69-11"><a href="explore-sparsity.html#cb69-11"></a>    <span class="kw">paste</span>(<span class="st">&quot;enet =&quot;</span>, <span class="kw">round</span>(test_enet_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb69-12"><a href="explore-sparsity.html#cb69-12"></a>    <span class="kw">paste</span>(<span class="st">&quot;lassoR =&quot;</span>, <span class="kw">round</span>(test_lassoR_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>)),</span>
<span id="cb69-13"><a href="explore-sparsity.html#cb69-13"></a>    <span class="kw">paste</span>(<span class="st">&quot;blended =&quot;</span>, <span class="kw">round</span>(test_blended_roc[[<span class="st">&quot;auc&quot;</span>]], <span class="dv">3</span>))</span>
<span id="cb69-14"><a href="explore-sparsity.html#cb69-14"></a>  ),</span>
<span id="cb69-15"><a href="explore-sparsity.html#cb69-15"></a>  <span class="dt">text.col =</span> col_vec[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>],</span>
<span id="cb69-16"><a href="explore-sparsity.html#cb69-16"></a>  <span class="dt">bty=</span><span class="st">&#39;n&#39;</span></span>
<span id="cb69-17"><a href="explore-sparsity.html#cb69-17"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:testROC2"></span>
<img src="Static/figures/testROC2-1.png" alt="Test data out-of-sample ROCs" width="480" />
<p class="caption">
Figure 4.5: Test data out-of-sample ROCs
</p>
</div>
<p>Look at densities of predicted probabilities.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="explore-sparsity.html#cb70-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb70-2"><a href="explore-sparsity.html#cb70-2"></a></span>
<span id="cb70-3"><a href="explore-sparsity.html#cb70-3"></a><span class="co"># lasso</span></span>
<span id="cb70-4"><a href="explore-sparsity.html#cb70-4"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_lasso_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb70-5"><a href="explore-sparsity.html#cb70-5"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb70-6"><a href="explore-sparsity.html#cb70-6"></a>)</span>
<span id="cb70-7"><a href="explore-sparsity.html#cb70-7"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_lasso_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb70-8"><a href="explore-sparsity.html#cb70-8"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb70-9"><a href="explore-sparsity.html#cb70-9"></a>)</span>
<span id="cb70-10"><a href="explore-sparsity.html#cb70-10"></a><span class="kw">title</span>(<span class="st">&quot;lasso&quot;</span>)</span>
<span id="cb70-11"><a href="explore-sparsity.html#cb70-11"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Control&quot;</span>, <span class="st">&quot;HCC&quot;</span>), <span class="dt">text.col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>))</span>
<span id="cb70-12"><a href="explore-sparsity.html#cb70-12"></a></span>
<span id="cb70-13"><a href="explore-sparsity.html#cb70-13"></a><span class="co"># enet</span></span>
<span id="cb70-14"><a href="explore-sparsity.html#cb70-14"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_enet_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb70-15"><a href="explore-sparsity.html#cb70-15"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb70-16"><a href="explore-sparsity.html#cb70-16"></a>)</span>
<span id="cb70-17"><a href="explore-sparsity.html#cb70-17"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_enet_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb70-18"><a href="explore-sparsity.html#cb70-18"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb70-19"><a href="explore-sparsity.html#cb70-19"></a>)</span>
<span id="cb70-20"><a href="explore-sparsity.html#cb70-20"></a><span class="kw">title</span>(<span class="st">&quot;enet&quot;</span>)</span>
<span id="cb70-21"><a href="explore-sparsity.html#cb70-21"></a></span>
<span id="cb70-22"><a href="explore-sparsity.html#cb70-22"></a><span class="co"># lassoR</span></span>
<span id="cb70-23"><a href="explore-sparsity.html#cb70-23"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_lassoR_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb70-24"><a href="explore-sparsity.html#cb70-24"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb70-25"><a href="explore-sparsity.html#cb70-25"></a>)</span>
<span id="cb70-26"><a href="explore-sparsity.html#cb70-26"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_lassoR_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb70-27"><a href="explore-sparsity.html#cb70-27"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb70-28"><a href="explore-sparsity.html#cb70-28"></a>)</span>
<span id="cb70-29"><a href="explore-sparsity.html#cb70-29"></a><span class="kw">title</span>(<span class="st">&quot;lassoR&quot;</span>)</span>
<span id="cb70-30"><a href="explore-sparsity.html#cb70-30"></a></span>
<span id="cb70-31"><a href="explore-sparsity.html#cb70-31"></a><span class="co">#sapply(split(test_lassoR_predProb_vec, test_group_vec), summary)</span></span>
<span id="cb70-32"><a href="explore-sparsity.html#cb70-32"></a></span>
<span id="cb70-33"><a href="explore-sparsity.html#cb70-33"></a></span>
<span id="cb70-34"><a href="explore-sparsity.html#cb70-34"></a><span class="co"># blended</span></span>
<span id="cb70-35"><a href="explore-sparsity.html#cb70-35"></a><span class="kw">plot</span>(<span class="kw">density</span>(test_blended_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;Control&quot;</span>]),</span>
<span id="cb70-36"><a href="explore-sparsity.html#cb70-36"></a>  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span></span>
<span id="cb70-37"><a href="explore-sparsity.html#cb70-37"></a>)</span>
<span id="cb70-38"><a href="explore-sparsity.html#cb70-38"></a><span class="kw">lines</span>(<span class="kw">density</span>(test_blended_predProb_vec[test_group_vec <span class="op">==</span><span class="st"> &quot;HCC&quot;</span>]),</span>
<span id="cb70-39"><a href="explore-sparsity.html#cb70-39"></a>  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span></span>
<span id="cb70-40"><a href="explore-sparsity.html#cb70-40"></a>)</span>
<span id="cb70-41"><a href="explore-sparsity.html#cb70-41"></a><span class="kw">title</span>(<span class="st">&quot;blended&quot;</span>)</span>
<span id="cb70-42"><a href="explore-sparsity.html#cb70-42"></a></span>
<span id="cb70-43"><a href="explore-sparsity.html#cb70-43"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">outer =</span> T, <span class="st">&quot;test set predicted probability&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span>
<span id="cb70-44"><a href="explore-sparsity.html#cb70-44"></a><span class="kw">mtext</span>(<span class="dt">side =</span> <span class="dv">2</span>, <span class="dt">outer =</span> T, <span class="st">&quot;density&quot;</span>, <span class="dt">cex =</span> <span class="fl">1.25</span>)</span></code></pre></div>
<div class="figure"><span id="fig:testOOFprobs"></span>
<img src="Static/figures/testOOFprobs-1.png" alt="Test data out-of-fold predicted probabilities" width="960" />
<p class="caption">
Figure 4.6: Test data out-of-fold predicted probabilities
</p>
</div>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="explore-sparsity.html#cb71-1"></a><span class="co"># Define plotting function</span></span>
<span id="cb71-2"><a href="explore-sparsity.html#cb71-2"></a>bxpPredProb_f &lt;-<span class="st"> </span><span class="cf">function</span>(cv_fit, <span class="dt">Gamma=</span><span class="ot">NULL</span>) {</span>
<span id="cb71-3"><a href="explore-sparsity.html#cb71-3"></a>  <span class="co"># Train - preval is out-of-fold linear predictor for training design points</span></span>
<span id="cb71-4"><a href="explore-sparsity.html#cb71-4"></a>  onese_ndx &lt;-<span class="st"> </span><span class="kw">match</span>(cv_fit<span class="op">$</span>lambda<span class="fl">.1</span>se, cv_fit<span class="op">$</span>lambda)</span>
<span id="cb71-5"><a href="explore-sparsity.html#cb71-5"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(Gamma)) </span>
<span id="cb71-6"><a href="explore-sparsity.html#cb71-6"></a>   train_1se_preval_vec &lt;-<span class="st"> </span>cv_fit<span class="op">$</span>fit.preval[, onese_ndx] <span class="cf">else</span></span>
<span id="cb71-7"><a href="explore-sparsity.html#cb71-7"></a>   train_1se_preval_vec &lt;-<span class="st"> </span>cv_fit<span class="op">$</span>fit.preval[[Gamma]][, onese_ndx] </span>
<span id="cb71-8"><a href="explore-sparsity.html#cb71-8"></a></span>
<span id="cb71-9"><a href="explore-sparsity.html#cb71-9"></a>  train_1se_predProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(train_1se_preval_vec)</span>
<span id="cb71-10"><a href="explore-sparsity.html#cb71-10"></a></span>
<span id="cb71-11"><a href="explore-sparsity.html#cb71-11"></a>  <span class="co"># Test</span></span>
<span id="cb71-12"><a href="explore-sparsity.html#cb71-12"></a>  test_1se_predProb_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb71-13"><a href="explore-sparsity.html#cb71-13"></a>    cv_fit,</span>
<span id="cb71-14"><a href="explore-sparsity.html#cb71-14"></a>    <span class="dt">newx =</span> test_lcpm_mtx,</span>
<span id="cb71-15"><a href="explore-sparsity.html#cb71-15"></a>    <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>,</span>
<span id="cb71-16"><a href="explore-sparsity.html#cb71-16"></a>    <span class="dt">type =</span> <span class="st">&quot;resp&quot;</span></span>
<span id="cb71-17"><a href="explore-sparsity.html#cb71-17"></a>  )</span>
<span id="cb71-18"><a href="explore-sparsity.html#cb71-18"></a></span>
<span id="cb71-19"><a href="explore-sparsity.html#cb71-19"></a>  tmp &lt;-<span class="st"> </span><span class="kw">c</span>(</span>
<span id="cb71-20"><a href="explore-sparsity.html#cb71-20"></a>    <span class="dt">train =</span> <span class="kw">split</span>(train_1se_predProb_vec, train_group_vec),</span>
<span id="cb71-21"><a href="explore-sparsity.html#cb71-21"></a>    <span class="dt">test =</span> <span class="kw">split</span>(test_1se_predProb_vec, test_group_vec)</span>
<span id="cb71-22"><a href="explore-sparsity.html#cb71-22"></a>  )</span>
<span id="cb71-23"><a href="explore-sparsity.html#cb71-23"></a>  <span class="kw">names</span>(tmp) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">sub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">.&quot;</span>, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="kw">names</span>(tmp)))</span>
<span id="cb71-24"><a href="explore-sparsity.html#cb71-24"></a></span>
<span id="cb71-25"><a href="explore-sparsity.html#cb71-25"></a>  <span class="kw">boxplot</span>(tmp)</span>
<span id="cb71-26"><a href="explore-sparsity.html#cb71-26"></a>}</span>
<span id="cb71-27"><a href="explore-sparsity.html#cb71-27"></a></span>
<span id="cb71-28"><a href="explore-sparsity.html#cb71-28"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">oma =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb71-29"><a href="explore-sparsity.html#cb71-29"></a></span>
<span id="cb71-30"><a href="explore-sparsity.html#cb71-30"></a><span class="kw">bxpPredProb_f</span>(cv_lasso)</span>
<span id="cb71-31"><a href="explore-sparsity.html#cb71-31"></a><span class="kw">title</span>(<span class="st">&#39;lasso&#39;</span>)</span>
<span id="cb71-32"><a href="explore-sparsity.html#cb71-32"></a></span>
<span id="cb71-33"><a href="explore-sparsity.html#cb71-33"></a><span class="kw">bxpPredProb_f</span>(cv_enet)</span>
<span id="cb71-34"><a href="explore-sparsity.html#cb71-34"></a><span class="kw">title</span>(<span class="st">&#39;enet&#39;</span>)</span>
<span id="cb71-35"><a href="explore-sparsity.html#cb71-35"></a></span>
<span id="cb71-36"><a href="explore-sparsity.html#cb71-36"></a><span class="kw">bxpPredProb_f</span>(cv_lassoR, <span class="dt">Gamma=</span><span class="st">&#39;g:0&#39;</span>)</span>
<span id="cb71-37"><a href="explore-sparsity.html#cb71-37"></a><span class="kw">title</span>(<span class="st">&#39;lassoR&#39;</span>)</span>
<span id="cb71-38"><a href="explore-sparsity.html#cb71-38"></a></span>
<span id="cb71-39"><a href="explore-sparsity.html#cb71-39"></a><span class="kw">bxpPredProb_f</span>(cv_lassoR, <span class="dt">Gamma=</span><span class="st">&#39;g:0.5&#39;</span>)</span>
<span id="cb71-40"><a href="explore-sparsity.html#cb71-40"></a><span class="kw">title</span>(<span class="st">&#39;blended&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fitPrevalByGroup"></span>
<img src="Static/figures/fitPrevalByGroup-1.png" alt="Predicted Probabilities - Train and Test" width="768" />
<p class="caption">
Figure 4.7: Predicted Probabilities - Train and Test
</p>
</div>
<!--
Another look - plot train and test set logistic curves with annotation.

The following shows that predicted classes come from fitted
probabilities - not out of sample probabilities.

Also shows that threshold is at 0.5 

SKIP
-->
<p>We have seen above that assessments of model performance based on the out-of-fold
predicted values are close to the test set assessments, and that
assessments based on prediction extracted from glmnet object are optimistic.
Here we look at confusion matrices to see how this affects the
classification results.</p>
<p>Here we us a threshold of 0.5 to dichotomize the predicted
probabilities into a class prediction, as is done in the
glmnet predictions.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="explore-sparsity.html#cb72-1"></a><span class="co"># lasso </span></span>
<span id="cb72-2"><a href="explore-sparsity.html#cb72-2"></a><span class="co">##########################</span></span>
<span id="cb72-3"><a href="explore-sparsity.html#cb72-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb72-4"><a href="explore-sparsity.html#cb72-4"></a>train_lasso_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-5"><a href="explore-sparsity.html#cb72-5"></a> cv_lasso,</span>
<span id="cb72-6"><a href="explore-sparsity.html#cb72-6"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb72-7"><a href="explore-sparsity.html#cb72-7"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-8"><a href="explore-sparsity.html#cb72-8"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-9"><a href="explore-sparsity.html#cb72-9"></a>)</span>
<span id="cb72-10"><a href="explore-sparsity.html#cb72-10"></a></span>
<span id="cb72-11"><a href="explore-sparsity.html#cb72-11"></a><span class="co"># train - oof</span></span>
<span id="cb72-12"><a href="explore-sparsity.html#cb72-12"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lasso<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lasso<span class="op">$</span>lambda)</span>
<span id="cb72-13"><a href="explore-sparsity.html#cb72-13"></a>train_lasso_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lasso<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb72-14"><a href="explore-sparsity.html#cb72-14"></a>train_lasso_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb72-15"><a href="explore-sparsity.html#cb72-15"></a>   train_lasso_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb72-16"><a href="explore-sparsity.html#cb72-16"></a></span>
<span id="cb72-17"><a href="explore-sparsity.html#cb72-17"></a><span class="co"># test </span></span>
<span id="cb72-18"><a href="explore-sparsity.html#cb72-18"></a>test_lasso_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-19"><a href="explore-sparsity.html#cb72-19"></a> cv_lasso,</span>
<span id="cb72-20"><a href="explore-sparsity.html#cb72-20"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb72-21"><a href="explore-sparsity.html#cb72-21"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-22"><a href="explore-sparsity.html#cb72-22"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-23"><a href="explore-sparsity.html#cb72-23"></a>)</span>
<span id="cb72-24"><a href="explore-sparsity.html#cb72-24"></a></span>
<span id="cb72-25"><a href="explore-sparsity.html#cb72-25"></a><span class="co"># enet</span></span>
<span id="cb72-26"><a href="explore-sparsity.html#cb72-26"></a><span class="co">##########################</span></span>
<span id="cb72-27"><a href="explore-sparsity.html#cb72-27"></a><span class="co"># train - cv predicted</span></span>
<span id="cb72-28"><a href="explore-sparsity.html#cb72-28"></a>train_enet_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-29"><a href="explore-sparsity.html#cb72-29"></a> cv_enet,</span>
<span id="cb72-30"><a href="explore-sparsity.html#cb72-30"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb72-31"><a href="explore-sparsity.html#cb72-31"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-32"><a href="explore-sparsity.html#cb72-32"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-33"><a href="explore-sparsity.html#cb72-33"></a>)</span>
<span id="cb72-34"><a href="explore-sparsity.html#cb72-34"></a></span>
<span id="cb72-35"><a href="explore-sparsity.html#cb72-35"></a><span class="co"># train - oof</span></span>
<span id="cb72-36"><a href="explore-sparsity.html#cb72-36"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_enet<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_enet<span class="op">$</span>lambda)</span>
<span id="cb72-37"><a href="explore-sparsity.html#cb72-37"></a>train_enet_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_enet<span class="op">$</span>fit.preval[,ndx_1se])</span>
<span id="cb72-38"><a href="explore-sparsity.html#cb72-38"></a>train_enet_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb72-39"><a href="explore-sparsity.html#cb72-39"></a>   train_enet_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb72-40"><a href="explore-sparsity.html#cb72-40"></a></span>
<span id="cb72-41"><a href="explore-sparsity.html#cb72-41"></a><span class="co"># test</span></span>
<span id="cb72-42"><a href="explore-sparsity.html#cb72-42"></a>test_enet_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-43"><a href="explore-sparsity.html#cb72-43"></a> cv_enet,</span>
<span id="cb72-44"><a href="explore-sparsity.html#cb72-44"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb72-45"><a href="explore-sparsity.html#cb72-45"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-46"><a href="explore-sparsity.html#cb72-46"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-47"><a href="explore-sparsity.html#cb72-47"></a>)</span>
<span id="cb72-48"><a href="explore-sparsity.html#cb72-48"></a></span>
<span id="cb72-49"><a href="explore-sparsity.html#cb72-49"></a></span>
<span id="cb72-50"><a href="explore-sparsity.html#cb72-50"></a><span class="co"># lasso - relaxed (gamma=0)</span></span>
<span id="cb72-51"><a href="explore-sparsity.html#cb72-51"></a><span class="co">##########################</span></span>
<span id="cb72-52"><a href="explore-sparsity.html#cb72-52"></a><span class="co"># train - cv predicted</span></span>
<span id="cb72-53"><a href="explore-sparsity.html#cb72-53"></a>train_lassoR_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-54"><a href="explore-sparsity.html#cb72-54"></a> cv_lassoR,</span>
<span id="cb72-55"><a href="explore-sparsity.html#cb72-55"></a> <span class="dt">g=</span><span class="dv">0</span>,</span>
<span id="cb72-56"><a href="explore-sparsity.html#cb72-56"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb72-57"><a href="explore-sparsity.html#cb72-57"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-58"><a href="explore-sparsity.html#cb72-58"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-59"><a href="explore-sparsity.html#cb72-59"></a>)</span>
<span id="cb72-60"><a href="explore-sparsity.html#cb72-60"></a></span>
<span id="cb72-61"><a href="explore-sparsity.html#cb72-61"></a><span class="co"># train - oof</span></span>
<span id="cb72-62"><a href="explore-sparsity.html#cb72-62"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb72-63"><a href="explore-sparsity.html#cb72-63"></a>train_lassoR_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0&#39;</span>]][,ndx_1se])</span>
<span id="cb72-64"><a href="explore-sparsity.html#cb72-64"></a>train_lassoR_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb72-65"><a href="explore-sparsity.html#cb72-65"></a>   train_lassoR_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb72-66"><a href="explore-sparsity.html#cb72-66"></a></span>
<span id="cb72-67"><a href="explore-sparsity.html#cb72-67"></a><span class="co"># test </span></span>
<span id="cb72-68"><a href="explore-sparsity.html#cb72-68"></a>test_lassoR_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-69"><a href="explore-sparsity.html#cb72-69"></a> cv_lassoR,</span>
<span id="cb72-70"><a href="explore-sparsity.html#cb72-70"></a> <span class="dt">g=</span><span class="dv">0</span>,</span>
<span id="cb72-71"><a href="explore-sparsity.html#cb72-71"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb72-72"><a href="explore-sparsity.html#cb72-72"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-73"><a href="explore-sparsity.html#cb72-73"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-74"><a href="explore-sparsity.html#cb72-74"></a>)</span>
<span id="cb72-75"><a href="explore-sparsity.html#cb72-75"></a></span>
<span id="cb72-76"><a href="explore-sparsity.html#cb72-76"></a></span>
<span id="cb72-77"><a href="explore-sparsity.html#cb72-77"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb72-78"><a href="explore-sparsity.html#cb72-78"></a><span class="co">###############################</span></span>
<span id="cb72-79"><a href="explore-sparsity.html#cb72-79"></a><span class="co"># train - cv predicted</span></span>
<span id="cb72-80"><a href="explore-sparsity.html#cb72-80"></a>train_blended_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-81"><a href="explore-sparsity.html#cb72-81"></a> cv_lassoR,</span>
<span id="cb72-82"><a href="explore-sparsity.html#cb72-82"></a> <span class="dt">g=</span><span class="fl">0.5</span>,</span>
<span id="cb72-83"><a href="explore-sparsity.html#cb72-83"></a> <span class="dt">newx=</span>train_lcpm_mtx,</span>
<span id="cb72-84"><a href="explore-sparsity.html#cb72-84"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-85"><a href="explore-sparsity.html#cb72-85"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span></span>
<span id="cb72-86"><a href="explore-sparsity.html#cb72-86"></a>)</span>
<span id="cb72-87"><a href="explore-sparsity.html#cb72-87"></a></span>
<span id="cb72-88"><a href="explore-sparsity.html#cb72-88"></a><span class="co"># train - oof</span></span>
<span id="cb72-89"><a href="explore-sparsity.html#cb72-89"></a>ndx_1se &lt;-<span class="st"> </span><span class="kw">match</span>(cv_lassoR<span class="op">$</span>lambda<span class="fl">.1</span>se,cv_lassoR<span class="op">$</span>lambda)</span>
<span id="cb72-90"><a href="explore-sparsity.html#cb72-90"></a>train_blended_oofProb_vec &lt;-<span class="st"> </span><span class="kw">logistic_f</span>(cv_lassoR<span class="op">$</span>fit.preval[[<span class="st">&#39;g:0.5&#39;</span>]][,ndx_1se])</span>
<span id="cb72-91"><a href="explore-sparsity.html#cb72-91"></a>train_blended_oofClass_vec &lt;-<span class="st"> </span><span class="kw">ifelse</span>(</span>
<span id="cb72-92"><a href="explore-sparsity.html#cb72-92"></a>   train_blended_oofProb_vec <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&#39;HCC&#39;</span>, <span class="st">&#39;Control&#39;</span>)</span>
<span id="cb72-93"><a href="explore-sparsity.html#cb72-93"></a></span>
<span id="cb72-94"><a href="explore-sparsity.html#cb72-94"></a><span class="co"># test</span></span>
<span id="cb72-95"><a href="explore-sparsity.html#cb72-95"></a>test_blended_predClass_vec &lt;-<span class="st"> </span><span class="kw">predict</span>(</span>
<span id="cb72-96"><a href="explore-sparsity.html#cb72-96"></a> cv_lassoR,</span>
<span id="cb72-97"><a href="explore-sparsity.html#cb72-97"></a> <span class="dt">g=</span><span class="fl">0.5</span>,</span>
<span id="cb72-98"><a href="explore-sparsity.html#cb72-98"></a> <span class="dt">newx=</span>test_lcpm_mtx,</span>
<span id="cb72-99"><a href="explore-sparsity.html#cb72-99"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb72-100"><a href="explore-sparsity.html#cb72-100"></a> <span class="dt">type=</span><span class="st">&#39;class&#39;</span> </span>
<span id="cb72-101"><a href="explore-sparsity.html#cb72-101"></a>)</span>
<span id="cb72-102"><a href="explore-sparsity.html#cb72-102"></a></span>
<span id="cb72-103"><a href="explore-sparsity.html#cb72-103"></a><span class="co"># put it all together</span></span>
<span id="cb72-104"><a href="explore-sparsity.html#cb72-104"></a><span class="co">########################</span></span>
<span id="cb72-105"><a href="explore-sparsity.html#cb72-105"></a>all_models_confustion_mtx &lt;-<span class="st"> </span><span class="kw">rbind</span>(</span>
<span id="cb72-106"><a href="explore-sparsity.html#cb72-106"></a> <span class="dt">train_lasso_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lasso_predClass_vec, train_group_vec)),</span>
<span id="cb72-107"><a href="explore-sparsity.html#cb72-107"></a> <span class="dt">train_lasso_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lasso_oofClass_vec, train_group_vec)),</span>
<span id="cb72-108"><a href="explore-sparsity.html#cb72-108"></a> <span class="dt">test_lasso =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_lasso_predClass_vec, test_group_vec)),</span>
<span id="cb72-109"><a href="explore-sparsity.html#cb72-109"></a> <span class="dt">train_enet_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_enet_predClass_vec, train_group_vec)),</span>
<span id="cb72-110"><a href="explore-sparsity.html#cb72-110"></a> <span class="dt">train_enet_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_enet_oofClass_vec, train_group_vec)),</span>
<span id="cb72-111"><a href="explore-sparsity.html#cb72-111"></a> <span class="dt">test_enet =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_enet_predClass_vec, test_group_vec)),</span>
<span id="cb72-112"><a href="explore-sparsity.html#cb72-112"></a> <span class="dt">train_lassoR_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lassoR_predClass_vec, train_group_vec)),</span>
<span id="cb72-113"><a href="explore-sparsity.html#cb72-113"></a> <span class="dt">train_lassoR_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_lassoR_oofClass_vec, train_group_vec)),</span>
<span id="cb72-114"><a href="explore-sparsity.html#cb72-114"></a> <span class="dt">test_lassoR =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_lassoR_predClass_vec, test_group_vec)),</span>
<span id="cb72-115"><a href="explore-sparsity.html#cb72-115"></a> <span class="dt">train_blended_cv =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_blended_predClass_vec, train_group_vec)),</span>
<span id="cb72-116"><a href="explore-sparsity.html#cb72-116"></a> <span class="dt">train_blended_oof =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(train_blended_oofClass_vec, train_group_vec)),</span>
<span id="cb72-117"><a href="explore-sparsity.html#cb72-117"></a> <span class="dt">test_blended =</span> <span class="kw">as.vector</span>(<span class="kw">table</span>(test_blended_predClass_vec, test_group_vec))</span>
<span id="cb72-118"><a href="explore-sparsity.html#cb72-118"></a>)</span>
<span id="cb72-119"><a href="explore-sparsity.html#cb72-119"></a><span class="kw">colnames</span>(all_models_confustion_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;C:C&#39;</span>,<span class="st">&#39;C:H&#39;</span>,<span class="st">&#39;H:C&#39;</span>, <span class="st">&#39;H:H&#39;</span>)</span>
<span id="cb72-120"><a href="explore-sparsity.html#cb72-120"></a></span>
<span id="cb72-121"><a href="explore-sparsity.html#cb72-121"></a></span>
<span id="cb72-122"><a href="explore-sparsity.html#cb72-122"></a>all_models_confustionRates_mtx &lt;-<span class="st"> </span><span class="kw">sweep</span>(</span>
<span id="cb72-123"><a href="explore-sparsity.html#cb72-123"></a> all_models_confustion_mtx, <span class="dv">1</span>, <span class="kw">rowSums</span>(all_models_confustion_mtx), <span class="st">&#39;/&#39;</span>)</span>
<span id="cb72-124"><a href="explore-sparsity.html#cb72-124"></a></span>
<span id="cb72-125"><a href="explore-sparsity.html#cb72-125"></a>all_models_confustionRates_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(all_models_confustionRates_mtx,</span>
<span id="cb72-126"><a href="explore-sparsity.html#cb72-126"></a>  <span class="dt">error =</span> <span class="kw">rowSums</span>(all_models_confustionRates_mtx[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]))</span>
<span id="cb72-127"><a href="explore-sparsity.html#cb72-127"></a></span>
<span id="cb72-128"><a href="explore-sparsity.html#cb72-128"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dv">100</span><span class="op">*</span>all_models_confustionRates_mtx, </span>
<span id="cb72-129"><a href="explore-sparsity.html#cb72-129"></a>  <span class="dt">caption=</span><span class="st">&quot;confusion: Columns are Truth:Predicted&quot;</span>,</span>
<span id="cb72-130"><a href="explore-sparsity.html#cb72-130"></a>  <span class="dt">digits=</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb72-131"><a href="explore-sparsity.html#cb72-131"></a><span class="st">  </span>kableExtra<span class="op">::</span><span class="kw">kable_styling</span>(<span class="dt">full_width =</span> F)</span></code></pre></div>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:confMtxTrainLasso">Table 4.6: </span>confusion: Columns are Truth:Predicted
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
C:C
</th>
<th style="text-align:right;">
C:H
</th>
<th style="text-align:right;">
H:C
</th>
<th style="text-align:right;">
H:H
</th>
<th style="text-align:right;">
error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
train_lasso_cv
</td>
<td style="text-align:right;">
57.6
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
2.9
</td>
<td style="text-align:right;">
38.7
</td>
<td style="text-align:right;">
3.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_lasso_oof
</td>
<td style="text-align:right;">
56.7
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
5.3
</td>
<td style="text-align:right;">
36.3
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
test_lasso
</td>
<td style="text-align:right;">
55.6
</td>
<td style="text-align:right;">
2.6
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
34.2
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
<tr>
<td style="text-align:left;">
train_enet_cv
</td>
<td style="text-align:right;">
57.6
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
3.8
</td>
<td style="text-align:right;">
37.8
</td>
<td style="text-align:right;">
4.6
</td>
</tr>
<tr>
<td style="text-align:left;">
train_enet_oof
</td>
<td style="text-align:right;">
57.1
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
5.9
</td>
<td style="text-align:right;">
35.7
</td>
<td style="text-align:right;">
7.2
</td>
</tr>
<tr>
<td style="text-align:left;">
test_enet
</td>
<td style="text-align:right;">
55.3
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
32.7
</td>
<td style="text-align:right;">
12.0
</td>
</tr>
<tr>
<td style="text-align:left;">
train_lassoR_cv
</td>
<td style="text-align:right;">
56.7
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
3.0
</td>
<td style="text-align:right;">
38.6
</td>
<td style="text-align:right;">
4.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_lassoR_oof
</td>
<td style="text-align:right;">
53.8
</td>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
6.4
</td>
<td style="text-align:right;">
35.2
</td>
<td style="text-align:right;">
11.0
</td>
</tr>
<tr>
<td style="text-align:left;">
test_lassoR
</td>
<td style="text-align:right;">
54.1
</td>
<td style="text-align:right;">
4.1
</td>
<td style="text-align:right;">
6.8
</td>
<td style="text-align:right;">
35.0
</td>
<td style="text-align:right;">
10.9
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_cv
</td>
<td style="text-align:right;">
57.1
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
38.2
</td>
<td style="text-align:right;">
4.7
</td>
</tr>
<tr>
<td style="text-align:left;">
train_blended_oof
</td>
<td style="text-align:right;">
54.3
</td>
<td style="text-align:right;">
4.1
</td>
<td style="text-align:right;">
6.2
</td>
<td style="text-align:right;">
35.4
</td>
<td style="text-align:right;">
10.3
</td>
</tr>
<tr>
<td style="text-align:left;">
test_blended
</td>
<td style="text-align:right;">
54.9
</td>
<td style="text-align:right;">
3.4
</td>
<td style="text-align:right;">
6.8
</td>
<td style="text-align:right;">
35.0
</td>
<td style="text-align:right;">
10.2
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="compare-predictions-at-misclassified-samples" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Compare predictions at misclassified samples</h2>
<p>It is useful to examine classification errors more carefully.
If models have different failure modes, one might get improved
performance by combining model predictions. Note that the models
considered here are not expected to compliment each other usefully
as they are too similar in nature.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="explore-sparsity.html#cb73-1"></a>misclass_id_vec &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">c</span>(</span>
<span id="cb73-2"><a href="explore-sparsity.html#cb73-2"></a> <span class="kw">names</span>(train_lasso_oofClass_vec)[train_lasso_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb73-3"><a href="explore-sparsity.html#cb73-3"></a> <span class="kw">names</span>(train_enet_oofClass_vec)[train_enet_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb73-4"><a href="explore-sparsity.html#cb73-4"></a> <span class="kw">names</span>(train_lassoR_oofClass_vec)[train_lassoR_oofClass_vec<span class="op">!=</span>train_group_vec],</span>
<span id="cb73-5"><a href="explore-sparsity.html#cb73-5"></a> <span class="kw">names</span>(train_blended_oofClass_vec)[train_blended_oofClass_vec<span class="op">!=</span>train_group_vec]</span>
<span id="cb73-6"><a href="explore-sparsity.html#cb73-6"></a> )</span>
<span id="cb73-7"><a href="explore-sparsity.html#cb73-7"></a>)</span>
<span id="cb73-8"><a href="explore-sparsity.html#cb73-8"></a></span>
<span id="cb73-9"><a href="explore-sparsity.html#cb73-9"></a></span>
<span id="cb73-10"><a href="explore-sparsity.html#cb73-10"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb73-11"><a href="explore-sparsity.html#cb73-11"></a> train_lasso_oofProb_vec[misclass_id_vec],</span>
<span id="cb73-12"><a href="explore-sparsity.html#cb73-12"></a> train_enet_oofProb_vec[misclass_id_vec],</span>
<span id="cb73-13"><a href="explore-sparsity.html#cb73-13"></a> train_lassoR_oofProb_vec[misclass_id_vec],</span>
<span id="cb73-14"><a href="explore-sparsity.html#cb73-14"></a> train_blended_oofProb_vec[misclass_id_vec]</span>
<span id="cb73-15"><a href="explore-sparsity.html#cb73-15"></a>)</span>
<span id="cb73-16"><a href="explore-sparsity.html#cb73-16"></a><span class="kw">colnames</span>(missclass_oofProb_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;lasso&#39;</span>,<span class="st">&#39;enet&#39;</span>, <span class="st">&#39;lassoR&#39;</span>, <span class="st">&#39;blended&#39;</span>)</span>
<span id="cb73-17"><a href="explore-sparsity.html#cb73-17"></a></span>
<span id="cb73-18"><a href="explore-sparsity.html#cb73-18"></a>row_med_vec &lt;-<span class="st"> </span><span class="kw">apply</span>(missclass_oofProb_mtx, <span class="dv">1</span>, median)</span>
<span id="cb73-19"><a href="explore-sparsity.html#cb73-19"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span>missclass_oofProb_mtx[</span>
<span id="cb73-20"><a href="explore-sparsity.html#cb73-20"></a>  <span class="kw">order</span>(train_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)], row_med_vec),]</span>
<span id="cb73-21"><a href="explore-sparsity.html#cb73-21"></a></span>
<span id="cb73-22"><a href="explore-sparsity.html#cb73-22"></a><span class="kw">plot</span>(</span>
<span id="cb73-23"><a href="explore-sparsity.html#cb73-23"></a> <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(missclass_oofProb_mtx)), <span class="dt">xlab=</span><span class="st">&#39;samples&#39;</span>,</span>
<span id="cb73-24"><a href="explore-sparsity.html#cb73-24"></a> <span class="dt">y=</span><span class="kw">range</span>(missclass_oofProb_mtx), <span class="dt">ylab=</span><span class="st">&#39;out-of-fold predicted probability&#39;</span>,</span>
<span id="cb73-25"><a href="explore-sparsity.html#cb73-25"></a> <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">type=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb73-26"><a href="explore-sparsity.html#cb73-26"></a></span>
<span id="cb73-27"><a href="explore-sparsity.html#cb73-27"></a><span class="cf">for</span>(RR <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(missclass_oofProb_mtx))</span>
<span id="cb73-28"><a href="explore-sparsity.html#cb73-28"></a><span class="kw">points</span>(</span>
<span id="cb73-29"><a href="explore-sparsity.html#cb73-29"></a> <span class="kw">rep</span>(RR, <span class="kw">ncol</span>(missclass_oofProb_mtx)), </span>
<span id="cb73-30"><a href="explore-sparsity.html#cb73-30"></a> missclass_oofProb_mtx[RR,],</span>
<span id="cb73-31"><a href="explore-sparsity.html#cb73-31"></a> <span class="dt">col=</span><span class="kw">ifelse</span>(train_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)[RR]] <span class="op">==</span><span class="st"> &#39;Control&#39;</span>,</span>
<span id="cb73-32"><a href="explore-sparsity.html#cb73-32"></a>  <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>),</span>
<span id="cb73-33"><a href="explore-sparsity.html#cb73-33"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(missclass_oofProb_mtx))</span>
<span id="cb73-34"><a href="explore-sparsity.html#cb73-34"></a></span>
<span id="cb73-35"><a href="explore-sparsity.html#cb73-35"></a><span class="kw">legend</span>(<span class="st">&#39;top&#39;</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(missclass_oofProb_mtx), </span>
<span id="cb73-36"><a href="explore-sparsity.html#cb73-36"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb73-37"><a href="explore-sparsity.html#cb73-37"></a></span>
<span id="cb73-38"><a href="explore-sparsity.html#cb73-38"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:misclassTrain"></span>
<img src="Static/figures/misclassTrain-1.png" alt="out-of-fold predicted probabilities at miscassified samples" width="768" />
<p class="caption">
Figure 4.8: out-of-fold predicted probabilities at miscassified samples
</p>
</div>
<p>As we’ve seen above, predictions from lassoR and the blended mix model
are basically dichotomous; 0 or 1. Samples have been order by group, and
median P(HCC) within group. For the Controls (green), predicted probabilities
less than 0.5 are considered correct here. For the HCC (red) samples,
predicted probabilities greater than 0.5 are considered correct here.</p>
<p>Now look at the same plot on the test data set.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="explore-sparsity.html#cb74-1"></a>misclass_id_vec &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">c</span>(</span>
<span id="cb74-2"><a href="explore-sparsity.html#cb74-2"></a> <span class="kw">names</span>(test_lasso_predClass_vec[,<span class="dv">1</span>])[test_lasso_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb74-3"><a href="explore-sparsity.html#cb74-3"></a> <span class="kw">names</span>(test_enet_predClass_vec[,<span class="dv">1</span>])[test_enet_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb74-4"><a href="explore-sparsity.html#cb74-4"></a> <span class="kw">names</span>(test_lassoR_predClass_vec[,<span class="dv">1</span>])[test_lassoR_predClass_vec<span class="op">!=</span>test_group_vec],</span>
<span id="cb74-5"><a href="explore-sparsity.html#cb74-5"></a> <span class="kw">names</span>(test_blended_predClass_vec[,<span class="dv">1</span>])[test_blended_predClass_vec<span class="op">!=</span>test_group_vec]</span>
<span id="cb74-6"><a href="explore-sparsity.html#cb74-6"></a> )</span>
<span id="cb74-7"><a href="explore-sparsity.html#cb74-7"></a>)</span>
<span id="cb74-8"><a href="explore-sparsity.html#cb74-8"></a></span>
<span id="cb74-9"><a href="explore-sparsity.html#cb74-9"></a></span>
<span id="cb74-10"><a href="explore-sparsity.html#cb74-10"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb74-11"><a href="explore-sparsity.html#cb74-11"></a> test_lasso_predProb_vec[misclass_id_vec,],</span>
<span id="cb74-12"><a href="explore-sparsity.html#cb74-12"></a> test_enet_predProb_vec[misclass_id_vec,],</span>
<span id="cb74-13"><a href="explore-sparsity.html#cb74-13"></a> test_lassoR_predProb_vec[misclass_id_vec,],</span>
<span id="cb74-14"><a href="explore-sparsity.html#cb74-14"></a> test_blended_predProb_vec[misclass_id_vec,]</span>
<span id="cb74-15"><a href="explore-sparsity.html#cb74-15"></a>)</span>
<span id="cb74-16"><a href="explore-sparsity.html#cb74-16"></a><span class="kw">colnames</span>(missclass_oofProb_mtx) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;lasso&#39;</span>,<span class="st">&#39;enet&#39;</span>, <span class="st">&#39;lassoR&#39;</span>, <span class="st">&#39;blended&#39;</span>)</span>
<span id="cb74-17"><a href="explore-sparsity.html#cb74-17"></a></span>
<span id="cb74-18"><a href="explore-sparsity.html#cb74-18"></a>row_med_vec &lt;-<span class="st"> </span><span class="kw">apply</span>(missclass_oofProb_mtx, <span class="dv">1</span>, median)</span>
<span id="cb74-19"><a href="explore-sparsity.html#cb74-19"></a>missclass_oofProb_mtx &lt;-<span class="st"> </span>missclass_oofProb_mtx[</span>
<span id="cb74-20"><a href="explore-sparsity.html#cb74-20"></a>  <span class="kw">order</span>(test_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)], row_med_vec),]</span>
<span id="cb74-21"><a href="explore-sparsity.html#cb74-21"></a></span>
<span id="cb74-22"><a href="explore-sparsity.html#cb74-22"></a><span class="kw">plot</span>(</span>
<span id="cb74-23"><a href="explore-sparsity.html#cb74-23"></a> <span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(missclass_oofProb_mtx)), <span class="dt">xlab=</span><span class="st">&#39;samples&#39;</span>,</span>
<span id="cb74-24"><a href="explore-sparsity.html#cb74-24"></a> <span class="dt">y=</span><span class="kw">range</span>(missclass_oofProb_mtx), <span class="dt">ylab=</span><span class="st">&#39;out-of-fold predicted probability&#39;</span>,</span>
<span id="cb74-25"><a href="explore-sparsity.html#cb74-25"></a> <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">type=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb74-26"><a href="explore-sparsity.html#cb74-26"></a></span>
<span id="cb74-27"><a href="explore-sparsity.html#cb74-27"></a><span class="cf">for</span>(RR <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(missclass_oofProb_mtx))</span>
<span id="cb74-28"><a href="explore-sparsity.html#cb74-28"></a><span class="kw">points</span>(</span>
<span id="cb74-29"><a href="explore-sparsity.html#cb74-29"></a> <span class="kw">rep</span>(RR, <span class="kw">ncol</span>(missclass_oofProb_mtx)), </span>
<span id="cb74-30"><a href="explore-sparsity.html#cb74-30"></a> missclass_oofProb_mtx[RR,],</span>
<span id="cb74-31"><a href="explore-sparsity.html#cb74-31"></a> <span class="dt">col=</span><span class="kw">ifelse</span>(test_group_vec[<span class="kw">rownames</span>(missclass_oofProb_mtx)[RR]] <span class="op">==</span><span class="st"> &#39;Control&#39;</span>,</span>
<span id="cb74-32"><a href="explore-sparsity.html#cb74-32"></a>  <span class="st">&#39;green&#39;</span>, <span class="st">&#39;red&#39;</span>),</span>
<span id="cb74-33"><a href="explore-sparsity.html#cb74-33"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(missclass_oofProb_mtx))</span>
<span id="cb74-34"><a href="explore-sparsity.html#cb74-34"></a></span>
<span id="cb74-35"><a href="explore-sparsity.html#cb74-35"></a><span class="kw">legend</span>(<span class="st">&#39;top&#39;</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">legend=</span><span class="kw">colnames</span>(missclass_oofProb_mtx), </span>
<span id="cb74-36"><a href="explore-sparsity.html#cb74-36"></a> <span class="dt">pch=</span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb74-37"><a href="explore-sparsity.html#cb74-37"></a></span>
<span id="cb74-38"><a href="explore-sparsity.html#cb74-38"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="fl">0.5</span>)</span></code></pre></div>
<div class="figure"><span id="fig:misclassTest"></span>
<img src="Static/figures/misclassTest-1.png" alt="Test data predicted probabilities at miscassified samples" width="768" />
<p class="caption">
Figure 4.9: Test data predicted probabilities at miscassified samples
</p>
</div>
</div>
<div id="compare-coefficient-profiles" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Compare coefficient profiles</h2>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="explore-sparsity.html#cb75-1"></a><span class="co"># lasso </span></span>
<span id="cb75-2"><a href="explore-sparsity.html#cb75-2"></a><span class="co">##########################</span></span>
<span id="cb75-3"><a href="explore-sparsity.html#cb75-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb75-4"><a href="explore-sparsity.html#cb75-4"></a>lasso_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb75-5"><a href="explore-sparsity.html#cb75-5"></a> cv_lasso,</span>
<span id="cb75-6"><a href="explore-sparsity.html#cb75-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb75-7"><a href="explore-sparsity.html#cb75-7"></a>)</span>
<span id="cb75-8"><a href="explore-sparsity.html#cb75-8"></a>lasso_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb75-9"><a href="explore-sparsity.html#cb75-9"></a> <span class="dt">gene=</span>lasso_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lasso_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb75-10"><a href="explore-sparsity.html#cb75-10"></a> <span class="dt">lasso=</span>lasso_coef<span class="op">@</span>x)</span>
<span id="cb75-11"><a href="explore-sparsity.html#cb75-11"></a></span>
<span id="cb75-12"><a href="explore-sparsity.html#cb75-12"></a></span>
<span id="cb75-13"><a href="explore-sparsity.html#cb75-13"></a><span class="co"># enet</span></span>
<span id="cb75-14"><a href="explore-sparsity.html#cb75-14"></a><span class="co">##########################</span></span>
<span id="cb75-15"><a href="explore-sparsity.html#cb75-15"></a>enet_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb75-16"><a href="explore-sparsity.html#cb75-16"></a> cv_enet,</span>
<span id="cb75-17"><a href="explore-sparsity.html#cb75-17"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb75-18"><a href="explore-sparsity.html#cb75-18"></a>)</span>
<span id="cb75-19"><a href="explore-sparsity.html#cb75-19"></a>enet_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb75-20"><a href="explore-sparsity.html#cb75-20"></a> <span class="dt">gene=</span>enet_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, enet_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb75-21"><a href="explore-sparsity.html#cb75-21"></a> <span class="dt">enet=</span>enet_coef<span class="op">@</span>x)</span>
<span id="cb75-22"><a href="explore-sparsity.html#cb75-22"></a></span>
<span id="cb75-23"><a href="explore-sparsity.html#cb75-23"></a><span class="co"># lasso - relaxed (gamma=0)</span></span>
<span id="cb75-24"><a href="explore-sparsity.html#cb75-24"></a><span class="co">##########################</span></span>
<span id="cb75-25"><a href="explore-sparsity.html#cb75-25"></a>lassoR_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb75-26"><a href="explore-sparsity.html#cb75-26"></a> cv_lassoR,</span>
<span id="cb75-27"><a href="explore-sparsity.html#cb75-27"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb75-28"><a href="explore-sparsity.html#cb75-28"></a> <span class="dt">g=</span><span class="dv">0</span></span>
<span id="cb75-29"><a href="explore-sparsity.html#cb75-29"></a>)</span>
<span id="cb75-30"><a href="explore-sparsity.html#cb75-30"></a>lassoR_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb75-31"><a href="explore-sparsity.html#cb75-31"></a> <span class="dt">gene=</span>lassoR_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lassoR_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb75-32"><a href="explore-sparsity.html#cb75-32"></a> <span class="dt">lassoR=</span>lassoR_coef<span class="op">@</span>x)</span>
<span id="cb75-33"><a href="explore-sparsity.html#cb75-33"></a></span>
<span id="cb75-34"><a href="explore-sparsity.html#cb75-34"></a></span>
<span id="cb75-35"><a href="explore-sparsity.html#cb75-35"></a><span class="co"># blended mix (gamma=0.5)</span></span>
<span id="cb75-36"><a href="explore-sparsity.html#cb75-36"></a><span class="co">###############################</span></span>
<span id="cb75-37"><a href="explore-sparsity.html#cb75-37"></a>blended_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb75-38"><a href="explore-sparsity.html#cb75-38"></a> cv_lassoR,</span>
<span id="cb75-39"><a href="explore-sparsity.html#cb75-39"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span>,</span>
<span id="cb75-40"><a href="explore-sparsity.html#cb75-40"></a> <span class="dt">g=</span><span class="fl">0.5</span></span>
<span id="cb75-41"><a href="explore-sparsity.html#cb75-41"></a>)</span>
<span id="cb75-42"><a href="explore-sparsity.html#cb75-42"></a>blended_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb75-43"><a href="explore-sparsity.html#cb75-43"></a> <span class="dt">gene=</span>blended_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, blended_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb75-44"><a href="explore-sparsity.html#cb75-44"></a> <span class="dt">blended=</span>blended_coef<span class="op">@</span>x)</span>
<span id="cb75-45"><a href="explore-sparsity.html#cb75-45"></a></span>
<span id="cb75-46"><a href="explore-sparsity.html#cb75-46"></a></span>
<span id="cb75-47"><a href="explore-sparsity.html#cb75-47"></a><span class="co"># put it all together</span></span>
<span id="cb75-48"><a href="explore-sparsity.html#cb75-48"></a>all_coef_frm &lt;-<span class="st"> </span></span>
<span id="cb75-49"><a href="explore-sparsity.html#cb75-49"></a><span class="st"> </span>base<span class="op">::</span><span class="kw">merge</span>(</span>
<span id="cb75-50"><a href="explore-sparsity.html#cb75-50"></a> <span class="dt">x =</span> lasso_coef_frm, </span>
<span id="cb75-51"><a href="explore-sparsity.html#cb75-51"></a> <span class="dt">y =</span> base<span class="op">::</span><span class="kw">merge</span>(</span>
<span id="cb75-52"><a href="explore-sparsity.html#cb75-52"></a>     <span class="dt">x =</span> enet_coef_frm,</span>
<span id="cb75-53"><a href="explore-sparsity.html#cb75-53"></a>     <span class="dt">y =</span> base<span class="op">::</span><span class="kw">merge</span>(</span>
<span id="cb75-54"><a href="explore-sparsity.html#cb75-54"></a>         <span class="dt">x =</span> lassoR_coef_frm,</span>
<span id="cb75-55"><a href="explore-sparsity.html#cb75-55"></a>         <span class="dt">y =</span> blended_coef_frm,</span>
<span id="cb75-56"><a href="explore-sparsity.html#cb75-56"></a>         <span class="dt">by=</span><span class="st">&#39;gene&#39;</span>, <span class="dt">all=</span>T),</span>
<span id="cb75-57"><a href="explore-sparsity.html#cb75-57"></a>     <span class="dt">by=</span><span class="st">&#39;gene&#39;</span>, <span class="dt">all=</span>T),</span>
<span id="cb75-58"><a href="explore-sparsity.html#cb75-58"></a> <span class="dt">by=</span><span class="st">&#39;gene&#39;</span>, <span class="dt">all=</span>T)</span>
<span id="cb75-59"><a href="explore-sparsity.html#cb75-59"></a></span>
<span id="cb75-60"><a href="explore-sparsity.html#cb75-60"></a>all_coef_frm[,<span class="op">-</span><span class="dv">1</span>][<span class="kw">is.na</span>(all_coef_frm[,<span class="op">-</span><span class="dv">1</span>])] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb75-61"><a href="explore-sparsity.html#cb75-61"></a></span>
<span id="cb75-62"><a href="explore-sparsity.html#cb75-62"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="kw">ncol</span>(all_coef_frm)<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb75-63"><a href="explore-sparsity.html#cb75-63"></a></span>
<span id="cb75-64"><a href="explore-sparsity.html#cb75-64"></a><span class="cf">for</span>(CC <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="kw">ncol</span>(all_coef_frm)) {</span>
<span id="cb75-65"><a href="explore-sparsity.html#cb75-65"></a> <span class="kw">plot</span>(</span>
<span id="cb75-66"><a href="explore-sparsity.html#cb75-66"></a>  <span class="dt">x=</span><span class="dv">1</span><span class="op">:</span>(<span class="kw">nrow</span>(all_coef_frm)<span class="op">-</span><span class="dv">1</span>), <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>, </span>
<span id="cb75-67"><a href="explore-sparsity.html#cb75-67"></a>  <span class="dt">y=</span>all_coef_frm[<span class="op">-</span><span class="dv">1</span>, CC], <span class="dt">ylab=</span><span class="kw">colnames</span>(all_coef_frm)[CC],</span>
<span id="cb75-68"><a href="explore-sparsity.html#cb75-68"></a>  <span class="dt">type=</span><span class="st">&#39;h&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>)</span>
<span id="cb75-69"><a href="explore-sparsity.html#cb75-69"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:compCoeffProf"></span>
<img src="Static/figures/compCoeffProf-1.png" alt="Coefficient Profiles" width="768" />
<p class="caption">
Figure 4.10: Coefficient Profiles
</p>
</div>
<p>Coefficients in the relaxed lasso fit are much larger than those in the
lasso fit, or zero. As a consequence, the blended fit coefficients look
like a shrunken version of the relaxed lasso fit coefficients.</p>
<p>We can also examine these with a scatter plot matrix.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="explore-sparsity.html#cb76-1"></a><span class="kw">pairs</span>(all_coef_frm[<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb76-2"><a href="explore-sparsity.html#cb76-2"></a>  <span class="dt">lower.panel =</span> <span class="ot">NULL</span>,</span>
<span id="cb76-3"><a href="explore-sparsity.html#cb76-3"></a>  <span class="dt">panel =</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb76-4"><a href="explore-sparsity.html#cb76-4"></a>    <span class="kw">points</span>(x, y, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb76-5"><a href="explore-sparsity.html#cb76-5"></a>  }</span>
<span id="cb76-6"><a href="explore-sparsity.html#cb76-6"></a>)</span></code></pre></div>
<div class="figure"><span id="fig:pairsCoeffProf"></span>
<img src="Static/figures/pairsCoeffProf-1.png" alt="Coefficients from fits" width="768" />
<p class="caption">
Figure 4.11: Coefficients from fits
</p>
</div>
</div>
<div id="examine-feature-selection" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Examine feature selection</h2>
<p>Recall from <code>glmnet</code> vignette:</p>
<pre><code>It is known that the ridge penalty shrinks the coefficients of correlated predictors
towards each other while the lasso tends to pick one of them and discard the others.
The elastic-net penalty mixes these two; if predictors are correlated in groups,
an $\alpha$=0.5 tends to select the groups in or out together.
This is a higher level parameter, and users might pick a value upfront,
else experiment with a few different values. One use of $\alpha$ is for numerical stability;
for example, the *elastic net with $\alpha = 1 - \epsilon$ for some small $\epsilon$&gt;0
performs much like the lasso, but removes any degeneracies and wild behavior caused
by extreme correlations*.</code></pre>
<p>To see how this plays out in this dataset, we can look at feature expression
heat maps.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="explore-sparsity.html#cb78-1"></a> <span class="kw">suppressPackageStartupMessages</span>(<span class="kw">require</span>(gplots))</span>
<span id="cb78-2"><a href="explore-sparsity.html#cb78-2"></a></span>
<span id="cb78-3"><a href="explore-sparsity.html#cb78-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb78-4"><a href="explore-sparsity.html#cb78-4"></a>lasso_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb78-5"><a href="explore-sparsity.html#cb78-5"></a> cv_lasso,</span>
<span id="cb78-6"><a href="explore-sparsity.html#cb78-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb78-7"><a href="explore-sparsity.html#cb78-7"></a>)</span>
<span id="cb78-8"><a href="explore-sparsity.html#cb78-8"></a>lasso_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb78-9"><a href="explore-sparsity.html#cb78-9"></a> <span class="dt">gene=</span>lasso_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, lasso_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb78-10"><a href="explore-sparsity.html#cb78-10"></a> <span class="dt">lasso=</span>lasso_coef<span class="op">@</span>x)</span>
<span id="cb78-11"><a href="explore-sparsity.html#cb78-11"></a></span>
<span id="cb78-12"><a href="explore-sparsity.html#cb78-12"></a> </span>
<span id="cb78-13"><a href="explore-sparsity.html#cb78-13"></a>  Mycol &lt;-<span class="st"> </span><span class="kw">colorpanel</span>(<span class="dv">1000</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb78-14"><a href="explore-sparsity.html#cb78-14"></a>  <span class="kw">heatmap.2</span>(</span>
<span id="cb78-15"><a href="explore-sparsity.html#cb78-15"></a>    <span class="dt">x=</span><span class="kw">t</span>(train_lcpm_mtx[,lasso_coef_frm<span class="op">$</span>gene[<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb78-16"><a href="explore-sparsity.html#cb78-16"></a>    <span class="dt">scale=</span><span class="st">&quot;row&quot;</span>,</span>
<span id="cb78-17"><a href="explore-sparsity.html#cb78-17"></a>    <span class="dt">labRow=</span>lasso_coef_frm<span class="op">$</span>gene,</span>
<span id="cb78-18"><a href="explore-sparsity.html#cb78-18"></a>    <span class="dt">labCol=</span>train_group_vec,</span>
<span id="cb78-19"><a href="explore-sparsity.html#cb78-19"></a>    <span class="dt">col=</span>Mycol, </span>
<span id="cb78-20"><a href="explore-sparsity.html#cb78-20"></a>    <span class="dt">trace=</span><span class="st">&quot;none&quot;</span>, <span class="dt">density.info=</span><span class="st">&quot;none&quot;</span>, </span>
<span id="cb78-21"><a href="explore-sparsity.html#cb78-21"></a>    <span class="co">#margin=c(8,6), lhei=c(2,10), </span></span>
<span id="cb78-22"><a href="explore-sparsity.html#cb78-22"></a>    <span class="co">#lwid=c(0.1,4), #lhei=c(0.1,4)</span></span>
<span id="cb78-23"><a href="explore-sparsity.html#cb78-23"></a>    <span class="dt">key=</span>F,</span>
<span id="cb78-24"><a href="explore-sparsity.html#cb78-24"></a>    <span class="dt">ColSideColors=</span><span class="kw">ifelse</span>(train_group_vec<span class="op">==</span><span class="st">&#39;Control&#39;</span>, <span class="st">&#39;green&#39;</span>,<span class="st">&#39;red&#39;</span>),</span>
<span id="cb78-25"><a href="explore-sparsity.html#cb78-25"></a>    <span class="dt">dendrogram=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb78-26"><a href="explore-sparsity.html#cb78-26"></a>    <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;lasso genes - N =&#39;</span>, <span class="kw">nrow</span>(lasso_coef_frm)<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:heatmapLasso"></span>
<img src="Static/figures/heatmapLasso-1.png" alt="Lasso Model Genes" width="768" />
<p class="caption">
Figure 4.12: Lasso Model Genes
</p>
</div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="explore-sparsity.html#cb79-1"></a> <span class="kw">suppressPackageStartupMessages</span>(<span class="kw">require</span>(gplots))</span>
<span id="cb79-2"><a href="explore-sparsity.html#cb79-2"></a></span>
<span id="cb79-3"><a href="explore-sparsity.html#cb79-3"></a><span class="co"># train - cv predicted</span></span>
<span id="cb79-4"><a href="explore-sparsity.html#cb79-4"></a>enet_coef &lt;-<span class="st"> </span><span class="kw">coef</span>(</span>
<span id="cb79-5"><a href="explore-sparsity.html#cb79-5"></a> cv_enet,</span>
<span id="cb79-6"><a href="explore-sparsity.html#cb79-6"></a> <span class="dt">s=</span><span class="st">&#39;lambda.1se&#39;</span></span>
<span id="cb79-7"><a href="explore-sparsity.html#cb79-7"></a>)</span>
<span id="cb79-8"><a href="explore-sparsity.html#cb79-8"></a>enet_coef_frm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb79-9"><a href="explore-sparsity.html#cb79-9"></a> <span class="dt">gene=</span>enet_coef<span class="op">@</span>Dimnames[[<span class="dv">1</span>]][<span class="kw">c</span>(<span class="dv">1</span>, enet_coef<span class="op">@</span>i[<span class="op">-</span><span class="dv">1</span>])],</span>
<span id="cb79-10"><a href="explore-sparsity.html#cb79-10"></a> <span class="dt">enet=</span>enet_coef<span class="op">@</span>x)</span>
<span id="cb79-11"><a href="explore-sparsity.html#cb79-11"></a></span>
<span id="cb79-12"><a href="explore-sparsity.html#cb79-12"></a> </span>
<span id="cb79-13"><a href="explore-sparsity.html#cb79-13"></a>  Mycol &lt;-<span class="st"> </span><span class="kw">colorpanel</span>(<span class="dv">1000</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb79-14"><a href="explore-sparsity.html#cb79-14"></a>  <span class="kw">heatmap.2</span>(</span>
<span id="cb79-15"><a href="explore-sparsity.html#cb79-15"></a>    <span class="dt">x=</span><span class="kw">t</span>(train_lcpm_mtx[,enet_coef_frm<span class="op">$</span>gene[<span class="op">-</span><span class="dv">1</span>]]),</span>
<span id="cb79-16"><a href="explore-sparsity.html#cb79-16"></a>    <span class="dt">scale=</span><span class="st">&quot;row&quot;</span>,</span>
<span id="cb79-17"><a href="explore-sparsity.html#cb79-17"></a>    <span class="dt">labRow=</span>enet_coef_frm<span class="op">$</span>gene,</span>
<span id="cb79-18"><a href="explore-sparsity.html#cb79-18"></a>    <span class="dt">labCol=</span>train_group_vec,</span>
<span id="cb79-19"><a href="explore-sparsity.html#cb79-19"></a>    <span class="dt">col=</span>Mycol, </span>
<span id="cb79-20"><a href="explore-sparsity.html#cb79-20"></a>    <span class="dt">trace=</span><span class="st">&quot;none&quot;</span>, <span class="dt">density.info=</span><span class="st">&quot;none&quot;</span>, </span>
<span id="cb79-21"><a href="explore-sparsity.html#cb79-21"></a>    <span class="co">#margin=c(8,6), lhei=c(2,10), </span></span>
<span id="cb79-22"><a href="explore-sparsity.html#cb79-22"></a>    <span class="co">#lwid=c(0.1,4), #lhei=c(0.1,4)</span></span>
<span id="cb79-23"><a href="explore-sparsity.html#cb79-23"></a>    <span class="dt">key=</span>F,</span>
<span id="cb79-24"><a href="explore-sparsity.html#cb79-24"></a>    <span class="dt">ColSideColors=</span><span class="kw">ifelse</span>(train_group_vec<span class="op">==</span><span class="st">&#39;Control&#39;</span>, <span class="st">&#39;green&#39;</span>,<span class="st">&#39;red&#39;</span>),</span>
<span id="cb79-25"><a href="explore-sparsity.html#cb79-25"></a>    <span class="dt">dendrogram=</span><span class="st">&quot;both&quot;</span>,</span>
<span id="cb79-26"><a href="explore-sparsity.html#cb79-26"></a>    <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;enet genes - N =&#39;</span>, <span class="kw">nrow</span>(enet_coef_frm)<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<div class="figure"><span id="fig:heatmapEnet"></span>
<img src="Static/figures/heatmapEnet-1.png" alt="Enet Model Genes" width="768" />
<p class="caption">
Figure 4.13: Enet Model Genes
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-background.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-suite.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["HCC5hmCExplore.pdf", "HCC5hmCExplore.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
