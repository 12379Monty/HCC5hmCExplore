[
["index.html", "DNA Hydroxymethylation in Hepatocellular Carcinoma Preamble License", " DNA Hydroxymethylation in Hepatocellular Carcinoma Francois Collin 2020-09-01 Preamble This vignette offers some exploratory data analyses of data available from the NCBI GEO web site. License This work by Francois Collin is licensed under a Creative Commons Attribution 4.0 International License "],
["intro.html", "Section 1 Introduction", " Section 1 Introduction The goal of detecting cancer at the earliest stage of development with a non-invasive procedure has busied many groups with the task of perfecting techniques to support what has become commonly known as a liquid biopsy - the analysis of biomarkers circulating in fluids such as blood, saliva or urine. Epigenetic biomarkers present themselves as good candidates for this application (Gai and Sun (2019) [1]). In particular, given their prevalence in the human genome, close correlation with gene expression and high chemical stability, DNA modifications such as 5-methylcytosine (5mC) and 5-hydroxymethylcytosine (5hmC) are DNA epigenetic marks that provide much promise as cancer diagnosis biomarkers that could be profitably analyzed in liquid biopsies [2–5]. Li et al. (2017) [3] used a sensitive and selective chemical labeling technology to extract genome-wide 5hmC profiles from circulating cell-free DNA (cfDNA) as well as from genomic DNA (gDNA) collected from a cohort of 260 patients recently diagnosed with colorectal, gastric, pancreatic, liver or thyroid cancer and normal tissues from 90 healthy individuals They found 5hmC-based biomarkers of circulating cfDNA to be highly predictive of some cancer types. Similar small sample size findings were reported in Song et al. (2017) [4]. Focusing on hepatocellular carcinoma, Cai et al. (2019) [2] assembled a sizable dataset to demonstrate the feasibility of using features derived from 5-hydroxymethylcytosines marks in circulating cell-free DNA as a non-invasive approach for the early detection of hepatocellular carcinoma. The data that are the basis of that report are available on the NCBI GEO web site (Series GSE112679). The data have also been bundled in a R data package which can be installed from github: if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;12379Monty/GSE112679&quot;) An important question in the early development of classifiers of the sorts that are the basis of any liquid biopsy diagnostic tool is how many samples should be collected to make properly informed decisions. In this report we will explore the GSE112679 data to shed some light on the relationship between sample size and model performance in the context classifying samples based on 5hmC data. In Section 2 we preprocess the data that we will use for the classification analysis and perform some light QC analyses. In Section 3 we provide some background to our modeling approach. In Section ?? we explore some losso fits that discriminate between early stage HCC and control samples. In Section 5 we examine the results of fitting a suite of models to investigate the effect of sample size on model performance. "],
["preproc.html", "Section 2 Preprocessing 2.1 Load the data 2.2 Differential representation analysis 2.3 Analysis of coverage variability", " Section 2 Preprocessing 2.1 Load the data The data that are available from NCBI GEO Series GSE112679 can be conveniently accessed through an R data package. Attaching the GSE112679 package makes the count data tables available as well as a gene annotation table and a sample description table. See GSE112679 R Data Package page. For the Cai et al. [2] model fitting and analysis, samples were separated into Train and Val-1 subsets. Val-2 was an external validation set. if (!(&quot;GSE112679&quot; %in% rownames(installed.packages()))) { if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;12379Monty/GSE112679&quot;) } library(GSE112679) sampDesc$DxStage &lt;- with(sampDesc, ifelse(outcome==&#39;HCC&#39;, paste0(outcome,&#39;:&#39;,stage), outcome)) with( sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot;), knitr::kable(table(DxStage, trainValGroup, exclude = NULL), caption=&quot;GSE112679 Samples by Dx Group and Subset&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) ## Warning in kableExtra::kable_styling(., full_width = F): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Table 2.1: GSE112679 Samples by Dx Group and Subset Train Val-1 Val-2 Benign 253 132 3 CHB 190 96 0 Cirrhosis 73 33 0 HCC:Early 335 220 24 HCC:Late 0 442 13 HCC:NA 0 147 23 Healthy 269 124 177 For this analysis, we will consider early stage cancer samples and healthy or benign samples from the Train or Val-1 subsets. The appropriate outcome variable will be renamed or aliased group Table 2.2: Samples used in this analysis group Freq Control 778 HCC 555 The features are counts of reads captured by chemical labeling, and indicate the level of 5-hydroxymethylcytosines within each gene body. Cai et al. (2019), Li et al. (2017) and Song et al. (2017) [2–4] all analyze 5hmC gene body counts using standard RNA-Seq methodologies, and we will do the same here. Note that before conducting any substantive analyses, the data would normally be very carefully examined for any sign of quality variation between groups of samples. This analysis would integrate sample meta data - where and when were the blood samples collected - as well as library preparation and sequencing metrics in order to detect any sign of processing artifacts that may be present in the dataset. This is particularly important when dealing with blood samples as variable DNA quality degradation is a well known challenge that is encountered when dealing with such samples [6]. Although blood specimen handling protocols can be put in place to minimize quality variation [7], variability can never be completely eradicated, especially in the context of blood samples collected by different groups, working in different environments. The problem of variable DNA quality becomes paricularly pernicuous when it is compounded with a confounding factor that sneaks in when the control sample collection events are separated in time and space from the cancer sample collection events; an all too common occurence. As proper data QC requires an intimate familiarity with the details of data collection and processing, such a task cannot be untertaken here. We will simply run a minimal set of QC sanity checks to make sure that there are no apparent systematic effects in the data. We first look at coverage - make sure there isn’t too much disparity of coverage across samples. To detect shared variability, samples can be annotated and ordered according to sample features that may be linked to sample batch processing. Here we the samples have been ordered by group and sample id (an alias of geoAcc). Figure 2.1: Sample log2 count boxplots Table 2.3: Coverage Summary - Columns are sample coverage quantiles and total coverage Rows are quartiles across samples 15% 25% 50% 75% totCovM 25% 4 24 111 321 5.5 50% 5 30 135 391 6.7 75% 6 35 162 468 8.0 From this table, we see that 25% of the samples have total coverage exceeding 8M reads, 25% of samples have a 15 percentile of coverage lower than 4, etc. 2.2 Differential representation analysis In the remainder of this section, we will process the data and perform differential expression analysis as outlined in Law et al. (2018) [8]. The main analysis steps are: remove lowly expressed genes normalize gene expression distributions remove heteroscedascity fit linear models and examine DE results It is good practice to perform this differential expression analysis prior to fitting models to get an idea of how difficult it will be to discriminate between samples belonging to the different subgroups. The pipeline outlined in Law et al. (2018) [8] also provides some basic quality assessment opportunities. Remove lowly expressed genes Genes that are not expressed at a biologically meaningful level in any condition should be discarded to reduce the subset of genes to those that are of interest, and to reduce the number of tests carried out downstream when looking at differential expression. Carrying un-informative genes may also be a hindrance to classification and other downtream analyses. To determine a sensible threshold we can begin by examining the shapes of the distributions. Figure 2.2: Sample \\(log_2\\) CPM densities As is typically the case with RNA-Seq data, we notice many weakly represented genes in this dataset. A cpm value of 1 appears to adequatly separate the expressed from the un-expressed genes, but we will be slightly more strict here and require a CPM threshold of 3 . Using a nominal CPM value of 3, genes are deeemed to be represented if their expression is above this threshold, and not represented otherwise. For this analysis we will require that genes be represented in at least 25 samples across the entire dataset to be retained for downstream analysis. Here, a CPM value of 3 means that a gene is represented if it has at least 9 reads in the sample with the lowest sequencing depth (library size 2.9 million). Note that the thresholds used here are arbitrary as there are no hard and fast rules to set these by. The voom-plot, which is part of analyses done to remove heteroscedasticity, can be examined to verify that the filtering performed is adequate. Remove weakly represented genes and replot densities. Removing 17.5% of genes… Figure 2.3: Sample \\(log_2\\) CPM densities after removing weak genes As another sanity check, we will look at a multidimensional scaling plot of distances between gene expression profiles. We use plotMDS in limma package [9]), which plots samples on a two-dimensional scatterplot so that distances on the plot approximate the typical log2 fold changes between the samples. Before producing the MDS plot we will normalize the distributions. We will store the data into s DGEList object as this is convenient when running many of the analyses implemented in the edgeR and limma packages. filteredCountsAF_dgel &lt;- edgeR::DGEList( counts = featureCountsAF, genes = genes_annotAF, samples = sampDescA, group = sampDescA$group ) filteredCountsAF_dgel &lt;- edgeR::calcNormFactors(filteredCountsAF_dgel) filteredCountsAF_lcmp_mtx &lt;- edgeR::cpm(filteredCountsAF_dgel, log = T) # Save filteredCountsAF_dgel to facilitate restarting # remove from final version save(list = &quot;filteredCountsAF_dgel&quot;, file = &quot;RData/filteredCountsAF_dgel&quot;) Verify that the counts are properly normalized. Figure 2.4: Sample log2 count boxplots Proceed with MDS plots. Figure 2.5: MDS plots of log-CPM values The MDS plot, which is analogous to a PCA plot adapted to gene exression data, does not indicate strong clustering of samples. The fanning pattern observed in the first two dimensions indicates that a few samples are drifting way from the core set, but in no particular direction. There is some structure in the 3rd and 4th dimension plot which should be investigated. glMDSPlot from package Glimma provides an interactive MDS plot that can extremely usful for exploration Link to glMDSPlot: Here No obvious factor links the samples in the 3 clusters observed on the 4th MDS dimensions. The percent of variance exaplained by this dimension or \\(\\approx\\) 4%. The glMDSPlot indicates further segregation along the 6th dimension. The percent of variance exaplained by this dimension or \\(\\approx\\) 2%. Tracking down this source of variability may be quite challenging, especially without having the complete information about the sample attributes and provenance. Unwanted variability is a well-documented problem in the analysis of RNA-Seq data (see Peixoto et al. (2015) [10]), and many procedures have been proposed to reduce the effect of unwanted variation on RNA-Seq analsys results ([10–12]). There are undoubtedly some similar sources of systematic variation in the 5hmC data, but it is beyond the scope of this work to investigate these in this particular dataset. Given that the clustering of samples occurs in MDS dimensions that explain a small fraction of variability, and that these is no assocation with the factor of interest, HCC vs Control, these sources of variability should not interfere too much with our classification analysis. It would nonetheless be interesting to assess whether downstream results can be improved by removing this variability. Creating a design matrix and contrasts Before proceeding with the statistical modeling used for the differential expression analysis, we need to set up a model design matrix. ## colSums(Design_mtx): ## Control HCC ## 778 555 ## Contrasts: ## Contrasts ## Levels HCCvsControl ## Control -1 ## HCC 1 Removing heteroscedasticity from the count data As for RNA-Seq data, for 5hmC count data the variance is not independent of the mean. In limma, the R package we are using for our analyses, linear modeling is carried out on the log-CPM values which are assumed to be normally distributed and the mean-variance relationship is accommodated using precision weights calculated by the voom function. We apply this transformation next. par(mfrow=c(1,1)) filteredCountsAF_voom &lt;- limma::voom(filteredCountsAF_dgel, Design_mtx, plot=T) Figure 2.6: Removing heteroscedascity Note that the voom-plot provides a visual check on the level of filtering performed upstream. If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be observed at the low end of the expression scale due to very small counts. We observe that the variability in the 5hmC data is quite a bit lower. Statistical summaries would give a better idea. –&gt; Fit linear models and examine the results Having properly filtered and normalized the data, the linear models can be fitted to each gene and the results examined to assess differential expression between the two groups of interest, in our case HCC vs Control. Table 2.4 displays the counts of genes in each DE category: Table 2.4: DE Results at FDR = 0.05 HCCvsControl Down 5214 NotSig 5280 Up 5258 Graphical representations of differential expression results: MD Plots To summarise results for all genes visually, mean-difference plots (aka MA plot), which display log-FCs from the linear model fit against the average log-CPM values can be generated using the plotMD function, with the differentially expressed genes highlighted. We may also be interested in whether certain gene features are related to gene identification. Gene GC content, for example, might be of interest. Figure 2.7: HCC vs Control - Identified Genes at FDR = 0,05 Table 2.5: log FC quartiles by gene identification down notDE up 25% -0.07 -0.01 0.04 50% -0.05 0.00 0.06 75% -0.03 0.01 0.09 We see that while many genes are identified, the effect sizes are quite small. The GC content of down regulated genes tends to be slightly lower than the rest of the genes. 2.2.1 Number of DE genes at 10% fold change For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The treat method (McCarthy and Smyth 2009 [13]) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement. The number of differentially expressed genes are greatly reduced if we impose a minimal fold-change requirement of 10%. ## 10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05: ## HCCvsControl ## Down 3 ## NotSig 15550 ## Up 199 Figure 2.8: HCC vs Control - Identified Genes at FDR = 0,05 and logFC &gt; 10% 2.3 Analysis of coverage variability We will use the methods described in Hart et al. (2013) [14] to characterize coverage variability in these data. These methods do not take multiple comparisons into account. Other tools for sample size calculation in RNA-Seq studies include Bi and Liu (2016) [15], Baccarella (2018) [16], Guo (2014) [17], Yu (2017) [18], and Zhao (2018) [19]. Poplawski (2018) [20] evaluated RNA-seq sample size tools identified from a systematic search. They found the six evaluated tools provided widely different answers, which were strongly affected by fold change. The references listed above aim at providing guidance for RNA-Seq experimental design. There is much discussion and a wide range of opinion on sample size requirements to ensure reproducibility in RNA-Seq results. At one end of the spectrum, Ein-Dor et al. (2006) [21] argue that thousands of samples are needed to generate a robust gene list for predicting outcome in cancer. At the other end, Dobbin et al. (2007, 2008) [22,23] claim that sample sizes in the range of 20–30 per class may be adequate for building a good predictor in many cases. Part of the disparity in sample size requirement recomendation comes from differences of opinion in terms of what constitutes reproducible results. In the context of sample classification, if we focus on the predicted probabilities for individual samples, we may find good reproducibility across studies with moderate samples sizes. If, on the other hand, we closely inspect the gene signatures reported across studies, much greater sample sizes may be required to achieve concordance. Kim (2009) [24], like Ein-Dor et al., also find issues in RNA-Seq research in terms of the instability of identified prognostic gene signatures, few overlap between independently developed prognostic gene signatures, and poor inter-study applicability of gene signatures. Fan et al. (2006) [25], on the other hand, found good concordance among gene-expression–based predictors for breast cancer. We will return to this question when we examine the relationship between classification model results and sample size in this dataset later on this paper. For two groups comparisons, the basic formula for the required number of samples per group is: \\[ n = 2(z_{1-\\alpha/2} + z_{\\beta})^2 \\frac{(1/\\mu + \\sigma^2)}{ln(\\Delta^2)} \\] The parameters \\(\\alpha\\) and \\(\\beta\\) are size and power of the test. \\(\\Delta\\) is the targeted effect size. \\(\\mu\\) and \\(\\sigma\\) are the mean and coefficient of variation of the distribution of measurement, gene representation indices in this case. These three parameters will be fixed across genes or a given study, and are often dictated by external requirements. Typical values might be an effect size of \\(\\Delta = 1.5\\) (a.k.a fold change), corresponding to detection of a 50% change in gene expression between the two groups. \\(z_{1 - .05/2} = 1.96\\), corresponding to a two sided test at size \\(\\alpha = 0.05\\); and \\(z_{.90}= 1.28\\) corresponding to 90% power. The other two variables will be gene and experiment dependent: the normalized depth of coverage \\(\\mu\\) of the gene, and the coefficient of variation \\(\\sigma\\) in this gene between biological replicates. The technical variation of the comparison is inversely proportional to the number of sequenced reads for the gene and therefore decreases with sequencing depth. The biological variation is a property of the particular gene/model system/condition under study. One would expect it to be smaller for uniform systems such as cell culture and/or products that are under tight regulatory control, and larger for less uniform replicates such as human subject samples. The dataset under study in this report is the first of its kind to give us an idea of variability levels in 5hmC representation. As in Hart et al. (2013) [14] we estimate the biological coefficient of variation (CV) in expression across samples in the data set using a negative binomial model. Figure 2.9: Cumulative Distribution of CV - rug = 90th percentile BCV values are quite low: Figure 2.10: BCV distributions We can now look at sample size estimates to required to detect various effect sizes. The effect sizes examined here are selected based on the differential representation analysis in Section 2.2 below. Figure 2.11: Sample Size Estimates For the filtered reads, coverage looks like this: Table 2.6: Coverage Summary - Columns are sample coverage quantiles and total coverage Rows are quartiles across samples 15% 25% 50% 75% 25% 37 63 163 396 50% 45 76 198 483 75% 54 91 237 577 From this table, we see that 25% of the samples have an upper quartile of gene coverage exceeding 577 reads, 25% of samples have a 15 percentile of coverage lower than 37, etc. Note that with these data, moderate sample sizes are adequate to detect genes with effect sizes as small as 1.05 (5% fold change). This is due to the fact that the biological variability in gene body 5hmC density is quite low. For human samples, RNA-Seq within group biological variability is typically in the 0.4-1.0 range [14]. Another look at BCV Alternatively, we can extract sigma and signal from the fit objects. lib.size &lt;- colSums(filteredCountsAF_dgel$counts) fit &lt;- filteredCountsAF_voom_efit sx &lt;- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06) sy &lt;- sqrt(fit$sigma) CV &lt;- sy/sx cor(cbind(BCV_mtx, CV)) ## Control HCC CV ## Control 1.0000000 0.9591234 0.7597385 ## HCC 0.9591234 1.0000000 0.7687380 ## CV 0.7597385 0.7687380 1.0000000 boxplot(cbind(BCV_mtx, CV), outline=F) Figure 2.12: Alternative CV Calculation CV computed frm sigma and coverage extracted from the fit are slighly higher. For purposes of gene identification, we really want to look at effect size in relation to residual standard deviation. Figure 2.13: Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile Table 2.7: SNR Summary x 25% 0.018 50% 0.036 75% 0.060 90% 0.082 "],
["modeling-background.html", "Section 3 Modeling - Background 3.1 Predictive modeling for genomic data 3.2 glmnet", " Section 3 Modeling - Background Refer to first pass study for background. In the section we look at some models fitted to discriminate between early stage HCC and control samples comprising of healthy and benign samples from the GSE112679 data set. Baseline model how separable are the data: what accuracy do we expect individual sample quality scores: which samples are hard to classify? 3.1 Predictive modeling for genomic data The main problem in calibrating predictive models to genomic data is that there are way more features than there are example cases to fit to. As we have too many variables, fitting methods tend to overfit. This problem requires that we remove variables, regularize or both. See the Trevor Hastie talk: Statistical Learning with Big Data - Trevor Hastie. 3.1.1 caret for model evaluation The caret Package provide a set of functions that streamline the process for fitting and evalluating a large numbet of predictive models in parallel. The package contains tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation The tools facilitate the process of automating randomly spliting data sets into training, testing and evaluating so that predictive models can be evaluated on a comparable and exhaustive basis. Especially useful is the functionality that is provided to repeatedly randomly stratify samples into train and test set so that any sample selection bias is removed. Some of the models which can be evaluated with caret include: (only some of these can be used with multinomial responses) FDA - Flexible Discriminant Analysis stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection knn - k nearest neighbors pam - Nearest shrunken centroids rf - Random forests svmRadial - Support vector machines (RBF kernel) gbm - Boosted trees xgbLinear - eXtreme Gradient Boosting xgbTree - eXtreme Gradient Boosting neuralnet - neural network Many more models can be implemented and evaluated with caret, including some deep learning methods. Simulated Annealing Feature Selection and Genetic Algorithms. Many methods found here are also worth investigating. 3.2 glmnet In this investigation we will focus on models that can be analyzed with the the glmnet R package [26]. Several factors favor this choice: the glmnet package is a well supported package providing extensive functionality for regularized regression and classification models the hyper-parameters of the elastic net enable us to explore the relationship between model size, or sparsity, and predictive accuracy. ie. we can investigate the “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. in our experience building classifiers from genomic scale data, regularized classification models using the elastic net penalty do as well as any other, and are more economical in terms of computing time, espacially in comparison to the more exotic boosting algorithms. the relaxed lasso, introduced in version 3.0 of glmnet, has been shown to be near optimal over a wide range of signal-to-noise regiments. One question that is of keen interest is where on the bias-variance, or signal-to-noise (SNR) spectrum does the classification of blood samples by means of 5hmc profiles lie? Much of the following comes from the Glmnet Vignette. Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter lambda ([26–29]). glmnet solves the following problem: \\[\\min_{\\beta_0,\\beta} \\frac{1}{N} \\sum_{i=1}^{N} w_i l(y_i,\\beta_0+\\beta^T x_i) + \\lambda\\left[(1-\\alpha)||\\beta||_2^2/2 + \\alpha ||\\beta||_1\\right],\\] over a grid of values of \\(\\lambda\\). Here \\(l(y,\\eta)\\) is the negative log-likelihood contribution for observation i; e.g. for the Gaussian case it is \\(\\frac{1}{2}(y-\\eta)^2\\). alpha hyper-parameter {-} The elastic-net penalty is controlled by \\(\\alpha\\), and bridges the gap between lasso (\\(\\alpha\\)=1, the default) and ridge (\\(\\alpha\\)=0). The tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an \\(\\alpha\\)=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of \\(\\alpha\\) is for numerical stability; for example, the elastic net with \\(\\alpha = 1 - \\epsilon\\) for some small \\(\\epsilon\\)&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations. Lasso vs Best Subset Best subset selection \\[\\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_0 \\leq k\\] lasso \\[\\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_1 \\leq t\\] Bertsimas et al. (2016) [30] presented a mixed integer optimization (MIO) formulation for the best subset selection problem Using these MIO solvers, can solve problems with p in the hundreds and even thousands demonstrated that best subset selection generally gives superior prediction accuracy compared to forward stepwise selection and the lasso, over a variety of problem setups. Hastie et al. (2017) [31] neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes; best subset selection and forward stepwise perform quite similarly throughout; the relaxed lasso is the overall winner, performing just about as well as the lasso in low SNR scenarios, and as well as best subset selection in high SNR scenarios. We conclude that it is able to use its auxiliary shrinkage parameter (γ) to get the “best of both worlds”: it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not. relaxed lasso \\[\\hat{\\beta}^{relax}(\\lambda, \\gamma) = \\gamma \\beta^{lasso}(\\lambda) + (1 - \\gamma)(\\beta^{LS}(\\lambda)\\] SNR \\(y_0=f(x_0) + \\epsilon_0\\) \\(SNR=\\frac{var(f(x_0))}{var(\\epsilon_0)}\\) \\(PVE(g)=1 - \\frac{\\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}\\) \\(PVE(f) = 1 - \\frac{Var(\\epsilon_0)}{Var(y_0)} = \\frac{SNR}{1+SNR}\\) \\(SNR = \\frac{PVE}{1-PVE}\\) \\(c_v = \\frac{\\sigma}{\\mu}=\\frac{Var(y)}{\\mathbb{E}(y)}\\) a PVE of 0.5 is rare for noisy observational data, and 0.2 may be more typical A PVE of 0.86, corresponding to an SNR of 6, is unheard of! For small SNR, SNR \\(\\approx\\) PVE See Xiang et al. (2020) [32], Lozoya et al. (2018) [33], Simonson et al. (2018) [34] and Rapaport et al. (2013) [35] for SNR in RNA-Seq "],
["model-baseline-lasso.html", "Section 4 Explore lasso models", " Section 4 Explore lasso models "],
["model-suite.html", "Section 5 Fitted Model Suite", " Section 5 Fitted Model Suite We examine the results of fitting a suite of models to investigate the effect of sample size on model performance. "],
["conclusions.html", "Section 6 Conclusions", " Section 6 Conclusions We have found that … Other questions … "],
["references.html", "References", " References "]
]
