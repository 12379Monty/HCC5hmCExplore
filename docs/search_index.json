[
["index.html", "DNA Hydroxymethylation in Hepatocellular Carcinoma Preamble License", " DNA Hydroxymethylation in Hepatocellular Carcinoma Francois Collin 2020 Preamble This vignette offers some exploratory data analyses of DNA Hydroxymethylation data available from NCBI GEO Series GSE112679. These data can be conveniently accessed through an R data package. See GSE112679 R Data Package page. License This work by Francois Collin is licensed under a Creative Commons Attribution 4.0 International License "],
["intro.html", "Section 1 Introduction", " Section 1 Introduction The goal of detecting cancer at the earliest stage of development with a non-invasive procedure has busied many groups with the task of perfecting techniques to support what has become commonly known as a liquid biopsy - the analysis of biomarkers circulating in fluids such as blood, saliva or urine. Epigenetic biomarkers present themselves as good candidates for this application (Gai and Sun (2019) [1]). In particular, given their prevalence in the human genome, close correlation with gene expression and high chemical stability, DNA modifications such as 5-methylcytosine (5mC) and 5-hydroxymethylcytosine (5hmC) are DNA epigenetic marks that provide much promise as cancer diagnosis biomarkers that could be profitably analyzed in liquid biopsies [2–5]. Li et al. (2017) [3] used a sensitive and selective chemical labeling technology to extract genome-wide 5hmC profiles from circulating cell-free DNA (cfDNA) as well as from genomic DNA (gDNA) collected from a cohort of 260 patients recently diagnosed with colorectal, gastric, pancreatic, liver or thyroid cancer and normal tissues from 90 healthy individuals They found 5hmC-based biomarkers of circulating cfDNA to be highly predictive of some cancer types. Similar small sample size findings were reported in Song et al. (2017) [4]. Focusing on hepatocellular carcinoma, Cai et al. (2019) [2] assembled a sizable dataset to demonstrate the feasibility of using features derived from 5-hydroxymethylcytosines marks in circulating cell-free DNA as a non-invasive approach for the early detection of hepatocellular carcinoma. The data that are the basis of that report are available on the NCBI GEO web site (Series GSE112679). The data have also been bundled in a R data package which can be installed from github: if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;12379Monty/GSE112679&quot;) An important question in the early development of classifiers of the sorts that are the basis of any liquid biopsy diagnostic tool is how many samples should be collected to make properly informed decisions. In this report we will explore the GSE112679 data to shed some light on the relationship between sample size and model performance in the context classifying samples based on 5hmC data. The same analyses are repeated on Breast Cancer RNA-Seq data to provide a different signal to noise context in which to evaluate classification model performance in relation to sample size. The layout of the report is the following: In Section 2 we provide background on fitting and analyzing predictive models in the n &lt;&lt; p context and give some detail about the primary tool used in this report. In Section 3 we pre-process the 5hmC data that we will use for the classification analysis and perform some light QC analyses. In Section 4 we explore sparse models that discriminate between early stage HCC and control samples based on 5hmC gene body intensity data. In Section 5 we examine the results of fitting a suite of models to the HCC 5hmC data investigate the effect of sample size on model performance. Sections 6, 7 and 8 repeat the analyses run on the HCC 5hmC data on breast cancer RNA-Seq data. Concluding remarks are in Section 9. "],
["modeling-background.html", "Section 2 Classification Analysis in the n &lt;&lt; p Context 2.1 caret for model evaluation 2.2 The glmnet R package 2.3 Signal-to-noise Ratio 2.4 Lasso vs Best Subset", " Section 2 Classification Analysis in the n &lt;&lt; p Context The main challenge in calibrating predictive models to genomic data is that there are many more features than there are example cases to fit to; the now classic \\(n &lt;&lt; p\\) problem. In this scenario, fitting methods tend to over fit. The problem can be addressed by selecting variables, regularizing the fit or both. In this report, we use genomic data to illustrate analyses of regularized regression models in the n &lt;&lt; p context. Much of the analysis focuses on lasso models fitted and analyzed using tools from the glmnet R package. Before providing some background on that glmnet package and the models that it supports, we briefly describe another R package which is an essential tool for anyone who is doing predictive analytics in R - caret R package. 2.1 caret for model evaluation The caret Package provides a rich set of functions that streamline the process for fitting and evaluating a large number of predictive models in parallel. The package contains tools for: data splitting pre-processing feature selection model tuning using re-sampling variable importance estimation The tools facilitate the process of automating randomly splitting data sets into training, testing and evaluating so that predictive models can be evaluated on a comparable and exhaustive basis. Especially useful is the functionality that is provided to repeatedly randomly stratify samples into train and test set so that any sample selection bias is removed. What makes the caret package extremely useful is that it provides a common interface to an exhaustive collection of fitting procedures. Without this common interface one has to learn the specific syntax that used in each fitting procedure to be included in a comparative analysis, which can be quite burdensome. Some of the models which can be evaluated with caret include: (only some of these can be used with multinomial responses) FDA - Flexible Discriminant Analysis stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection knn - k nearest neighbors pam - Nearest shrunken centroids rf - Random forests svmRadial - Support vector machines (RBF kernel) gbm - Boosted trees xgbLinear - eXtreme Gradient Boosting xgbTree - eXtreme Gradient Boosting neuralnet - neural network Many more models can be implemented and evaluated with caret, including some deep learning methods, Simulated Annealing Feature Selection and Genetic Algorithms. Many other methods found here are also worth investigating. We mention caret here because it is an extremely useful tool for anyone interested in exploring and comparing the performance of many predictive models which could be applied to a given predictive analysis context. We have gone through this exercise with a number of genomic scale, RNA-Seq data sets in the past and have found that in this context regularized regression models perform as well as any. For this report, we will only consider regularized classification models and will focus on the particular set of tools for fitting and analyzing these models provided by the glmnet R package [6]. 2.2 The glmnet R package Several factors favor using the glmnet R package to analyze classification models fitted to genomic scale data: the glmnet package is a well supported package providing extensive functionality for regularized regression and classification models. the hyper-parameters of the elastic net enable us to explore the relationship between model size, or sparsity, and predictive accuracy. ie. we can investigate the “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. in our experience building classifiers from genomic scale data, regularized classification models using the elastic net penalty do as well as any other, and are more economical in terms of computing time, especially in comparison to the more exotic boosting algorithms. the lasso has been shown to be near optimal for the \\(n&lt;&lt;p\\) problem over a wide range of signal-to-noise regiments (Hastie et al. (2017) [7]). Much of the following comes from the Glmnet Vignette. Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elastic net penalty at a grid of values for the regularization parameter lambda ([6,8–10]). glmnet solves the following problem: \\[\\min_{\\beta_0,\\beta} \\frac{1}{N} \\sum_{i=1}^{N} w_i l(y_i,\\beta_0+\\beta^T x_i) + \\lambda\\left[(1-\\alpha)||\\beta||_2^2/2 + \\alpha ||\\beta||_1\\right],\\] over a grid of values of \\(\\lambda\\). Here \\(l(y,\\eta)\\) is the negative log-likelihood contribution for observation i; e.g. for the Gaussian case it is \\(\\frac{1}{2}(y-\\eta)^2\\). alpha hyper-parameter The elastic-net penalty is controlled by \\(\\alpha\\), and bridges the gap between lasso (\\(\\alpha\\)=1, the default) and ridge (\\(\\alpha\\)=0). The tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an \\(\\alpha\\)=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of \\(\\alpha\\) is for numerical stability; for example, the elastic net with \\(\\alpha = 1 - \\epsilon\\) for some small \\(\\epsilon\\)&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations. 2.3 Signal-to-noise Ratio A key characteristic of classification problems is the prevailing signal-to-noise ratio (SNR) of the problem at hand. To define SNR, let \\((x_0, y_0) \\in \\mathbb{R}^p \\times \\mathbb{R}\\) be a pair of predcitor and response variables and define \\(f(x_0) = \\mathbb{E}(y_0|x_0)\\) and \\(\\epsilon = y_0 - f(x_0)\\) so that \\[y_0 = f(x_0) + \\epsilon_0.\\] The signal-to-noise ratio in this model is defined as \\[SNR=\\frac{var(f(x_0))}{var(\\epsilon_0)}.\\] It is useful to relate the SNR of a model to the proportion of variance explained (PVE). For a given prediction function g — eg. one trained on n samples \\((x_i, y_i) i = 1, \\dots, n\\) that are i.i.d. to \\((x_0, y_0)\\) — its associated proportion of variance explained is defined as \\[PVE(g)=1 - \\frac{\\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}.\\] This is maximized when we take \\(g\\) to be the mean function \\(f\\) itself, in which case \\[PVE(f) = 1 - \\frac{Var(\\epsilon_0)}{Var(y_0)} = \\frac{SNR}{1+SNR}.\\] Or equivalently, \\[SNR = \\frac{PVE}{1-PVE}.\\] Hastie, Tibshirani, and Tibshirani (2017) [7], point out that PVE is typically in the 0.2 range, and much lower in financial data. It is also much lower in 5hmC data, as we will see in the next section. Note that the SNR is a different characterization of noise level than the coefficient of variation: \\[c_v = \\frac{\\sigma}{\\mu}=\\frac{Var(y)}{\\mathbb{E}(y)}\\] Note that for small SNR, SNR \\(\\approx\\) PVE. See Xiang et al. (2020) [11], Lozoya et al. (2018) [12], Simonson et al. (2018) [13] and Rapaport et al. (2013) [14] for SNR in RNA-Seq 2.4 Lasso vs Best Subset Best subset selection finds the subset of k predictors that produces the best fit in terms of squared error, solving the non-convex problem: \\[\\begin{equation} \\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_0 \\leq k \\tag{2.1} \\end{equation}\\] The lasso solves a convex relaxation of the above where we replace the \\(l_0\\) norm by the \\(l_1\\) norm, namely \\[\\begin{equation} \\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_1 \\leq t \\tag{2.2} \\end{equation}\\] where \\(||\\beta||_1 = \\sum_{i=1}^{p} |\\beta_i|\\), and \\(t \\geq 0\\) is a tuning parameter. Bertsimas et al. (2016) [15] presented a mixed integer optimization (MIO) formulation for the best subset selection problem. Using these MIO solvers, one can solve problems with p in the hundreds and even thousands. Bertsimas et al. showed evidence that best subset selection generally gives superior prediction accuracy compared to forward stepwise selection and the lasso, over a variety of problem setups. In Hastie et al. (2017) [7], the authors countered by arguing that neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes. Best subset selection and forward stepwise perform quite similarly over a range of SNR contexts, but the relaxed lasso is the overall best option, performing just about as well as the lasso in low SNR scenarios, and as well as best subset selection in high SNR scenarios. Hastie et al. conclude that a blended mix of lasso and relaxed lasso estimator, the shrunken relaxed lasso fit, is able to use its auxiliary shrinkage parameter (\\(\\gamma\\)) to get the “best of both worlds”: it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not. Suppose the glmnet fitted linear predictor at \\(\\lambda\\) is \\(\\hat{\\eta}_\\lambda(x)\\) and the relaxed version is \\(\\tilde{\\eta}_\\lambda(x)\\), then the shrunken relaxed lasso fit is \\[\\begin{equation} \\tilde{\\eta}_{\\lambda,\\gamma}(x)=(1-\\gamma)\\tilde{\\eta}_\\lambda(x) + \\gamma \\hat{\\eta}_\\lambda(x) \\tag{2.3} \\end{equation}\\] \\(\\gamma \\in [0,\\, 1]\\) is an additional tuning parameter which can be selected by cross validation. The de-biasing will potentially improve prediction performance, and cross-validation will typically select a model with a smaller number of variables. This procedure is very competitive with forward-stepwise and best-subset regression, and has a considerable speed advantage when the number of variables is large. This is especially true for best-subset, but even so for forward stepwise. The latter has to plod through the variables one-at-a-time, while glmnet will just plunge in and find a good active set. Further details may be found in Friedman, Hastie, and Tibshirani (2010), Tibshirani et al. (2012), Simon et al. (2011), Simon, Friedman, and Hastie (2013) and Hastie, Tibshirani, and Tibshirani (2017) ([6–10]). "],
["hcc-5hmcseq-preproc.html", "Section 3 HCC 5hmC-Seq : Preprocessing 3.1 Load the data 3.2 Differential representation analysis 3.3 Signal-to-noise ratio regime", " Section 3 HCC 5hmC-Seq : Preprocessing Reader note: The most interesting part of this section is Subsection 3.3 which tells us where on the SNR spectrum the 5hmC data lies. In view of the work by Hastie et al. (2017) [7] summarized in Subsection 2.4, we would expect to lasso model, and the relaxed version, to do very well with this classification problem. 3.1 Load the data The data that are available from NCBI GEO Series GSE112679 can be conveniently accessed through an R data package. Attaching the GSE112679 package makes the count data tables available as well as a gene annotation table and a sample description table. See GSE112679 R Data Package page. In the Cai et al. [2] paper, samples were separated into Train and Val-1 subsets for model fitting and analysis. Val-2 was used as an external validation set. #### CLEAR CACHE if (!(&quot;GSE112679&quot; %in% rownames(installed.packages()))) { if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;12379Monty/GSE112679&quot;) } library(GSE112679) # Strip GSE_ID prefix from object names ### MOD: (2020.09.21) keep &#39;hcc5hmC_&#39; as prefix to object names. ### Otherwise we run into problems when different datasets are ### analyzed for(OBJ in data(package=&#39;GSE112679&#39;)$results[, &#39;Item&#39;]) assign(sub(&#39;GSE112679_&#39;,&#39;hcc5hmC_&#39;,OBJ), get(OBJ)) detach(package:GSE112679, unload = T ) # Continue hcc5hmC_sampDesc$DxStage &lt;- with(hcc5hmC_sampDesc, ifelse(outcome==&#39;HCC&#39;, paste0(outcome,&#39;:&#39;,stage), outcome)) with( hcc5hmC_sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot;), knitr::kable(table(DxStage, trainValGroup, exclude = NULL), caption=&quot;GSE112679 Samples by Dx Group and Subset&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) For this analysis, we will consider early stage cancer samples and healthy or benign samples from the Train or Val-1 subsets. The appropriate outcome variable will be renamed or aliased group #### CLEAR CACHE # Subset analysis samples hcc5hmC_sampDesc &lt;- hcc5hmC_sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot; &amp; (trainValGroup %in% c(&quot;Train&quot;, &quot;Val-1&quot;)) &amp; ((outcome2 == &quot;BenignHealthy&quot;) | (outcome2 == &quot;HCC&quot; &amp; stage == &quot;Early&quot;))) %&gt;% dplyr::rename(group = outcome2) %&gt;% dplyr::arrange(group, sampID) # Recode group hcc5hmC_sampDesc$group &lt;- with( hcc5hmC_sampDesc, ifelse(group == &quot;BenignHealthy&quot;, &quot;Control&quot;, group) ) # set hcc5hmC_groupCol for later hcc5hmC_groupCol &lt;- c(&quot;#F3C300&quot;, &quot;#875692&quot;) names(hcc5hmC_groupCol) &lt;- unique(hcc5hmC_sampDesc$group) with(hcc5hmC_sampDesc, knitr::kable(table(group, exclude = NULL), caption=&quot;Samples used in this analysis&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) Table 3.1: Samples used in this analysis group Freq Control 778 HCC 555 The features are counts of reads captured by chemical labeling, and indicate the level of 5-hydroxymethylcytosines within each gene body. Cai et al. (2019), Li et al. (2017) and Song et al. (2017) [2–4] all analyze 5hmC gene body counts using standard RNA-Seq methodologies, and we will do the same here. Note that before conducting any substantive analyses, the data would normally be very carefully examined for any sign of quality variation between groups of samples. This analysis would integrate sample meta data - where and when were the blood samples collected - as well as library preparation and sequencing metrics in order to detect any sign of processing artifacts that may be present in the dataset. This is particularly important when dealing with blood samples as variable DNA quality degradation is a well known challenge that is encountered when dealing with such samples [16]. Although blood specimen handling protocols can be put in place to minimize quality variation [17], variability can never be completely eradicated, especially in the context of blood samples collected by different groups, working in different environments. The problem of variable DNA quality becomes particularly pernicious when it is compounded with a confounding factor that sneaks in when the control sample collection events are separated in time and space from the cancer sample collection events; an all too common occurrence. As proper data QC requires an intimate familiarity with the details of data collection and processing, such a task cannot be undertaken here. We will simply run a minimal set of QC sanity checks to make sure that there are no apparent systematic effects in the data. #### CLEAR CACHE hcc5hmC_featureCounts &lt;- cbind( hcc5hmC_Train_featureCount, hcc5hmC_Val1_featureCount, hcc5hmC_Val2_featureCount )[, rownames(hcc5hmC_sampDesc)] We first look at coverage - make sure there isn’t too much disparity of coverage across samples. To detect shared variability, samples can be annotated and ordered according to sample features that may be linked to sample batch processing. Here we the samples have been ordered by group and sample id (an alias of geoAcc). #### CLEAR CACHE par(mar = c(1, 3, 2, 1)) boxplot(log2(hcc5hmC_featureCounts + 1), ylim = c(3, 11), ylab=&#39;log2 Count&#39;, staplewex = 0, # remove horizontal whisker lines staplecol = &quot;white&quot;, # just to be totally sure :) outline = F, # remove outlying points whisklty = 0, # remove vertical whisker lines las = 2, horizontal = F, xaxt = &quot;n&quot;, border = hcc5hmC_groupCol[hcc5hmC_sampDesc$group] ) legend(&quot;top&quot;, legend = names(hcc5hmC_groupCol), text.col = hcc5hmC_groupCol, ncol = 2, bty = &quot;n&quot;) # Add reference lines SampleMedian &lt;- apply(log2(hcc5hmC_featureCounts + 1), 2, median) abline(h = median(SampleMedian), col = &quot;grey&quot;) axis(side = 4, at = round(median(SampleMedian), 1), las = 2, col = &quot;grey&quot;, line = -1, tick = F) Figure 3.1: Sample log2 count boxplots #### CLEAR CACHE hcc5hmC_featureCounts_quant &lt;- apply(hcc5hmC_featureCounts, 2, function(CC) { c(quantile(CC, prob = c(.15, (1:3) / 4)), totCovM = sum(CC) / 1e6) }) hcc5hmC_featureCounts_quant2 &lt;- apply(hcc5hmC_featureCounts_quant, 1, function(RR) { quantile(RR, prob = (1:3) / 4) }) knitr::kable(hcc5hmC_featureCounts_quant2, digits = 1, caption = paste( &quot;Coverage Summary - Columns are sample coverage quantiles and total coverage&quot;, &quot;\\nRows are quartiles across samples&quot; ) ) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.2: Coverage Summary - Columns are sample coverage quantiles and total coverage Rows are quartiles across samples 15% 25% 50% 75% totCovM 25% 4 24 111 321 5.5 50% 5 30 135 391 6.7 75% 6 35 162 468 8.0 From this table, we see that 25% of the samples have total coverage exceeding 8M reads, 25% of samples have a 15 percentile of coverage lower than 4, etc. 3.2 Differential representation analysis In the remainder of this section, we will process the data and perform differential expression analysis as outlined in Law et al. (2018) [18]. The main analysis steps are: remove lowly expressed genes normalize gene expression distributions remove heteroscedasticity fit linear models and examine DE results It is good practice to perform this differential expression analysis prior to fitting models to get an idea of how difficult it will be to discriminate between samples belonging to the different subgroups. The pipeline outlined in Law et al. (2018) [18] also provides some basic quality assessment opportunities. Remove lowly expressed genes Genes that are not expressed at a biologically meaningful level in any condition should be discarded to reduce the subset of genes to those that are of interest, and to reduce the number of tests carried out downstream when looking at differential expression. Carrying uninformative genes may also be a hindrance to classification and other downstream analyses. To determine a sensible threshold we can begin by examining the shapes of the distributions. #### CLEAR CACHE par(mar = c(4, 3, 2, 1)) plot(density(hcc5hmC_lcpm_mtx[, 1]), col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[1]], lwd = 2, ylim = c(0, .25), las = 2, main = &quot;&quot;, xlab = &quot;log2 CPM&quot; ) abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(hcc5hmC_lcpm_mtx), size = 100)) { den &lt;- density(hcc5hmC_lcpm_mtx[, JJ]) lines(den$x, den$y, col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(hcc5hmC_groupCol), text.col = hcc5hmC_groupCol, bty = &quot;n&quot;) Figure 3.2: Sample \\(log_2\\) CPM densities As is typically the case with RNA-Seq data, we notice many weakly represented genes in this dataset. A cpm value of 1 appears to adequatly separate the expressed from the un-expressed genes, but we will be slightly more strict here and require a CPM threshold of 3 . Using a nominal CPM value of 3, genes are deeemed to be represented if their expression is above this threshold, and not represented otherwise. For this analysis we will require that genes be represented in at least 25 samples across the entire dataset to be retained for downstream analysis. Here, a CPM value of 3 means that a gene is represented if it has at least 9 reads in the sample with the lowest sequencing depth (library size 2.9 million). Note that the thresholds used here are arbitrary as there are no hard and fast rules to set these by. The voom-plot, which is part of analyses done to remove heteroscedasticity, can be examined to verify that the filtering performed is adequate. Remove weakly represented genes and re-plot densities. Removing 17.5% of genes… #### CLEAR CACHE par(mar = c(4, 3, 2, 1)) plot(density(hcc5hmC_F_lcpm_mtx[, 1]), col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[1]], lwd = 2, ylim = c(0, .25), las = 2, main = &quot;&quot;, xlab = &quot;log2 CPM&quot; ) #abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(hcc5hmC_F_lcpm_mtx), size = 100)) { den &lt;- density(hcc5hmC_F_lcpm_mtx[, JJ]) lines(den$x, den$y, col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(hcc5hmC_groupCol), text.col = hcc5hmC_groupCol, bty = &quot;n&quot;) Figure 3.3: Sample \\(log_2\\) CPM densities after removing weak genes As another sanity check, we will look at a multidimensional scaling plot of distances between gene expression profiles. We use plotMDS in limma package [19]), which plots samples on a two-dimensional scatterplot so that distances on the plot approximate the typical log2 fold changes between the samples. Before producing the MDS plot we will normalize the distributions. We will store the data into s DGEList object as this is convenient when running many of the analyses implemented in the edgeR and limma packages. #### CLEAR CACHE hcc5hmC_F_dgel &lt;- edgeR::DGEList( counts = hcc5hmC_featureCounts_F, genes = hcc5hmC_genes_annot_F, samples = hcc5hmC_sampDesc, group = hcc5hmC_sampDesc$group ) hcc5hmC_F_dgel &lt;- edgeR::calcNormFactors(hcc5hmC_F_dgel) hcc5hmC_F_lcpm_mtx &lt;- edgeR::cpm(hcc5hmC_F_dgel, log = T) # Save hcc5hmC_F_dgel to facilitate restarting # remove from final version save(list = &quot;hcc5hmC_F_dgel&quot;, file = &quot;RData/hcc5hmC_F_dgel&quot;) Verify that the counts are properly normalized. #### CLEAR CACHE par(mar = c(1, 3, 2, 1)) boxplot(hcc5hmC_F_lcpm_mtx, ylim = c(1, 8), ylab=&#39;Normalized Log CPM&#39;, staplewex = 0, # remove horizontal whisker lines staplecol = &quot;white&quot;, # just to be totally sure :) outline = F, # remove outlying points whisklty = 0, # remove vertical whisker lines las = 2, horizontal = F, xaxt = &quot;n&quot;, border = hcc5hmC_groupCol[hcc5hmC_sampDesc$group] ) legend(&quot;top&quot;, legend = names(hcc5hmC_groupCol), text.col = hcc5hmC_groupCol, ncol = 2, bty = &quot;n&quot;) # Add reference lines SampleMedian &lt;- apply(hcc5hmC_F_lcpm_mtx, 2, median) abline(h = median(SampleMedian), col = &quot;grey&quot;) axis(side = 4, at = round(median(SampleMedian), 1), las = 2, col = &quot;grey&quot;, line = -1, tick = F) Figure 3.4: Sample log2 count boxplots #### CLEAR CACHE par(mfcol = c(1, 2), mar = c(4, 4, 2, 1), xpd = NA, oma = c(0, 0, 2, 0)) # wo loss of generality, sample 500 samples # simply a matter of convenience to save time # remove from final version set.seed(1) samp_ndx &lt;- sample(1:ncol(hcc5hmC_F_lcpm_mtx), size = 500) MDS.out &lt;- limma::plotMDS(hcc5hmC_F_lcpm_mtx[, samp_ndx], col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[samp_ndx]], pch = 1 ) legend(&quot;topleft&quot;, legend = names(hcc5hmC_groupCol), text.col = hcc5hmC_groupCol, bty = &quot;n&quot; ) MDS.out &lt;- limma::plotMDS(hcc5hmC_F_lcpm_mtx[, samp_ndx], col = hcc5hmC_groupCol[hcc5hmC_sampDesc$group[samp_ndx]], pch = 1, dim.plot = 3:4 ) Figure 3.5: MDS plots of log-CPM values The MDS plot, which is analogous to a PCA plot adapted to gene expression data, does not indicate strong clustering of samples. The fanning pattern observed in the first two dimensions indicates that a few samples are drifting way from the core set, but in no particular direction. There is some structure in the 3rd and 4th dimension plot which should be investigated. glMDSPlot from package Glimma provides an interactive MDS plot that can extremely usful for exploration #### CLEAR CACHE Glimma::glMDSPlot(hcc5hmC_F_dgel[, samp_ndx], groups = hcc5hmC_F_dgel$samples[ samp_ndx, c(&quot;group&quot;, &quot;trainValGroup&quot;, &quot;sampType&quot;, &quot;tissue&quot;, &quot;title&quot;, &quot;stage&quot;) ], main = paste(&quot;MDS plot: filtered counts&quot;), #### , Excluding outlier samples&quot;), path = &quot;.&quot;, folder = figures_DIR, html = paste0(&quot;GlMDSplot&quot;), launch = F ) Link to glMDSPlot: Here No obvious factor links the samples in the 3 clusters observed on the 4th MDS dimensions. The percent of variance explained by this dimension or \\(\\approx\\) 4%. The glMDSPlot indicates further segregation along the 6th dimension. The percent of variance explained by this dimension or \\(\\approx\\) 2%. Tracking down this source of variability may be quite challenging, especially without having the complete information about the sample attributes and provenance. Unwanted variability is a well-documented problem in the analysis of RNA-Seq data (see Peixoto et al. (2015) [20]), and many procedures have been proposed to reduce the effect of unwanted variation on RNA-Seq analsys results ([20–22]). There are undoubtedly some similar sources of systematic variation in the 5hmC data, but it is beyond the scope of this work to investigate these in this particular dataset. Given that the clustering of samples occurs in MDS dimensions that explain a small fraction of variability, and that these is no association with the factor of interest, HCC vs Control, these sources of variability should not interfere too much with our classification analysis. It would nonetheless be interesting to assess whether downstream results can be improved by removing this variability. Creating a design matrix and contrasts Before proceeding with the statistical modeling used for the differential expression analysis, we need to set up a model design matrix. #### CLEAR CACHE Design_mtx &lt;- model.matrix( ~ -1 + group, data=hcc5hmC_F_dgel$samples) colnames(Design_mtx) &lt;- sub(&#39;group&#39;, &#39;&#39;, colnames(Design_mtx)) cat(&quot;colSums(Design_mtx):\\n&quot;) ## colSums(Design_mtx): colSums(Design_mtx) ## Control HCC ## 778 555 Contrasts_mtx &lt;- limma::makeContrasts( HCCvsControl = HCC - Control, levels=colnames(Design_mtx)) cat(&quot;Contrasts:\\n&quot;) ## Contrasts: Contrasts_mtx ## Contrasts ## Levels HCCvsControl ## Control -1 ## HCC 1 Removing heteroscedasticity from the count data As for RNA-Seq data, for 5hmC count data the variance is not independent of the mean. In limma, the R package we are using for our analyses, linear modeling is carried out on the log-CPM values which are assumed to be normally distributed and the mean-variance relationship is accommodated using precision weights calculated by the voom function. We apply this transformation next. #### CLEAR CACHE par(mfrow=c(1,1)) hcc5hmC_F_voom &lt;- limma::voom(hcc5hmC_F_dgel, Design_mtx, plot=T) Figure 3.6: Removing heteroscedascity Note that the voom-plot provides a visual check on the level of filtering performed upstream. If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be observed at the low end of the expression scale due to very small counts. Fit linear models and examine the results Having properly filtered and normalized the data, the linear models can be fitted to each gene and the results examined to assess differential expression between the two groups of interest, in our case HCC vs Control. Table ?? displays the counts of genes in each DE category: #### CLEAR CACHE hcc5hmC_F_voom_fit &lt;- limma::lmFit(hcc5hmC_F_voom, Design_mtx) colnames(hcc5hmC_F_voom_fit$coefficients) &lt;- sub(&quot;\\\\(Intercept\\\\)&quot;, &quot;Intercept&quot;, colnames(hcc5hmC_F_voom_fit$coefficients) ) hcc5hmC_F_voom_fit &lt;- limma::contrasts.fit( hcc5hmC_F_voom_fit, contrasts=Contrasts_mtx) hcc5hmC_F_voom_efit &lt;- limma::eBayes(hcc5hmC_F_voom_fit) hcc5hmC_F_voom_efit_dt &lt;- limma::decideTests(hcc5hmC_F_voom_efit,adjust.method = &quot;BH&quot;, p.value = 0.05) knitr::kable(t(summary(hcc5hmC_F_voom_efit_dt)), caption=&quot;DE Results at FDR = 0.05&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.3: DE Results at FDR = 0.05 Down NotSig Up HCCvsControl 5214 5280 5258 Graphical representations of DE results: MD Plots To summarise results for all genes visually, mean-difference plots (aka MA plot), which display log-FCs from the linear model fit against the average log-CPM values can be generated using the plotMD function, with the differentially expressed genes highlighted. We may also be interested in whether certain gene features are related to gene identification. Gene GC content, for example, might be of interest. #### CLEAR CACHE par(mfrow=c(1,3), mar=c(4.5,4.5,2,1),oma=c(1,1,2,0)) # log-fold-change vs ave-expr limma::plotMD(hcc5hmC_F_voom_efit, ylim = c(-0.4, 0.4), column=&#39;HCCvsControl&#39;, status=hcc5hmC_F_voom_efit_dt[,&#39;HCCvsControl&#39;], hl.pch = 16, hl.col = c(&quot;lightblue&quot;, &quot;pink&quot;), hl.cex = .5, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(hcc5hmC_F_voom_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F, cex.lab=1.5 ) abline(h = 0, col = &quot;black&quot;) rug(quantile(hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(hcc5hmC_F_voom_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) # log-fold-change vs identification boxplot(split( hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], hcc5hmC_F_voom_efit_dt[,&#39;HCCvsControl&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;log-fold-change&#39;, ylim=c(-.4, .4), cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) # gc vs identification genes_ndx &lt;- match(rownames(hcc5hmC_F_voom_efit), hcc5hmC_genes_annot_F$Symbol) if(sum(is.na(genes_ndx))) stop(&quot;hcc5hmC_F_voom_efit/hcc5hmC_genes_annot_F: genes mismatch&quot;) GC_vec &lt;- with(hcc5hmC_genes_annot_F[genes_ndx,],(G+C)/(A+C+G+T)) boxplot(split( GC_vec, hcc5hmC_F_voom_efit_dt[,&#39;HCCvsControl&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;gene-gc&#39;, cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) Figure 3.7: HCC vs Control - Genes Identified at FDR = 0,05 #mtext(side=3, outer=T, cex=1.25, &quot;Genes identified at adjusted p-value=0.05&quot;) #### CLEAR CACHE hcc5hmC_featureCounts_F_logFC_sum &lt;- sapply( split( hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], hcc5hmC_F_voom_efit_dt[,&#39;HCCvsControl&#39;]), quantile, prob = (1:3) / 4) colnames(hcc5hmC_featureCounts_F_logFC_sum) &lt;- as.character(factor( colnames(hcc5hmC_featureCounts_F_logFC_sum), levels=c(&quot;-1&quot;, &quot;0&quot;, &quot;1&quot;), labels=c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;) )) knitr::kable(hcc5hmC_featureCounts_F_logFC_sum, digits = 2, caption = &quot;log FC quartiles by gene identification&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.4: log FC quartiles by gene identification down notDE up 25% -0.07 -0.01 0.04 50% -0.05 0.00 0.06 75% -0.03 0.01 0.09 While many genes are identified, the effect sizes are quite small, which results in a low signal-to-noise ratio context. See Section 3.3 below. The log-fold-change distribution for up-represented genes is long-tailed, with many high log fold-change values. By contrast, log-fold-change distribution for down-represented genes closer to symmetric and has few genes with low log fold-change values. We will see how this affects the results of identifying genes with an effect size requirement. The GC content of down regulated genes tends to be slightly lower than the rest of the genes. A statistical test would find that the difference between the mean of the down regulated gene population is significantly different than the mean of the other gene population even though the difference is quite small (-0.028). These asymmetries are minor, but it would still be good to establish that they reflect biology rather than processing artifacts. DE genes at 10% fold change For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The treat method (McCarthy and Smyth 2009 [23]) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement. The number of differentially expressed genes are greatly reduced if we impose a minimal fold-change requirement of 10%. #### CLEAR CACHE hcc5hmC_F_voom_tfit &lt;- limma::treat(hcc5hmC_F_voom_fit, lfc=log2(1.10)) hcc5hmC_F_voom_tfit_dt &lt;- limma::decideTests(hcc5hmC_F_voom_tfit) cat(&quot;10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05:\\n&quot;) ## 10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05: summary(hcc5hmC_F_voom_tfit_dt) ## HCCvsControl ## Down 3 ## NotSig 15550 ## Up 199 # log-fold-change vs ave-expr limma::plotMD(hcc5hmC_F_voom_efit, ylim = c(-0.5, 0.5), column=&#39;HCCvsControl&#39;, status=hcc5hmC_F_voom_tfit_dt[,&#39;HCCvsControl&#39;], hl.pch = 16, hl.col = c(&quot;blue&quot;, &quot;red&quot;), hl.cex = .7, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(hcc5hmC_F_voom_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F ) abline(h = 0, col = &quot;black&quot;) rug(quantile(hcc5hmC_F_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(hcc5hmC_F_voom_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) Figure 3.8: HCC vs Control - Identified Genes at FDR = 0,05 and logFC &gt; 10% As noted above, the log-fold-change distribution for the up-represented genes is long-tailed in comparison to log-fold-change distribution for the down-represented genes. As a result fewer down-represented than up-regulated genes are identified when a minimum log-FC requirement is imposed. 3.3 Signal-to-noise ratio regime In Hastie et al. (2017) [7]) results from lasso fits are compared with best subset and forward selection fits and it is argued that while best subset is optimal for high signal-to-noise regimes, the lasso gains some competitive advantage when the prevailing signal-to-noise ratio of the dataset is lowered. We can extract sigma and signal from the fit objects to get SNR values for each gene to see in what SNR regime the 5hmC gene body data are. #### CLEAR CACHE lib.size &lt;- colSums(hcc5hmC_F_dgel$counts) fit &lt;- hcc5hmC_F_voom_efit sx &lt;- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06) sy &lt;- sqrt(fit$sigma) CV &lt;- sy/sx #### CLEAR CACHE Effect &lt;- abs(hcc5hmC_F_voom_efit$coefficients[,&#39;HCCvsControl&#39;]) Noise &lt;- hcc5hmC_F_voom_efit$sigma SNR &lt;- Effect/Noise plot(spatstat::CDF(density(SNR)), col = 1, lwd = 2, ylab = &quot;Prob(SNR&lt;x)&quot;, xlim = c(0, 0.2) ) SNR_quant &lt;- quantile(SNR, prob=c((1:3)/4,.9)) rug(SNR_quant, lwd = 2, ticksize = 0.05, col = 1 ) Figure 3.9: Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile knitr::kable(t(SNR_quant), digits = 3, caption = paste( &quot;SNR Quantiles&quot;) ) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.5: SNR Quantiles 25% 50% 75% 90% 0.018 0.036 0.06 0.082 These SNR values are in the range where the lasso and relaxed lasso gain some advantage over best subset and forward selection fits (see Hastie et al. (2017) [7]). "],
["hcc-5hmcseq-explore-sparsity.html", "Section 4 HCC 5hmC-Seq: Exploring Sparsity 4.1 Cross-validation analysis setup 4.2 Fit and compare models 4.3 The relaxed lasso and blended mix models 4.4 Examination of sensitivity vs specificity 4.5 Compare predictions at misclassified samples 4.6 Compare coefficient profiles 4.7 Examine feature selection", " Section 4 HCC 5hmC-Seq: Exploring Sparsity In this section we explore various models fitted to the HCC 5hmC-Seq data set explored in Section 3. We focus our analyses on lasso fits which tend to favor sparse models. These fits can be computed and analyzed with tools provided in the glmnet package. Refer to the Glmnet Vignette for a quick reference guide. 4.1 Cross-validation analysis setup K_FOLD &lt;- 10 trainP &lt;- 0.8 First we divide the analysis dataset into train and test in a \\(4\\):1 ratio. set.seed(1) hcc5hmC_train_sampID_vec &lt;- with(hcc5hmC_F_dgel$samples, hcc5hmC_F_dgel$samples$sampID[caret::createDataPartition(y=group, p=trainP, list=F)] ) hcc5hmC_test_sampID_vec &lt;- with(hcc5hmC_F_dgel$samples, setdiff(sampID, hcc5hmC_train_sampID_vec) ) hcc5hmC_train_group_vec &lt;- hcc5hmC_F_dgel$samples[hcc5hmC_train_sampID_vec, &#39;group&#39;] names(hcc5hmC_train_group_vec) &lt;- hcc5hmC_F_dgel$samples[hcc5hmC_train_sampID_vec, &#39;sampID&#39;] hcc5hmC_test_group_vec &lt;- hcc5hmC_F_dgel$samples[hcc5hmC_test_sampID_vec, &#39;group&#39;] names(hcc5hmC_test_group_vec) &lt;- hcc5hmC_F_dgel$samples[hcc5hmC_test_sampID_vec, &#39;sampID&#39;] knitr::kable(table(hcc5hmC_train_group_vec), caption=&quot;Train set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Train set hcc5hmC_train_group_vec Freq Control 623 HCC 444 knitr::kable(table(hcc5hmC_test_group_vec), caption=&quot;Test set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Test set hcc5hmC_test_group_vec Freq Control 155 HCC 111 hcc5hmC_train_lcpm_mtx &lt;- t(hcc5hmC_lcpm_mtx[,hcc5hmC_train_sampID_vec]) hcc5hmC_test_lcpm_mtx &lt;- t(hcc5hmC_lcpm_mtx[,hcc5hmC_test_sampID_vec]) We explore some glmnet fits and the “bet on sparsity”. We consider three models, specified by the value of the alpha parameter in the elastic net parametrization: - lasso: \\(\\alpha = 1.0\\) - sparse models - ridge \\(\\alpha = 0\\) - shrunken coefficients models - elastic net: \\(\\alpha = 0.5\\) - semi sparse model Some questions of interest include: How sparse are models enabling good 5hmC classification of Early HCC vs Control samples? Does the shrunken relaxed lasso (aka the blended mix) improve performance in this case? Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set? In this analysis, we will only evaluate models in terms of model sparsity, stability and performance. We leave the question of significance testing of hypotheses about model parameters completely out. See Lockhart et al. (2014) [24] and Wassermam (2014) [25] for a discussion of this topic. In this section we look at the relative performance and sparsity of the models considered. The effect of the size of the sample set on the level and stability of performance will be investigated in the next section. First we create folds for \\(10\\)-fold cross-validation of models fitted to training data. We’ll use caret::createFolds to assign samples to folds while keeping the outcome ratios constant across folds. # This is too variable, both in terms of fold size And composition #foldid_vec &lt;- sample(1:10, size=length(hcc5hmC_train_group_vec), replace=T) set.seed(1) hcc5hmC_train_foldid_vec &lt;- caret::createFolds( factor(hcc5hmC_train_group_vec), k=K_FOLD, list=F) # hcc5hmC_train_foldid_vec contains the left-out IDs # the rest are kept fold_out_tbl &lt;- sapply(split(hcc5hmC_train_group_vec, hcc5hmC_train_foldid_vec), table) rownames(fold_out_tbl) &lt;- paste(rownames(fold_out_tbl), &#39;- Out&#39;) fold_in_tbl &lt;- do.call(&#39;cbind&#39;, lapply(sort(unique(hcc5hmC_train_foldid_vec)), function(FOLD) table(hcc5hmC_train_group_vec[hcc5hmC_train_foldid_vec != FOLD]))) rownames(fold_in_tbl) &lt;- paste(rownames(fold_in_tbl), &#39;- In&#39;) colnames(fold_in_tbl) &lt;- as.character(sort(unique(hcc5hmC_train_foldid_vec))) knitr::kable(rbind(fold_in_tbl, fold_out_tbl[,colnames(fold_in_tbl)]), caption=&quot;training samples fold composition&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.2: training samples fold composition 1 2 3 4 5 6 7 8 9 10 Control - In 561 561 561 560 561 561 560 561 560 561 HCC - In 399 400 400 399 400 399 400 399 400 400 Control - Out 62 62 62 63 62 62 63 62 63 62 HCC - Out 45 44 44 45 44 45 44 45 44 44 Note that the folds identify samples that are left-out of the training data for each fold fit. 4.2 Fit and compare models glmnet provides cross-validation methods to pick the parameter lambda which controls to size of the penalty function. The “one standard error rule” produces a model with fewer predictors then the minimum cv error model. On the training data, this usually results in increased MSE and more biased parameter estimates (see Engebretsen et al. (2019) [26] for example). The question of interest though is the performance on unseen data; not on the training data. The cv error rates computed and stored in fit results by glmnet methods are out-of-fold error rates computed from the training data. The results show that these are good indicators of test set error rates, and that the one standard error rule models do as well as the minimum cv error models for the lasso, which has the best overall performance. 4.2.1 Logistic regression in glmnet It is sometimes necessary to compute cv error rates manually from glmnet fit results. This section provides some necessary detail. glmnet provides functionality to extract various predicted of fitted values from calibrated models. Note that some folks make a distinction between fitted or estimated values for sample points in the training data versus predicted values for sample points that are not in the training dataset. glmnet makes no such distinction and the predict function is used to produce both fitted as well as predicted values. When predict is invoked to make predictions for design points that are part of the training dataset, what is returned are fitted values. When predict is invoked to make predictions for design points that are not part of the training dataset, what is returned are predicted values. For logistic regressions, which is the model fitted in a regularized fashion when models are fitted by glmnet with the parameter family='binomial', three fitted or predicted values can be extracted at a given design point. Suppose our response variable Y is either 0 or 1 (Control or HCC in our case). These are specified by the type parameter. type='resp' returns the fitted or predicted probability of \\(Y=1\\). type='class' returns the fitted or predicted class for the design point, which is simply dichotomizing the response: class = 1 if the fitted or predicted probability is greater than 0.5 (check to make sure class is no the Bayes estimate). type='link' returns the fitted or predicted value of the linear predictor \\(\\beta&#39;x\\). The relationship between the linear predictor and the response can be derided from the logistic regression model: \\[P(Y=1|x,\\beta) = g^{-1}(\\beta&#39;x) = h(\\beta&#39;x) = \\frac{e^{\\beta&#39;x}}{1+e^{\\beta&#39;x}}\\] where \\(g\\) is the link function, \\(g^{-1}\\) the mean function. The link function is given by: \\[g(y) = h^{-1}(y) = ln(\\frac{y}{1-y})\\] This link function is called the logit function, and its inverse the logistic function. logistic_f &lt;- function(x) ifelse(is.nan(exp(x)/(1+exp(x))), 1, exp(x)/(1+exp(x))) We should also point out that the cv error rates quoted in various glmnet summaries are computed from out-of-fold predictions. In other words, we cannot recover the cvm component of a glmnet fit object by comparing class predictions extracted by invoking predict() with parameters newx = train_data and type = 'class' to the true class labels. glmnet fitting functions have a parameter, keep, which instructs the fitting function to keep the out-of-fold, or prevalidated, predictions as part of the returned object. The out-of-fold predictions are predicted values for the samples in the left-out folds, pooled across all cv folds. For each hyper-parameter specification, we get one full set of out-of-fold predictions for the training set samples. Performance assessments based on these values are usually more generalizable. The cvm component of glmnet fitted objects are derived from these out-of-fold predictions. See Hofling and Tibshirani (2008) [27] for a description of the use of pre-validation in model assessment. start_time &lt;- proc.time() hcc5hmC_cv_lasso &lt;- glmnet::cv.glmnet( x=hcc5hmC_train_lcpm_mtx, y=hcc5hmC_train_group_vec, foldid=hcc5hmC_train_foldid_vec, alpha=1, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;lasso time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lasso time: 16.23s start_time &lt;- proc.time() hcc5hmC_cv_ridge &lt;- glmnet::cv.glmnet( x=hcc5hmC_train_lcpm_mtx, y=hcc5hmC_train_group_vec, foldid=hcc5hmC_train_foldid_vec, alpha=0, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;ridge time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## ridge time: 131.08s start_time &lt;- proc.time() hcc5hmC_cv_enet &lt;- glmnet::cv.glmnet( x=hcc5hmC_train_lcpm_mtx, y=hcc5hmC_train_group_vec, foldid=hcc5hmC_train_foldid_vec, alpha=0.5, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;enet time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## enet time: 14.52s plot_cv_f &lt;- function(cv_fit, Nzero=T, ...) { suppressPackageStartupMessages(require(glmnet)) # No nonger used #lambda.1se_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se] #lambda.min_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min] # Get oof error - cv errors produced by extraction method ARE oof!!! ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) train_oofPred_1se_vec &lt;- ifelse( logistic_f(cv_fit$fit.preval[,ndx_1se]) &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) train_oofPred_1se_error &lt;- mean(train_oofPred_1se_vec != hcc5hmC_train_group_vec) ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) train_oofPred_min_vec &lt;- ifelse( logistic_f(cv_fit$fit.preval[,ndx_min]) &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) train_oofPred_min_error &lt;- mean(train_oofPred_min_vec != hcc5hmC_train_group_vec) # Get test set error test_pred_1se_vec &lt;- predict( cv_fit, newx=hcc5hmC_test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_pred_1se_error &lt;- mean(test_pred_1se_vec != hcc5hmC_test_group_vec) test_pred_min_vec &lt;- predict( cv_fit, newx=hcc5hmC_test_lcpm_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_pred_min_error &lt;- mean(test_pred_min_vec != hcc5hmC_test_group_vec) plot( log(cv_fit$lambda), cv_fit$cvm, pch=16,col=&quot;red&quot;, xlab=&#39;&#39;,ylab=&#39;&#39;, ... ) abline(v=log(c(cv_fit$lambda.1se, cv_fit$lambda.min))) if(Nzero) axis(side=3, tick=F, at=log(cv_fit$lambda), labels=cv_fit$nzero, line = -1 ) LL &lt;- 2 #mtext(side=1, outer=F, line = LL, &quot;log(Lambda)&quot;) #LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;1se p =&quot;, lambda.1se_p),&#39;&#39;), &quot;1se: train =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], 1), ##&quot;oof =&quot;, round(100*train_oofPred_1se_error, 1), ### REDUNDANT &quot;test =&quot;, round(100*test_pred_1se_error, 1) )) LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;min p =&quot;, lambda.min_p),&#39;&#39;), &quot;min: train =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], 1), ##&quot;oof =&quot;, round(100*train_oofPred_min_error, 1), ### REDUNDANT &quot;test =&quot;, round(100*test_pred_min_error, 1) )) tmp &lt;- cbind( error_1se = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se], train = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], #train_oof = 100*train_oofPred_1se_error, ### REDUNANT test = 100*test_pred_1se_error), error_min = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min], train = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], #train_oof = 100*train_oofPred_min_error, ### REDUNDSANT test = 100*test_pred_min_error) ) # Need to fix names rownames(tmp) &lt;- c(&#39;p&#39;, &#39;train&#39;, &#39;test&#39;) tmp } Examine model performance. par(mfrow=c(1,3), mar=c(5, 2, 3, 1), oma=c(3,2,0,0)) lasso_errors_mtx &lt;- plot_cv_f(hcc5hmC_cv_lasso, ylim=c(0,.5)) ## Warning: package &#39;glmnet&#39; was built under R version 4.0.2 title(&#39;lasso&#39;) rifge_errors_mtx &lt;- plot_cv_f(hcc5hmC_cv_ridge, Nzero=F, ylim=c(0,.5)) title(&#39;ridge&#39;) enet_errors_mtx &lt;- plot_cv_f(hcc5hmC_cv_enet, ylim=c(0,.5)) title(&#39;enet&#39;) mtext(side=1, outer=T, cex=1.25, &#39;log(Lambda)&#39;) mtext(side=2, outer=T, cex=1.25, hcc5hmC_cv_lasso$name) Figure 4.1: compare fits errors_frm &lt;- data.frame( lasso = lasso_errors_mtx, ridge = rifge_errors_mtx, enet = enet_errors_mtx ) colnames(errors_frm) &lt;- sub(&#39;\\\\.error&#39;,&#39;&#39;, colnames(errors_frm)) knitr::kable(t(errors_frm), caption = &#39;Misclassifiaction error rates&#39;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.3: Misclassifiaction error rates p train test lasso_1se 68 7.9 10.9 lasso_min 226 7.0 10.2 ridge_1se 19100 15.0 18.8 ridge_min 19100 13.8 17.3 enet_1se 160 7.1 11.3 enet_min 284 6.6 9.4 We see that the lasso and enet models do better than the ridge model. The one standard error rule (1se) lambda lasso fit is only slightly more parsimonious than the 1se elastic net fit, but its test set accuracy is better. The min lambda models have lower test set error rates than the more parsimonious 1se models in the ridge and elastic net case. (for the lasso, 1se and minimum lambda rules give the same model). The minimum lambda elastic net fit performs as well as the lasso model, but is much less parsimonious. Note that the cv estimates of misclassification error produced by the glmnet extraction method are out-of-fold estimated error rates. In this particular dataset, these estimates of error are optimistic in comparison the test set error rates. 4.3 The relaxed lasso and blended mix models Next we look at the so-called relaxed lasso model, and the blended mix which is an optimized shrinkage between the relaxed lasso and the regular lasso. See (2.3) in Section 2. library(glmnet) hcc5hmC_cv_lassoR_sum &lt;- print(hcc5hmC_cv_lassoR) ## ## Call: glmnet::cv.glmnet(x = hcc5hmC_train_lcpm_mtx, y = hcc5hmC_train_group_vec, type.measure = &quot;class&quot;, foldid = hcc5hmC_train_foldid_vec, keep = T, relax = T, alpha = 1, family = &quot;binomial&quot;, nlambda = 30) ## ## Measure: Misclassification Error ## ## Gamma Lambda Measure SE Nonzero ## min 0.5 0.03790 0.06467 0.004819 38 ## 1se 0.5 0.04442 0.06560 0.005303 33 plot(hcc5hmC_cv_lassoR) Figure 4.2: Relaxed lasso fit # only report 1se ndx_1se &lt;- match(hcc5hmC_cv_lassoR$lambda.1se, hcc5hmC_cv_lassoR$lambda) ndx_min &lt;- match(hcc5hmC_cv_lassoR$lambda.min, hcc5hmC_cv_lassoR$lambda) # only show 1se anyway # if(ndx_1se != ndx_min) stop(&quot;lambda.1se != lambda.min&quot;) # train oof data - NOT CLEAR WHY THESE DIFFER FROM CV ERRORS EXTRACTED FROM MODEL # Get relaxed lasso (gamma=0) oof error train_oofPred_relaxed_1se_vec &lt;- ifelse( logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&quot;g:0&quot;]][, ndx_1se]) &gt; 0.5, &quot;HCC&quot;, &quot;Control&quot; ) train_oofPred_relaxed_1se_error &lt;- mean(train_oofPred_relaxed_1se_vec != hcc5hmC_train_group_vec) # blended mix (gamma=0.5) train_oofPred_blended_1se_vec &lt;- ifelse( logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&quot;g:0.5&quot;]][, ndx_1se]) &gt; 0.5, &quot;HCC&quot;, &quot;Control&quot; ) train_oofPred_blended_1se_error &lt;- mean(train_oofPred_blended_1se_vec != hcc5hmC_train_group_vec) # Test set error - relaxed test_pred_relaxed_1se_vec &lt;- predict( hcc5hmC_cv_lassoR, newx = hcc5hmC_test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0 ) test_pred_relaxed_1se_error &lt;- mean(test_pred_relaxed_1se_vec != hcc5hmC_test_group_vec) # Test set error - blended test_pred_blended_1se_vec &lt;- predict( hcc5hmC_cv_lassoR, newx = hcc5hmC_test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0.5 ) test_pred_blended_1se_error &lt;- mean(test_pred_blended_1se_vec != hcc5hmC_test_group_vec) hcc5hmC_cv_lassoR_1se_error &lt;- hcc5hmC_cv_lassoR$cvm[hcc5hmC_cv_lassoR$lambda==hcc5hmC_cv_lassoR$lambda.min] cv_blended_statlist &lt;- hcc5hmC_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] cv_blended_1se_error &lt;- cv_blended_statlist$cvm[cv_blended_statlist$lambda== hcc5hmC_cv_lassoR$relaxed$lambda.1se] knitr::kable(t(data.frame( train_relaxed = hcc5hmC_cv_lassoR_1se_error, train_blended = cv_blended_1se_error, #train_relaxed_oof = train_oofPred_relaxed_1se_error, #train_blended_oof = train_oofPred_blended_1se_error, test_relaxed = test_pred_relaxed_1se_error, test_blended = test_pred_blended_1se_error )) * 100, digits = 1, caption = &quot;Relaxed lasso and blended mix error rates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.4: Relaxed lasso and blended mix error rates train_relaxed 7.0 train_blended 6.6 test_relaxed 10.5 test_blended 9.8 The relaxed lasso and blended mix error rates are comparable to the regular lasso fit error rate. We see here too that the reported cv error rates are optimistic. The 1se lambda rule applied to the relaxed lasso fit selected a model with \\(68\\) features, while for the blended mix model (See (2.3) in Section 2) the 1se lambda rule selected \\(33\\) features (vertical dotted reference line in Figure ??). This feature is pointed out in the glmnet 3.0 vignette: The debiasing will potentially improve prediction performance, and CV will typically select a model with a smaller number of variables. 4.4 Examination of sensitivity vs specificity In the results above we reported error rates without inspecting the sensitivity versus specificity trade-off. ROC curves can be examined to get a sense of the trade-off. 4.4.1 Training data out-of-fold ROC curves # train # lasso ndx_1se &lt;- match(hcc5hmC_cv_lasso$lambda.1se,hcc5hmC_cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lasso$fit.preval[,ndx_1se]) train_lasso_roc &lt;- pROC::roc( response = as.numeric(hcc5hmC_train_group_vec==&#39;HCC&#39;), predictor = train_lasso_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # enet ndx_1se &lt;- match(hcc5hmC_cv_enet$lambda.1se,hcc5hmC_cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_enet$fit.preval[,ndx_1se]) train_enet_roc &lt;- pROC::roc( response = as.numeric(hcc5hmC_train_group_vec==&#39;HCC&#39;), predictor = train_enet_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # lasso - relaxed ndx_1se &lt;- match(hcc5hmC_cv_lassoR$lambda.1se,hcc5hmC_cv_lassoR$lambda) train_relaxed_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_relaxed_roc &lt;- pROC::roc( response = as.numeric(hcc5hmC_train_group_vec==&#39;HCC&#39;), predictor = train_relaxed_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # blended mix (gamma=0.5) ndx_1se &lt;- match(hcc5hmC_cv_lassoR$lambda.1se,hcc5hmC_cv_lassoR$lambda) train_blended_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_roc &lt;- pROC::roc( response = as.numeric(hcc5hmC_train_group_vec==&#39;HCC&#39;), predictor = train_blended_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(train_lasso_roc, col=col_vec[1]) lines(train_enet_roc, col=col_vec[2]) lines(train_relaxed_roc, col=col_vec[3]) lines(train_blended_roc, col=col_vec[4]) legend(&#39;bottomright&#39;, title=&#39;AUC&#39;, legend=c( paste(&#39;lasso =&#39;, round(train_lasso_roc[[&#39;auc&#39;]],3)), paste(&#39;enet =&#39;, round(train_enet_roc[[&#39;auc&#39;]],3)), paste(&#39;relaxed =&#39;, round(train_relaxed_roc[[&#39;auc&#39;]],3)), paste(&#39;blended =&#39;, round(train_blended_roc[[&#39;auc&#39;]],3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 4.3: Train data out-of-sample ROCs Compare thresholds for 90% Specificity: lasso_ndx &lt;- with(as.data.frame(pROC::coords(train_lasso_roc, transpose=F)), min(which(specificity &gt;= 0.9))) enet_ndx &lt;- with(as.data.frame(pROC::coords(train_enet_roc, transpose=F)), min(which(specificity &gt;= 0.9))) lassoR_ndx &lt;- with(as.data.frame(pROC::coords(train_relaxed_roc, transpose=F)), min(which(specificity &gt;= 0.9))) blended_ndx &lt;- with(as.data.frame(pROC::coords(train_blended_roc, transpose=F)), min(which(specificity &gt;= 0.9))) spec90_frm &lt;- data.frame(rbind( lasso=as.data.frame(pROC::coords(train_lasso_roc, transpose=F))[lasso_ndx,], enet=as.data.frame(pROC::coords(train_enet_roc, transpose=F))[enet_ndx,], relaxed=as.data.frame(pROC::coords(train_relaxed_roc, transpose=F))[lassoR_ndx,], blended=as.data.frame(pROC::coords(train_blended_roc, transpose=F))[blended_ndx,] )) knitr::kable(spec90_frm, digits=3, caption=&quot;Specificity = .90 Coordinates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.5: Specificity = .90 Coordinates threshold specificity sensitivity lasso 0.358 0.9 0.932 enet 0.350 0.9 0.937 relaxed 0.356 0.9 0.860 blended 0.281 0.9 0.867 This is strange. par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(train_lasso_oofProb_vec[hcc5hmC_train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_lasso_oofProb_vec[hcc5hmC_train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;HCC&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(train_enet_oofProb_vec[hcc5hmC_train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_enet_oofProb_vec[hcc5hmC_train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # lassoR plot(density(train_relaxed_oofProb_vec[hcc5hmC_train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_relaxed_oofProb_vec[hcc5hmC_train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lassoR&quot;) # blended plot(density(train_blended_oofProb_vec[hcc5hmC_train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_blended_oofProb_vec[hcc5hmC_train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;out-of-fold predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 4.4: Train data out-of-fold predicted probabilities The relaxed lasso fit results in essentially dichotomized predicted probability distribution - predicted probabilities are very close to 0 or 1. Look at test data ROC curves. # plot all plot(test_lasso_roc, col = col_vec[1]) lines(test_enet_roc, col = col_vec[2]) lines(test_relaxed_roc, col = col_vec[3]) lines(test_blended_roc, col = col_vec[4]) legend(&quot;bottomright&quot;, title = &quot;AUC&quot;, legend = c( paste(&quot;lasso =&quot;, round(test_lasso_roc[[&quot;auc&quot;]], 3)), paste(&quot;enet =&quot;, round(test_enet_roc[[&quot;auc&quot;]], 3)), paste(&quot;relaxed =&quot;, round(test_relaxed_roc[[&quot;auc&quot;]], 3)), paste(&quot;blended =&quot;, round(test_blended_roc[[&quot;auc&quot;]], 3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 4.5: Test data out-of-sample ROCs Look at densities of predicted probabilities. par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(test_lasso_predProb_vec[hcc5hmC_test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_lasso_predProb_vec[hcc5hmC_test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;HCC&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(test_enet_predProb_vec[hcc5hmC_test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_enet_predProb_vec[hcc5hmC_test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # relaxed plot(density(test_relaxed_predProb_vec[hcc5hmC_test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_relaxed_predProb_vec[hcc5hmC_test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;relaxed&quot;) #sapply(split(test_relaxed_predProb_vec, hcc5hmC_test_group_vec), summary) # blended plot(density(test_blended_predProb_vec[hcc5hmC_test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_blended_predProb_vec[hcc5hmC_test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;test set predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 4.6: Test data out-of-fold predicted probabilities # Define plotting function bxpPredProb_f &lt;- function(cv_fit, Gamma=NULL) { # Train - preval is out-of-fold linear predictor for training design points onese_ndx &lt;- match(cv_fit$lambda.1se, cv_fit$lambda) if(is.null(Gamma)) train_1se_preval_vec &lt;- cv_fit$fit.preval[, onese_ndx] else train_1se_preval_vec &lt;- cv_fit$fit.preval[[Gamma]][, onese_ndx] train_1se_predProb_vec &lt;- logistic_f(train_1se_preval_vec) # Test test_1se_predProb_vec &lt;- predict( cv_fit, newx = hcc5hmC_test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;resp&quot; ) tmp &lt;- c( train = split(train_1se_predProb_vec, hcc5hmC_train_group_vec), test = split(test_1se_predProb_vec, hcc5hmC_test_group_vec) ) names(tmp) &lt;- paste0(&quot;\\n&quot;, sub(&quot;\\\\.&quot;, &quot;\\n&quot;, names(tmp))) boxplot(tmp) } par(mfrow = c(2, 2), mar = c(5, 3, 2, 1), oma = c(2, 2, 2, 2)) bxpPredProb_f(hcc5hmC_cv_lasso) title(&#39;lasso&#39;) bxpPredProb_f(hcc5hmC_cv_enet) title(&#39;enet&#39;) bxpPredProb_f(hcc5hmC_cv_lassoR, Gamma=&#39;g:0&#39;) title(&#39;relaxed&#39;) bxpPredProb_f(hcc5hmC_cv_lassoR, Gamma=&#39;g:0.5&#39;) title(&#39;blended&#39;) Figure 4.7: Predicted Probabilities - Train and Test 4.5 Compare predictions at misclassified samples It is useful to examine classification errors more carefully. If models have different failure modes, one might get improved performance by combining model predictions. Note that the models considered here are not expected to compliment each other usefully as they are too similar in nature. # NOTE: here we use computred oofClass rather than predClass # as predClass extracted from predict() are fitted values. # train - oof ndx_1se &lt;- match(hcc5hmC_cv_lasso$lambda.1se,hcc5hmC_cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lasso$fit.preval[,ndx_1se]) train_lasso_oofClass_vec &lt;- ifelse( train_lasso_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) ndx_1se &lt;- match(hcc5hmC_cv_enet$lambda.1se,hcc5hmC_cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_enet$fit.preval[,ndx_1se]) train_enet_oofClass_vec &lt;- ifelse( train_enet_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # RECALL: hcc5hmC_cv_lassoR$nzero[hcc5hmC_cv_lassoR$lambda==hcc5hmC_cv_lassoR$lambda.1se] # train - oof ndx_1se &lt;- match(hcc5hmC_cv_lassoR$lambda.1se,hcc5hmC_cv_lassoR$lambda) train_relaxed_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_relaxed_oofClass_vec &lt;- ifelse( train_relaxed_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # RECALL $`r hcc5hmC_cv_lassoR$relaxed$nzero.1se`$ features (vertical # cv_blended_statlist &lt;- hcc5hmC_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] # cv_blended_1se_error &lt;- cv_blended_statlist$cvm[cv_blended_statlist$lambda== #hcc5hmC_cv_lassoR$relaxed$lambda.1se] # train - oof cv_blended_statlist &lt;- hcc5hmC_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] ndx_1se &lt;- match(hcc5hmC_cv_lassoR$relaxed$lambda.1se, cv_blended_statlist$lambda) train_blended_oofProb_vec &lt;- logistic_f(hcc5hmC_cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_oofClass_vec &lt;- ifelse( train_blended_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) misclass_id_vec &lt;- unique(c( names(train_lasso_oofClass_vec)[train_lasso_oofClass_vec != hcc5hmC_train_group_vec], names(train_enet_oofClass_vec)[train_enet_oofClass_vec != hcc5hmC_train_group_vec], names(train_relaxed_oofClass_vec)[train_relaxed_oofClass_vec != hcc5hmC_train_group_vec], names(train_blended_oofClass_vec)[train_blended_oofClass_vec != hcc5hmC_train_group_vec] ) ) # Dont know why NAs creeo in here missclass_oofProb_mtx &lt;- cbind( train_lasso_oofProb_vec[misclass_id_vec], train_enet_oofProb_vec[misclass_id_vec], train_relaxed_oofProb_vec[misclass_id_vec], train_blended_oofProb_vec[misclass_id_vec] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(hcc5hmC_train_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(hcc5hmC_train_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Control&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 4.8: out-of-fold predicted probabilities at miscassified samples As we’ve seen above, predictions from lassoR and the blended mix model are basically dichotomous; 0 or 1. Samples have been order by group, and median P(HCC) within group. For the Controls (green), predicted probabilities less than 0.5 are considered correct here. For the HCC (red) samples, predicted probabilities greater than 0.5 are considered correct here. Now look at the same plot on the test data set. test_lasso_predClass_vec &lt;- predict( hcc5hmC_cv_lasso, newx=hcc5hmC_test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_enet_predClass_vec &lt;- predict( hcc5hmC_cv_enet, newx=hcc5hmC_test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_relaxed_predClass_vec &lt;- predict( hcc5hmC_cv_lassoR, g=0, newx=hcc5hmC_test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_blended_predClass_vec &lt;- predict( hcc5hmC_cv_lassoR, g=0.5, newx=hcc5hmC_test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) misclass_id_vec &lt;- unique(c( names(test_lasso_predClass_vec[,1])[test_lasso_predClass_vec != hcc5hmC_test_group_vec], names(test_enet_predClass_vec[,1])[test_enet_predClass_vec != hcc5hmC_test_group_vec], names(test_relaxed_predClass_vec[,1])[test_relaxed_predClass_vec != hcc5hmC_test_group_vec], names(test_blended_predClass_vec[,1])[test_blended_predClass_vec != hcc5hmC_test_group_vec] ) ) missclass_oofProb_mtx &lt;- cbind( test_lasso_predProb_vec[misclass_id_vec,], test_enet_predProb_vec[misclass_id_vec,], test_relaxed_predProb_vec[misclass_id_vec,], test_blended_predProb_vec[misclass_id_vec,] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(hcc5hmC_test_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(hcc5hmC_test_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Control&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 4.9: Test data predicted probabilities at miscassified samples The relaxed lasso fit results in essentially dichotomized predicted probability distribution - predicted probabilities are very close to 0 or 1. We see that for design points in the training set, the predicted probabilities from the relaxed lasso are essentially dichotomized to be tightly distributed at the extremes of the response range. For design points in the test set, the predicted probabilities from the relaxed lasso are comparable to the lasso model predicted probabilities. This seems to indicate over-fitting in the relaxed lasso fit. 4.6 Compare coefficient profiles # lasso ########################## # train - cv predicted lasso_coef &lt;- coef( hcc5hmC_cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) # enet ########################## enet_coef &lt;- coef( hcc5hmC_cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) # THESE ARE NOT CORRECT - SKIP # relaxed lasso (gamma=0) ########################## SKIP &lt;- function() { lassoR_coef &lt;- coef( hcc5hmC_cv_lassoR, s=&#39;lambda.1se&#39;, g=0 ) lassoR_coef_frm &lt;- data.frame( gene=lassoR_coef@Dimnames[[1]][c(1, lassoR_coef@i[-1])], lassoR=lassoR_coef@x) } # blended mix (gamma=0.5) ############################### blended_coef &lt;- coef( hcc5hmC_cv_lassoR, s=&#39;lambda.1se&#39;, g=0.5 ) blended_coef_frm &lt;- data.frame( gene=blended_coef@Dimnames[[1]][c(1, blended_coef@i[-1])], blended=blended_coef@x) # put it all together all_coef_frm &lt;- base::merge( x = lasso_coef_frm, y = base::merge( x = enet_coef_frm, y = blended_coef_frm, by=&#39;gene&#39;, all=T), by=&#39;gene&#39;, all=T) # SKIPPED #base::merge( #x = lassoR_coef_frm, #y = blended_coef_frm, #by=&#39;gene&#39;, all=T), all_coef_frm[,-1][is.na(all_coef_frm[,-1])] &lt;- 0 par(mfrow=c(ncol(all_coef_frm)-1,1), mar=c(0,5,0,1), oma=c(3,1,2,0)) for(CC in 2:ncol(all_coef_frm)) { plot( x=1:(nrow(all_coef_frm)-1), xlab=&#39;&#39;, y=all_coef_frm[-1, CC], ylab=colnames(all_coef_frm)[CC], type=&#39;h&#39;, xaxt=&#39;n&#39;) } Figure 4.10: Coefficient Profiles Note that there is little difference between the elastic net and the lasso in the selected features, and when the coefficient is zero in one set, it is small in the other. By contrast, the blended fit produces more shrinkage. knitr::kable( with(all_coef_frm[,-1], table(lassoZero=lasso==0, enetZero=enet==0)), caption=&#39;Zero Ceofficient: rows are lasso, columns enet&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.6: Zero Ceofficient: rows are lasso, columns enet FALSE FALSE 69 TRUE 92 Coefficients in the blended fit are larger than those in the lasso fit, or zero. We can also examine these with a scatter plot matrix. pairs(all_coef_frm[-1,-1], lower.panel = NULL, panel = function(x, y) { points(x, y, pch = 16, col = &quot;blue&quot;) } ) Figure 4.11: Coefficients from fits 4.7 Examine feature selection Recall from glmnet vignette: It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an $\\alpha$=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of $\\alpha$ is for numerical stability; for example, the *elastic net with $\\alpha = 1 - \\epsilon$ for some small $\\epsilon$&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations*. To see how this plays out in this dataset, we can look at feature expression heat maps. Reader notes: Heat maps are rarely useful other than to display the obvious. Here too heat maps fail to yield any insights, or confirmation of the relationship between feature correlation and lasso vs enet feature selection. suppressPackageStartupMessages(require(gplots)) # train - cv predicted lasso_coef &lt;- coef( hcc5hmC_cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(hcc5hmC_train_lcpm_mtx[,lasso_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=lasso_coef_frm$gene, labCol=hcc5hmC_train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(hcc5hmC_train_group_vec==&#39;Control&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;lasso genes - N =&#39;, nrow(lasso_coef_frm)-1)) Figure 4.12: Lasso Model Genes suppressPackageStartupMessages(require(gplots)) # train - cv predicted enet_coef &lt;- coef( hcc5hmC_cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(hcc5hmC_train_lcpm_mtx[,enet_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=enet_coef_frm$gene, labCol=hcc5hmC_train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(hcc5hmC_train_group_vec==&#39;Control&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;enet genes - N =&#39;, nrow(enet_coef_frm)-1)) Figure 4.13: Enet Model Genes "],
["hcc-5hmcseq-model-suite.html", "Section 5 HCC 5hmC-Seq: Sample size investigation 5.1 Full data set fit 5.2 Selected feature list stability 5.3 Simulation Design 5.4 Setup simulation 5.5 Run simulations 5.6 Simulation results 5.7 Effect of sample consistency", " Section 5 HCC 5hmC-Seq: Sample size investigation We now examine the results of fitting a suite of lasso models to investigate the effect of sample size on various aspects of model performance: assessed accuracy: out-of-fold estimates of precision and variability vs train set estimates selected feature profile stability - to what extent does the feature set implicitly selected by the lasso vary across random sampling and what is the effect of sample size. It is hypothesized that below a certain threshold, sample sizes are too small to provide reliable estimates of performance or stable selected feature profiles. Reader Note It&#39;s not clear how to display the effect of sample set composition on model performance, or that it merits special attention as it may be obvious to anyone who is paying attention that not all sample points are created equal and that having a substantial number of mislabeled or otherwise hard to classify samples can throw off estimates of the separability of subgroups when sample sizes are small. Keep this stuff on consistency for now. In examining the relationship between sample size and model performance, including variability and reliability of performance indicators derived from fits, we should bear in mind that in addition to sample size, sample composition might also play a factor. If a training data set contains many outliers or otherwise hard to properly classify samples, the performance of models fitted to such a data set is expected to be negatively impacted. In the simulations that we run below we will track sample consistency to examine its impact on fitted model performance. We use consistency to refer to a measure of how hard it is to appropriately classify individual samples. Predicted probabilities from fitted model can be transformed into sample consistency scores: \\(Q_i = p_i^{y_i}(1-p_i)^{1-y_i}\\), where \\(p_i\\) is the estimated probability of HCC for sample i and \\(y_i\\) is 1 for HCC samples and 0 for Controls. ie. we use the fitted sample contribution to the likelihood function as a sample consistency score, which is \\(P(HCC)\\), the predicted probability of HCC for HCC samples and \\(1 - P(HCC)\\) for Controls. To derive the sample consistency scores, we will use the predicted response from a lasso model fitted to the entire data set. Hard to classify samples will have low consistency scores. In the results that we discuss below, when we look at variability across repeated random sampling of different sizes, we can use sample consistency scores to investigate how much of the variability is due to sample selection. Note that consistency here is not used to say anything about the sample data quality. Low consistency here only means that a sample is different from the core of the data set in a way that makes it hard to properly classify. That could happen if the sample were mislabeled, in which case we could think of this sample as being poor quality of course. The variability in sample set consistency that we get from simple random sampling, as is done in the simulation below, is not expected to adequately reflect sample set consistency variation encountered in practice when data for a particular study are accumulated over extensive time horizons and across varied physical settings. In order to accrue study samples at an acceptable rate, it is not uncommon for the study sponsor to work with several clinics, medical centers, or other tissue or blood sample provider. Or the sponsor may sub-contract the sample acquisition task to a third party who may have suppliers distributed across the globe. This is great news for the rate of sample acquisition, but not such great news for the uniformity of consistency of samples. In such a context, the variability in sample set consistency as the data set grows over time would not be adequately captured by simple random sampling variability; the variability would be more akin to cluster sampling with potential confounding batch effects. The impact that these effects can have on the classification analysis results cannot be understated, especially in the context of a new technology that is exploiting biological processes that are still not fully understood. All this to make the point that the variability the we are studying here has to be regarded as a lower bound on the variability that is to be expected in practice, and that without an external validation data set to verify results, one should be cautious when interpreting empirical findings, especially in the absence of solid biological underpinnings. Reader Note We still need to demonstrate the effect of sample set composition on results. With random sampling the effects are subtle and not immediately obvious. We may have to fabricate sample sets with low consistency scores - ie. made up of many hard to classify samples - in order to show this impact. 5.1 Full data set fit We begin by fitting a model to the entire data set in order to: obtain a baseline classification performance against which to judge the performance obtained from the fits to smaller sample sets, obtain sample consistency scores which can be used to explain variability in the performance of model fitted to data sets of a fixed size, and produce a full model gene signature which can be used to evaluate the stability of selected features in models fitted to data sets of different sizes. First assemble the data set. This entails simply re-combining the train and test data. # combine train and test hcc5hmC_all_lcpm_mtx &lt;- rbind(hcc5hmC_train_lcpm_mtx, hcc5hmC_test_lcpm_mtx) # we have to be careful with factors! # We&#39;ll keep as a character and change to factor when needed hcc5hmC_all_group_vec &lt;- c( as.character(hcc5hmC_train_group_vec), as.character(hcc5hmC_test_group_vec) ) # I suspect adding names to vectors breaks one of the tidy commandments, # but then again I am sure I have already offended the creed beyond salvation names(hcc5hmC_all_group_vec) &lt;- c( names(hcc5hmC_train_group_vec), names(hcc5hmC_test_group_vec) ) knitr::kable(table(group = hcc5hmC_all_group_vec), caption = &quot;samples by group&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 5.1: samples by group group Freq Control 778 HCC 555 Now fit the lasso model through cross-validation. Note that the results of a cv fit are random due to the random allocation of samples to folds. We can reduce this variability by properly averaging results over repeated cv fits. Here we will obtain sample consistency scores by averaging results over 30 cv runs. set.seed(1) start_time &lt;- proc.time() hcc5hmC_cv_lassoAll_lst &lt;- lapply(1:30, function(REP) { glmnet::cv.glmnet( x = hcc5hmC_all_lcpm_mtx, y = factor(hcc5hmC_all_group_vec,levels = c(&#39;Control&#39;, &#39;HCC&#39;)), alpha = 1, family = &#39;binomial&#39;, type.measure = &quot;class&quot;, keep = T, nlambda = 100 ) } ) message(&quot;lassoAll time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) Examine the fits. ### CLEAR CACHE plot( log(hcc5hmC_cv_lassoAll_lst[[1]]$lambda), hcc5hmC_cv_lassoAll_lst[[1]]$cvm, lwd=2, xlab=&#39;log(Lambda)&#39;, ylab=&#39;CV Misclassification Error&#39;, type=&#39;l&#39;, ylim=c(0, .5) ) for(JJ in 2:length(hcc5hmC_cv_lassoAll_lst)) lines( log(hcc5hmC_cv_lassoAll_lst[[JJ]]$lambda), hcc5hmC_cv_lassoAll_lst[[JJ]]$cvm, lwd=2 ) Figure 5.1: Repeated cv lasso models fitted to all samples These cv curves are remarkably consistent meaning that the determination of the size or sparsity of the model fitted through cross validation to the full data set is fairly precise: library(magrittr) par(mfrow=c(1,2), mar=c(3,4, 2, 1)) # nzero nzero_1se_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_min_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) boxplot(list(`1se`=nzero_1se_vec, min = nzero_min_vec), ylab=&quot;Selected Features&quot;) # error error_1se_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_min_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) boxplot( list(`1se`=error_1se_vec, min = error_min_vec), ylab=hcc5hmC_cv_lassoAll_lst[[1]]$name, ylim=c(0.06, .10) ) Figure 5.2: Feature selection and estimated error by repeated cv lasso models # tabular format tmp &lt;- data.frame(rbind( `features_1se` = summary(nzero_1se_vec), features_min = summary(nzero_min_vec), `features:min-1se` = summary(nzero_min_vec - nzero_1se_vec), `cv_error_1se` = summary(100*error_1se_vec), cv_error_min = summary(100*error_min_vec), `cv_error:1se-min` = summary(100*(error_1se_vec-error_min_vec)) )) knitr::kable(tmp %&gt;% dplyr::select(-Mean), caption = &quot;Number of selected features&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 5.2: Number of selected features Min. X1st.Qu. Median X3rd.Qu. Max. features_1se 54.0 102.0 108.0 132.5 194.0 features_min 118.0 175.2 200.0 261.0 365.0 features:min-1se 10.0 55.0 86.0 141.5 247.0 cv_error_1se 6.9 7.6 7.7 8.1 8.9 cv_error_min 6.7 7.1 7.1 7.3 8.0 cv_error:1se-min 0.2 0.5 0.6 0.7 1.0 The number of features selected by the minimum lambda models are larger than the number selected by the “one standard error” rule models by a median of 86. The cv error rates obtained from the minimum lambda models are lower then “one standard error” rule models error rates by a median of 0.6%. The cv error rates observed in this set are comparable to the rates observed in the lasso models fitted to the training sample set which consisted of 80% of the samples in this set. In other words, there is no obvious gain in performance in moving from a data set with 622 vs 444 samples to a data set with 778 vs 555 samples. See Table ??. It’s not clear at this point whether the minimum lambda model is truly better than the “one standard error” rule model. We would need and external validation set to make this determination. We can compare the two sets of out-of-fold predicted values, averaged across cv replicates, to see if there is a meaningful difference between the two. # predicted probs - 1se lassoAll_predResp_1se_mtx &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) { ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_1se]) }) lassoAll_predResp_1se_vec &lt;- rowMeans(lassoAll_predResp_1se_mtx) # predicted probs - min lassoAll_predResp_min_mtx &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) { ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_min]) }) lassoAll_predResp_min_vec &lt;- rowMeans(lassoAll_predResp_min_mtx) # plot par(mfrow=c(1,2), mar=c(5,5,2,1)) tmp &lt;- c( `1se` = split(lassoAll_predResp_1se_vec, hcc5hmC_all_group_vec), min = split(lassoAll_predResp_min_vec, hcc5hmC_all_group_vec) ) names(tmp) &lt;- sub(&#39;\\\\.&#39;, &#39;\\n&#39;, names(tmp)) boxplot( tmp, ylab=&#39;Predicted oof probability&#39;, border=c(&#39;green&#39;, &#39;red&#39;), xaxt=&#39;n&#39; ) axis(side=1, at=1:length(tmp), tick=F, names(tmp)) # compare the two plot( x = lassoAll_predResp_1se_vec, xlab=&#39;1se model oof Prob&#39;, y = lassoAll_predResp_min_vec, ylab=&#39;min lambda model oof Prob&#39;, col = ifelse(hcc5hmC_all_group_vec == &#39;HCC&#39;, &#39;red&#39;, &#39;green&#39;) ) # Add referecne lines at 10% false positive thres_1se &lt;- quantile(lassoAll_predResp_1se_vec[hcc5hmC_all_group_vec == &#39;Control&#39;], prob=.9) thres_min &lt;- quantile(lassoAll_predResp_min_vec[hcc5hmC_all_group_vec == &#39;Control&#39;], prob=.9) abline(v = thres_1se, h = thres_min, col=&#39;grey&#39;) Figure 5.3: Predicted probabilities - averaged over cv replicates We see that there isn’t a big difference in out-of-fold predicted probabilities between the one-standard-error rule and the minimum lambda models. One way to quantify the difference in classification errors is to classify samples according to each vector of predicted probabilities, setting the thresholds to achieve a fixed false positive rate, 10% say. These thresholds are indicated by the grey lines in the scatter plot on the right side of Figure ??. lassoAll_predClass_1se_vec &lt;- ifelse( lassoAll_predResp_1se_vec &gt; thres_1se, &#39;HCC&#39;, &#39;Control&#39;) lassoAll_predClass_min_vec &lt;- ifelse( lassoAll_predResp_min_vec &gt; thres_min, &#39;HCC&#39;, &#39;Control&#39;) tmp &lt;- cbind( table(truth=hcc5hmC_all_group_vec, `1se-pred`=lassoAll_predClass_1se_vec), table(truth=hcc5hmC_all_group_vec, `min-pred`=lassoAll_predClass_min_vec) ) # Hack for printing colnames(tmp) &lt;- c(&#39;1se-Control&#39;, &#39;1se-HCC&#39;, &#39;min-Control&#39;, &#39;min-HCC&#39;) knitr::kable(tmp, caption = &quot;Classifications: rows are truth&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 5.3: Classifications: rows are truth 1se-Control 1se-HCC min-Control min-HCC Control 700 78 700 78 HCC 39 516 32 523 When we fix the false positive rate at 10% (ie. the control samples error rates are fixed), the 1se model makes 39 false negative calls whereas the minimum lambda model makes 32. A difference of 1.3% Get sample consistency scores To compute consistency scores, we will use the out-of-fold predicted probabilities. # get qual scores y &lt;- as.numeric(hcc5hmC_all_group_vec == &#39;HCC&#39;) # 1se p &lt;- lassoAll_predResp_1se_vec hcc5hmC_sample_1se_qual_vec &lt;- p^y*(1-p)^(1-y) # min p &lt;- lassoAll_predResp_min_vec hcc5hmC_sample_min_qual_vec &lt;- p^y*(1-p)^(1-y) We can examine consistency scores as a function of classification bin. y &lt;- as.numeric(hcc5hmC_all_group_vec == &#39;HCC&#39;) # 1se lassoAll_1se_conf_vec &lt;- paste( y, as.numeric(lassoAll_predClass_1se_vec==&#39;HCC&#39;), sep = &#39;:&#39; ) # min lassoAll_min_conf_vec &lt;- paste( y, as.numeric(lassoAll_predClass_min_vec==&#39;HCC&#39;), sep = &#39;:&#39; ) tmp &lt;- c( split(hcc5hmC_sample_1se_qual_vec, lassoAll_1se_conf_vec), split(hcc5hmC_sample_min_qual_vec, lassoAll_min_conf_vec) ) par(mfrow=c(1,2), mar=c(4,3,3,2), oma=c(2,2,2,0)) gplots::boxplot2(split(hcc5hmC_sample_1se_qual_vec, lassoAll_1se_conf_vec), outline=F, ylab = &#39;&#39;, border=c(&#39;green&#39;, &#39;green&#39;, &#39;red&#39;, &#39;red&#39;), ylim=c(0,1)) title(&#39;1se Model&#39;) gplots::boxplot2(split(hcc5hmC_sample_min_qual_vec, lassoAll_min_conf_vec), outline=F, ylab = &#39;&#39;, border=c(&#39;green&#39;, &#39;green&#39;, &#39;red&#39;, &#39;red&#39;), ylim=c(0,1)) title(&#39;min lambda Model&#39;) mtext(side=1, outer=T, cex=1.5, &#39;Classification - Truth:Predicted&#39;) mtext(side=2, outer=T, cex=1.5, &#39;Consistency Score&#39;) mtext(side=3, outer=T, cex=1.5, &#39;Sample Quality vs Classification Outcome&#39;) Figure 5.4: quality scores by classification - Control=0, HCC=1 This figure shows that for false positive cases (0:1 or classifying a control as an affected case), the algorithm is less certain of its predicted outcome than for the false negative cases (1:0 or classifying an affected case as a control). ie. the misclassified HCC samples are quite similar to Controls, whereas there is more ambiguity in the misclassified Control samples. We will use the minimum lambda model to provide the fitted probabilities used to compute quality scores, but we could have used either one. sample_qual_vec &lt;- hcc5hmC_sample_min_qual_vec 5.2 Selected feature list stability Before moving on to the simulation, let’s examine gene selection stability on the full data set. We have two sets of selected features - one for the one standard deviation rule model, and one for the minimum lambda model. We saw in Table ?? that the number of features selected by the minimum lambda models had an IQR of 175.25-261, while the one standard error rule models had an IQR of 102-132.5. Let’s examine the stability of the gene lists across cv replicates. ### CLEAR CACHE # 1se lassoAll_coef_1se_lst &lt;- lapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix lassoAll_coef_1se_all &lt;- Reduce(union, lassoAll_coef_1se_lst) lassoAll_coef_1se_mtx &lt;- sapply(lassoAll_coef_1se_lst, function(LL) is.element(lassoAll_coef_1se_all, LL) ) rownames(lassoAll_coef_1se_mtx) &lt;- lassoAll_coef_1se_all genes_by_rep_1se_tbl &lt;- table(rowSums(lassoAll_coef_1se_mtx)) barplot( genes_by_rep_1se_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 5.5: Feature list stability for one standard error rule models We see that 44 features are included in every cv replicate. These make up between 33% and 43% (Q1 and Q3) of the cv replicate one standard error rule models feature lists. ### CLEAR CACHE # min lassoAll_coef_min_lst &lt;- lapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix lassoAll_coef_min_all &lt;- Reduce(union, lassoAll_coef_min_lst) lassoAll_coef_min_mtx &lt;- sapply(lassoAll_coef_min_lst, function(LL) is.element(lassoAll_coef_min_all, LL) ) rownames(lassoAll_coef_min_mtx) &lt;- lassoAll_coef_min_all genes_by_rep_min_tbl &lt;- table(rowSums(lassoAll_coef_min_mtx)) barplot( genes_by_rep_min_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 5.6: Feature list stability for minimum lambda models We see that 106 features are included in every cv replicate. These make up between 41% and 60% (Q1 and Q3) of the cv replicate min feature lists. We will consider the genes that are selected in all cv replicates as a gene signature produced by each model. lasso_gene_sign_1se_vec &lt;- rownames(lassoAll_coef_1se_mtx)[rowSums(lassoAll_coef_1se_mtx)==30] lasso_gene_sign_min_vec &lt;- rownames(lassoAll_coef_min_mtx)[rowSums(lassoAll_coef_min_mtx)==30] 44 out of 44 of the genes in the 1se model gene signature are contained in the min lambda model gene signature. 5.3 Simulation Design We are now ready to run the simulations. SIM &lt;- 30 SIZE &lt;- c(25, 50, 100, 200, 300) CV_REP &lt;- 30 Simluation parameters: Number of simulations : SIM = 30 Sample sizes: SIZE = 25, 50, 100, 200, 300 Number of CV Replicates: CV_REP = 30 We will repeat the simulation process SIM = 30 times. For each simulation iteration, we will select 300 Control and 300 HCC samples at random. Models will be fitted and analyzed to balanced subsets of SIZE = 25, 50, 100, 200, 300, in a Matryoshka doll manner to emulate a typical sample accrual process. Note that in this accrual process there is no time effect - the accrual process is completely randomized. In practice, there could be significant time effects. For example, the first 25 HCC samples could come from Center A, while the next 25 could come from Center B. And affected and control samples could be acquired from different clinics or in different time intervals. In other words, there is no batch effect or shared variability in our simulation, while these are almost always present in real data, including batch effects that are associated with class labels - controls being in different batches than affected samples is an all too common occurrence, for example. One should be especially watchful of potential batch effects when dealing with blood samples as blood is notoriously finicky in character [[16]; [17];]. Presented with results that look impressively good based on a small data set, one should definitely be skeptical of the promise of future equally good results. For a given simulation and a given sample size, we will obtain CV_REP = 30 cross-validated lasso fits. From these fits, we can obtain 30 out-of-fold assessments of classification accuracy to get a sense if its variability. From each cv replicate, we also obtain an estimated model size and a set of selected features. We will want to examine how these stabilize as the sample size increases. Note that we limit the simulations to a maximum of sample size of 300 in order to to have simulations with low overlap. With 300 randomly selected HCC samples, the expected overlap between two randomly selected sets of HCC samples is 29.2%. For Controls the expected overlap is 14.9%. Reader Note We are currently not following individual performance paths as a study set grows over time. We currently simply summarize results across simulations. Exmaining individual paths would show how chaotic the assessments of performance can be when sample sizes are small. To Do. 5.4 Setup simulation To setup the simulation, we only need two master tables: one for the selection of Controls and one for the selection of HCC samples. hcc5hmC_all_control_vec &lt;- names(hcc5hmC_all_group_vec[hcc5hmC_all_group_vec==&#39;Control&#39;]) hcc5hmC_all_affected_vec &lt;- names(hcc5hmC_all_group_vec[hcc5hmC_all_group_vec==&#39;HCC&#39;]) We have 778 control sample IDs stored in hcc5hmC_all_control_vec and 555 affected sample IDs stored in hcc5hmC_all_affected_vec. To create a suite of random samples from these, we only need to randomly select indices from each vector. set.seed(12379) hcc5hmC_sim_control_mtx &lt;- sapply( 1:SIM, function(dummy) sample(1:length(hcc5hmC_all_control_vec), size = max(SIZE)) ) hcc5hmC_sim_affected_mtx &lt;- sapply( 1:SIM, function(dummy) sample(1:length(hcc5hmC_all_affected_vec), size = max(SIZE)) ) Each simulation is specified by a given column of the simulation design matrices: hcc5hmC_sim_control_mtx and hcc5hmC_sim_affected_mtx, each with dimensions 300, 30. Within each simulation, we can run the analyses of size 25, 50, 100, 200, 300 by simply selecting samples specified in the appropriate rows of each design matrix. We can examine how much variability we have in the quality scores of the selected samples. Here we show results for the small sample sizes where variability will be the greatest. ### CLEAR CACHE all_control_qual_vec &lt;- sample_qual_vec[hcc5hmC_all_control_vec] sim_control_qual_mtx &lt;- sapply( 1:ncol(hcc5hmC_sim_control_mtx), function(CC) all_control_qual_vec[hcc5hmC_sim_control_mtx[,CC]] ) all_affected_qual_vec &lt;- sample_qual_vec[hcc5hmC_all_affected_vec] sim_affected_qual_mtx &lt;- sapply( 1:ncol(hcc5hmC_sim_affected_mtx), function(CC) all_affected_qual_vec[hcc5hmC_sim_affected_mtx[,CC]] ) # ONLY LOOK AT SAMPLE SIZE == 50 NN &lt;- 50 # PLOT par(mfrow=c(2,1), mar = c(2,5,2,1)) # control boxplot( sim_control_qual_mtx[1:NN,], outline = T, ylab = &#39;Quality Score&#39;, xaxt = &#39;n&#39; ) title(&quot;Control sample consistency across simulations&quot;) # affected boxplot( sim_affected_qual_mtx[1:NN,], outline = T, ylab = &#39;Quality Score&#39; ) title(&quot;Affected sample consistency across simulations&quot;) Figure 5.7: sample consistency by simulation run for size = 50 In this figure, we are summarizing the quality measures of 50 samples per group across 30 simulations, or random selections of control and affected samples. We see significant variability in sample consistency, especially in the affected cases. This may lead an unwary observer to be overly optimistic, or overly pessimistic, in the early accrual stages of a study. 5.5 Run simulations As these take a while to run, we will save the results of each simulation to a different object and store to disk. These can be easily read from disk when needed for analysis. The simulation results are saved to the file system and only needs to be run once. The simulation takes \\(\\approx\\) 8 to 10 minutes per iteration, or 4 to 5 hours of run time on a laptop. (Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Mojave 10.14.6) start_time &lt;- proc.time() # Get stage from SIZE stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0, SIZE), include.lowest = T) for (SIMno in 1:ncol(sim_control_qual_mtx)) { #cat(&quot;Running simulation &quot;, SIMno, &quot;\\n&quot;) sim_cv_lst &lt;- lapply(1:length(levels(stage_vec)), function(STGno) { Stage_rows_vec &lt;- which(stage_vec %in% levels(stage_vec)[1:STGno]) #cat(&quot;Stage &quot;, STGno, &quot;- analyzing&quot;, length(Stage_rows_vec), &quot;paired samples.\\n&quot;) sim_stage_samples_vec &lt;- c( hcc5hmC_all_control_vec[hcc5hmC_sim_control_mtx[Stage_rows_vec, SIMno]], hcc5hmC_all_affected_vec[hcc5hmC_sim_affected_mtx[Stage_rows_vec, SIMno]] ) sim_stage_lcpm_mtx &lt;- hcc5hmC_all_lcpm_mtx[sim_stage_samples_vec, ] sim_stage_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_samples_vec] #print(table(sim_stage_group_vec)) sim_stage_cv_lst &lt;- lapply(1:CV_REP, function(CV) { cv_fit &lt;- glmnet::cv.glmnet( x = sim_stage_lcpm_mtx, y = sim_stage_group_vec, alpha = 1, family = &quot;binomial&quot;, type.measure = &quot;class&quot;, keep = T, nlambda = 30 ) # Extract 1se metrics from cv_fit ####################### ndx_1se &lt;- which(cv_fit$lambda == cv_fit$lambda.1se) nzero_1se &lt;- cv_fit$nzero[ndx_1se] cvm_1se &lt;- cv_fit$cvm[ndx_1se] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(hcc5hmC_all_lcpm_mtx), sim_stage_samples_vec) sim_stage_hcc5hmC_test_lcpm_mtx &lt;- hcc5hmC_all_lcpm_mtx[sim_stage_test_samples_vec,] sim_stage_hcc5hmC_test_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_test_samples_vec] test_pred_1se_vec &lt;- predict( cv_fit, newx=sim_stage_hcc5hmC_test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_1se_error &lt;- mean(test_pred_1se_vec != sim_stage_hcc5hmC_test_group_vec) # genes coef_1se &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) genes_1se &lt;- coef_1se@Dimnames[[1]][coef_1se@i[-1]] # Extract min metrics from cv_fit ####################### ndx_min &lt;- which(cv_fit$lambda == cv_fit$lambda.min) nzero_min &lt;- cv_fit$nzero[ndx_min] cvm_min &lt;- cv_fit$cvm[ndx_min] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(hcc5hmC_all_lcpm_mtx), sim_stage_samples_vec) sim_stage_hcc5hmC_test_lcpm_mtx &lt;- hcc5hmC_all_lcpm_mtx[sim_stage_test_samples_vec,] sim_stage_hcc5hmC_test_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_test_samples_vec] test_pred_min_vec &lt;- predict( cv_fit, newx=sim_stage_hcc5hmC_test_lcpm_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_min_error &lt;- mean(test_pred_min_vec != sim_stage_hcc5hmC_test_group_vec) # genes coef_min &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) genes_min &lt;- coef_min@Dimnames[[1]][coef_min@i[-1]] # return cv_fit summary metrics list( p_1se = nzero_1se, p_min = nzero_min, cv_1se = cvm_1se, cv_min = cvm_min, test_1se=test_1se_error, test_min=test_min_error, genes_1se = genes_1se, genes_min = genes_min) }) sim_stage_cv_lst }) # save sim_cv_lst fName &lt;- paste0(&quot;hcc5hmC_sim_&quot;, SIMno, &quot;_cv_lst&quot;) assign(fName, sim_cv_lst) save(list = fName, file=file.path(&quot;RData&quot;, fName)) } 5.6 Simulation results Recall the we have 30 simulations, or randomly selected sets of HCC and Control samples, analyzed in increasing sizes of 25, 50, 100, 200, 300, with 30 repeated cross-validated lasso fits: Sample sizes: SIZE = 25, 50, 100, 200, 300 Number of CV Replicates: CV_REP = 30 First we extract simulation results and store into one big table (only showing the top of table here): ### CLEAR CACHE sim_files_vec &lt;- list.files(&#39;RData&#39;, &#39;^hcc5hmC_sim_&#39;) # define extraction methods # Each sumulation is a list of cv results ## nested in a list of replicates ############################################## # cvList2frm_f makes a frame out of the inner list cvList2frm_f &lt;- function(cv_lst) { frm1 &lt;- as.data.frame(t(sapply(cv_lst, function(x) x))) frm2 &lt;- data.frame( unlist(frm1[[1]]), unlist(frm1[[2]]), unlist(frm1[[3]]), unlist(frm1[[4]]), unlist(frm1[[5]]), unlist(frm1[[6]]), frm1[7], frm1[8]) names(frm2) &lt;- names(frm1) data.frame(Rep=1:nrow(frm2), frm2)} # cv_lst_to_frm loop over replicates, concatenating the inner list frames cv_lst_to_frm &lt;- function(sim_cv_lst) { do.call(&#39;rbind&#39;, lapply(1:length(sim_cv_lst), function(JJ) { siz_frm &lt;- cvList2frm_f(sim_cv_lst[[JJ]]) data.frame(Size=SIZE[JJ], siz_frm) })) } # we loop across simulations to combine all results into one big table hcc5hmC_lasso_sim_results_frm &lt;- do.call(&#39;rbind&#39;, lapply(1:length(sim_files_vec), function(SIM_NO) { load(file=file.path(&#39;RData&#39;, sim_files_vec[SIM_NO])) assign(&#39;sim_cv_lst&#39;, get(sim_files_vec[SIM_NO])) rm(list=sim_files_vec[SIM_NO]) data.frame(SimNo=paste0(&#39;Sim_&#39;,formatC(SIM_NO,width = 2,flag = 0)), cv_lst_to_frm(sim_cv_lst)) } )) ### CLEAR CACHE knitr::kable(head(with(hcc5hmC_lasso_sim_results_frm, table(SimNo, Size))), caption = paste(&quot;Simulation Results - N Sim =&quot;, SIM)) %&gt;% kableExtra::kable_styling(full_width = F) Table 5.4: Simulation Results - N Sim = 30 25 50 100 200 300 Sim_01 30 30 30 30 30 Sim_02 30 30 30 30 30 Sim_03 30 30 30 30 30 Sim_04 30 30 30 30 30 Sim_05 30 30 30 30 30 Sim_06 30 30 30 30 30 knitr::kable(head(hcc5hmC_lasso_sim_results_frm) %&gt;% dplyr::select(-c(genes_1se, genes_min)), caption = paste(&quot;Simulation Results - not showing genes column&quot;), digits=2) %&gt;% kableExtra::kable_styling(full_width = F) Table 5.4: Simulation Results - not showing genes column SimNo Size Rep p_1se p_min cv_1se cv_min test_1se test_min Sim_01 25 1 20 40 0.32 0.26 0.30 0.31 Sim_01 25 2 26 26 0.22 0.22 0.29 0.29 Sim_01 25 3 6 32 0.34 0.30 0.35 0.31 Sim_01 25 4 10 24 0.36 0.30 0.32 0.29 Sim_01 25 5 27 32 0.26 0.22 0.30 0.31 Sim_01 25 6 20 27 0.36 0.32 0.30 0.30 5.6.1 Simulation Results - look at one simulation 5.6.1.1 Model Accuracy Assessment First examine results for one simulation run. In the figures that follow, each boxplot summarized 30 repeated cross validation runs performed on a fixed random selection of Control and Affected samples. Recall that as we move from 25 to 50, etc., the sample sets are growing to emulate an accrual of samples over time. ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(hcc5hmC_lasso_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) # errors par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) ################### # 1se #################### cv_1se_lst &lt;- with(simNo_results_frm, split(cv_1se, Size)) names(cv_1se_lst) &lt;- paste0(stringr::str_pad(names(cv_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_1se_lst &lt;- with(simNo_results_frm, split(test_1se, Size)) names(test_1se_lst) &lt;- paste0(stringr::str_pad(names(test_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_1se_lst &lt;- c(cv_1se_lst, test_1se_lst) error_1se_lst &lt;- error_1se_lst[order(names(error_1se_lst))] boxplot(error_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 0.5), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_1se_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(affected_qual_vec, 2) ) }#SKIP # min #################### cv_min_lst &lt;- with(simNo_results_frm, split(cv_min, Size)) names(cv_min_lst) &lt;- paste0(stringr::str_pad(names(cv_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_min_lst &lt;- with(simNo_results_frm, split(test_min, Size)) names(test_min_lst) &lt;- paste0(stringr::str_pad(names(test_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_min_lst &lt;- c(cv_min_lst, test_min_lst) error_min_lst &lt;- error_min_lst[order(names(error_min_lst))] boxplot(error_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 0.5), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(affected_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 5.8: lasso Model Errors by Sample Size In this one simulation, we see: Model accuracy increases with sample size, with minimal improvement going from N=200 to N=300. CV error rates tend to be pessimistic, especially for the small sample sizes. This is odd and may be related to sample consistency. There isn’t much to chose from between the one standard error and the minimum lambda models. The latter may show lower propensity to produce optimistic cv error rates. 5.6.1.2 Feature Selection ### CLEAR CACHE # get full model nzero ref nzero_1se_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_1se_q2 &lt;- quantile(nzero_1se_vec, prob=c(2)/4) nzero_min_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) nzero_min_q2 &lt;- quantile(nzero_min_vec, prob=c(2)/4) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(hcc5hmC_lasso_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0)) ################### # 1se #################### # selected feature counts p_1se_lst &lt;- with(simNo_results_frm, split(p_1se, Size)) names(p_1se_lst) &lt;- paste0(stringr::str_pad(names(p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of lasso_gene_sign_1se_vec # - the signature selected genes sign_genes_1se_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_1se&#39;]), lasso_gene_sign_1se_vec)) sign_p_1se_lst &lt;- split(sapply(sign_genes_1se_lst, length), simNo_results_frm$Size) names(sign_p_1se_lst) &lt;- paste0(stringr::str_pad(names(sign_p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_1se_lst &lt;- c(p_1se_lst, sign_p_1se_lst) p_singP_1se_lst &lt;- p_singP_1se_lst[order(names(p_singP_1se_lst))] boxplot(p_singP_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;number of selected features&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(affected_qual_vec, 2) ) }#SKIP ################### # min #################### # selected feature counts p_min_lst &lt;- with(simNo_results_frm, split(p_min, Size)) names(p_min_lst) &lt;- paste0(stringr::str_pad(names(p_min_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of lasso_gene_sign_min_vec # - the signature selected genes sign_genes_min_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_min&#39;]), lasso_gene_sign_min_vec)) sign_p_min_lst &lt;- split(sapply(sign_genes_min_lst, length), simNo_results_frm$Size) names(sign_p_min_lst) &lt;- paste0(stringr::str_pad(names(sign_p_min_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_min_lst &lt;- c(p_min_lst, sign_p_min_lst) p_singP_min_lst &lt;- p_singP_min_lst[order(names(p_singP_min_lst))] boxplot(p_singP_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(affected_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 5.9: lasso Models Selected Features by Sample Size In this one simulation, we see: The selected number of features in the smaller sample size analyses are low, with few features belonging to the core signature identified in the full data set. As the sample size increases the number of features selected in the minimum lambda model remains variable, but the number of core signature features selected in the samples of sizes 200 and 300 is stable and between 40 and 50. 5.6.2 Summarize results across simulation runs. 5.6.2.1 Model Accuracy Assessment Now look across all simulations. In the figures that follow, each boxplot summarizes the results of 30 simulations. For a give sample size and a given simulation, each data point is the median across 30 repeated cv runs. ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) # 1se ######################################### ## cv cv_1se_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_1se_lst &lt;- with(sizeVal_results_frm, split(cv_1se, SimNo)) sapply(sizeVal_cv_1se_lst, median) }) names(cv_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_1se_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_1se_lst &lt;- with(sizeVal_results_frm, split(test_1se, SimNo)) sapply(sizeVal_test_1se_lst, median) }) names(test_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_1se_Bysize_lst &lt;- c(cv_1se_Bysize_lst, test_1se_Bysize_lst) error_1se_Bysize_lst &lt;- error_1se_Bysize_lst[order(names(error_1se_Bysize_lst))] boxplot(error_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, .5), outline=F, xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) for(JJ in 1:length(error_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(error_1se_Bysize_lst[[JJ]])), amount=0.25), y=error_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;)) # min ######################################### ## cv cv_min_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_min_lst &lt;- with(sizeVal_results_frm, split(cv_min, SimNo)) sapply(sizeVal_cv_min_lst, median) }) names(cv_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_min_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_min_lst &lt;- with(sizeVal_results_frm, split(test_min, SimNo)) sapply(sizeVal_test_min_lst, median) }) names(test_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_min_Bysize_lst &lt;- c(cv_min_Bysize_lst, test_min_Bysize_lst) error_min_Bysize_lst &lt;- error_min_Bysize_lst[order(names(error_min_Bysize_lst))] boxplot(error_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, .5), outline=F, xaxt=&#39;n&#39; ) for(JJ in 1:length(error_min_Bysize_lst)) points( x=jitter(rep(JJ, length(error_min_Bysize_lst[[JJ]])), amount=0.25), y=error_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;lasso fit error rates summarized across simulations&#39;)) Figure 5.10: lasso Model Errors by Sample Size For the smaller samples sizes, cv error rates for the minimum lambda models tend to be optimistic. For lower sample sizes, assess performance is quite variable. This is key: with N=25 cases and controls, the estimated classification error rate produced by a given cohort can range from below 0.2 to higher than 0.5, or basically no indication of discrimination value in the analyzed features. To appreciate how much variability can be encountered as samples are accrued over time we need to look at a typical path the assessed model accuracy estimates might take. ### CLEAR CACHE error_1se_Bysize_mtx &lt;- do.call(&#39;cbind&#39;, lapply(error_1se_Bysize_lst, function(LL) LL)) cv_error_1se_Bysize_mtx &lt;- error_1se_Bysize_mtx[,grep(&#39;_cv&#39;, colnames(error_1se_Bysize_mtx))] plot(x=c(1, ncol(cv_error_1se_Bysize_mtx)), y=c(0,0.6), xlab=&#39;sample size&#39;, ylab=&#39;Misclassification Error&#39;, type=&#39;n&#39;, xaxt=&#39;n&#39;) axis(side=1, at=1:ncol(cv_error_1se_Bysize_mtx), labels=sub(&#39;_cv&#39;,&#39;&#39;,colnames(cv_error_1se_Bysize_mtx))) for(JJ in 1:15) lines(x=1:ncol(cv_error_1se_Bysize_mtx), y=cv_error_1se_Bysize_mtx[JJ,], type=&#39;b&#39;, pch=JJ, col=JJ) title(&#39;Example Misclassification Error Paths&#39;) Figure 5.11: lasso Model Error Paths We see how erratic the assessed model accuracy can be when sample sizes are small, and that it would be hard to guess the ultimate level of accuracy the is achievable, or the number of samples required to get a reasonable estimate of the achievable level of accuracy. 5.6.2.2 Feature Selection ### CLEAR CACHE # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) # 1se ######################################### # selected features p_1se_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_1se_lst &lt;- with(sizeVal_results_frm, split(p_1se, SimNo)) sapply(sizeVal_p_1se_lst, median) }) names(p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_1se_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_1se_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_1se&#39;]), lasso_gene_sign_1se_vec)) sizeVal_sign_p_1se_lst &lt;- split(sapply(sizeVal_sign_genes_1se_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_1se_lst, median) }) names(sign_p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_1se_Bysize_lst &lt;- c(p_1se_Bysize_lst, sign_p_1se_Bysize_lst) p_singP_1se_Bysize_lst &lt;- p_singP_1se_Bysize_lst[order(names(p_singP_1se_Bysize_lst))] boxplot(p_singP_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;number of selected features&#39;) for(JJ in 1:length(p_singP_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_1se_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lamdba models&#39;)) # min ######################################### # selected features p_min_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_min_lst &lt;- with(sizeVal_results_frm, split(p_min, SimNo)) sapply(sizeVal_p_min_lst, median) }) names(p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_min_Bysize_lst &lt;- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- hcc5hmC_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_min_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_min&#39;]), lasso_gene_sign_min_vec)) sizeVal_sign_p_min_lst &lt;- split(sapply(sizeVal_sign_genes_min_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_min_lst, median) }) names(sign_p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_min_Bysize_lst &lt;- c(p_min_Bysize_lst, sign_p_min_Bysize_lst) p_singP_min_Bysize_lst &lt;- p_singP_min_Bysize_lst[order(names(p_singP_min_Bysize_lst))] boxplot(p_singP_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) for(JJ in 1:length(p_singP_min_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_min_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;lasso fit feature selection summarized across simulations&#39;)) Figure 5.12: lasso Models Selected Features by Sample Size The number of selected features increase with sample size. The number of selected features is quite variable, even for larger sample sizes. The number of core signature features among selected features is stable for larger sample sizes and represents 30 to 40% of the selected features (for N=200 and 300 respectively). 5.7 Effect of sample consistency To see the effect of sample consistency on classification results, we will focus on the performance of the small sample fits (N=25) where variability is the greatest, and the context where investigators should be most aware of the effect of sample selection over and beyond sample size concerns. In the plot below, control_Q and affected_Q are summary measures of sample set consistencies for the control and affected groups in the different cv runs. 1se_cv and 1se_test are the cross-validation and test set error rates, respectively. control_samp25_qual_vec &lt;- apply(sim_control_qual_mtx[1:25, ], 2, median) affected_samp25_qual_vec &lt;- apply(sim_affected_qual_mtx[1:25, ], 2, median) samp25_qual_error_mtx &lt;- cbind( `1se_cv` = error_1se_Bysize_lst[[&#39;025_cv&#39;]], `1se_test` = error_1se_Bysize_lst[[&#39;025_test&#39;]], `control_Q` = control_samp25_qual_vec, `affected_Q` = affected_samp25_qual_vec) # Correlation panel panel.cor &lt;- function(x, y){ usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- round(cor(x, y), digits=2) txt &lt;- paste0(&quot;R = &quot;, r) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = 1.5) ###cex.cor * r) } pairs(samp25_qual_error_mtx, lower.panel = panel.cor) Figure 5.13: sample consistency vs classifier performance We see that although the sample consistency effect is not strong, the consistency of the affected sample cohorts does have a measurable impact on the 1se cv and test set error rates (cor = -0.32 and -0.27, respectively). This plot is somewhat under-whelming. "],
["brca-rnaseq-preproc.html", "Section 6 BrCa RNA-Seq: Preprocessing 6.1 Load the data 6.2 Differential representation analysis 6.3 Signal-to-noise ratio regime", " Section 6 BrCa RNA-Seq: Preprocessing In this section we examine some RNA-Seq data from a breast cancer study to provide a different SNR context from the HCC 5hmC data in which to examine the relationship between sample size and model performance. For this exercise, we will look at discriminating between breast cancer samples of subtype LumA and all other subtypes among ER+/HER2- breast cancer samples. This may be a somewhat artificial and uninteresting classification problem, but it does provide a binary classification problem with many examples in the two subgroups allowing is to run simulations analogous to those that were run on the HCC 5hmC data. 6.1 Load the data The data that are available from NCBI GEO Series GSE96058 can be conveniently accessed through an R data package. Attaching the GSE96058 package makes the count data tables available as well as a gene annotation table and a sample description table. See GSE96058 R Data Package page. The data in GSE96058 have been separated into various data frames and matrices. The separation of the gene expression data into separate matrices was designed to get around github’s file size restrictions. See GSE96058 Package Page for details. ### CLEAR CACHE if (!(&quot;GSE96058&quot; %in% rownames(installed.packages()))) { if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;12379Monty/GSE96058&quot;) } library(GSE96058) # Strip GSE_ID prefix form object names ### MOD: (2020.09.21) keep &#39;brcaRna_&#39; as prefix to object names. ### Otherwise we run into problems when different datasets are ### analyzed for(OBJ in data(package=&#39;GSE96058&#39;)$results[, &#39;Item&#39;]) assign(sub(&#39;GSE96058_&#39;,&#39;brcaRna_&#39;,OBJ), get(OBJ)) detach(package:GSE96058, unload = T ) For this analysis, we will consider ER+/HER2- samples only and examine pam50_subtype = 'LumA' vs 'Other' as outcome. This choice is predicated on the desire to have a fairly balanced binary outcome to work with when we examine the effect of sample size on classification performance. The outcome in this dataset will be aliased as group. ER_HER2_tbl &lt;- with(brcaRna_sampDesc %&gt;% dplyr::filter(!grepl(&#39;repl$&#39;, title) &amp; !isRepl &amp; er_Status!=&#39;NA&#39; &amp; her2_Status!=&#39;NA&#39;), table(ER_HER2=paste(er_Status, her2_Status, sep=&#39;_&#39;), exclude=NULL) ) knitr::kable(ER_HER2_tbl, caption=&quot;ER, PR and HER2 Status&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 6.1: ER, PR and HER2 Status ER_HER2 Freq 0_0 165 0_1 63 1_0 2425 1_1 310 # Subset analysis samples brcaRna_sampDesc &lt;- brcaRna_sampDesc %&gt;% dplyr::filter(!grepl(&#39;repl$&#39;, title) &amp; er_Status == &#39;1&#39; &amp; her2_Status == &#39;0&#39;) %&gt;% dplyr::mutate(group = pam50_subtype) %&gt;% dplyr::rename(sampID = title) %&gt;% dplyr::arrange(group, sampID) #with(brcaRna_sampDesc, table(group, exclude=NULL)) # Recode group brcaRna_sampDesc$group &lt;- with( brcaRna_sampDesc, ifelse(group == &quot;LumA&quot;, &quot;LumA&quot;, &quot;Other&quot;) ## Luma &gt; Other ) # Re-order and name rows o.v &lt;- with(brcaRna_sampDesc, order(group, sampID)) brcaRna_sampDesc &lt;- brcaRna_sampDesc[o.v, ] rownames(brcaRna_sampDesc) &lt;- brcaRna_sampDesc$sampID # set brcaRna_groupCol for later brcaRna_groupCol &lt;- c(&quot;#F3C300&quot;, &quot;#875692&quot;) names(brcaRna_groupCol) &lt;- unique(brcaRna_sampDesc$group) with(brcaRna_sampDesc, knitr::kable(table(group, exclude = NULL), caption=&quot;Samples used in this analysis&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) Table 6.1: Samples used in this analysis group Freq LumA 1492 Other 933 The features in this dataset are gene expression indicators: Gene expression data in FPKM were generated using cufflinks 2.2.1 (default parameters except –GTF, –frag-bias-correct GRCh38.fa, –multi-read-correct, –library-type fr-firststrand, –total-hits-norm, –max-bundle-frags 10000000). The resulting data was was post-processed by collapsing on 30,865 unique gene symbols (sum of FPKM values of each matching transcript), adding to each expression measurement 0.1 FPKM, and performing a log2 transformation. 1. We now proceed to run a minimal set of QC sanity checks to make sure that there are no apparent systematic effects in the data. Assemble feature matrix for brcaRna_sampDesc samples. # Start for geneExpression_repl1 data repl1_ndx &lt;- which(is.element(colnames(brcaRna_geneExpression_repl1), brcaRna_sampDesc$sampID)) brcaRna_geneExpression &lt;- brcaRna_geneExpression_repl1[, repl1_ndx] # add samples from brcaRna_geneExpression_subJJ for(JJ in 1:5) { sub_ndx &lt;- which(is.element( colnames(get(paste0(&#39;brcaRna_geneExpression_sub&#39;,JJ))), brcaRna_sampDesc$sampID)) brcaRna_geneExpression &lt;- cbind(brcaRna_geneExpression, get(paste0(&#39;brcaRna_geneExpression_sub&#39;,JJ))[, sub_ndx]) } # align brcaRna_geneExpression columns to brcaRna_sampDesc samples order geneExpr_ndx &lt;- match(brcaRna_sampDesc$sampID, colnames(brcaRna_geneExpression)) if(any(is.na(geneExpr_ndx))) stop(&quot;brcaRna_sampDesc/brcaRna_geneExpression: sample mismatch&quot;) else brcaRna_geneExpression &lt;- brcaRna_geneExpression[, rownames(brcaRna_sampDesc)] We first look at coverage - make sure there isn’t too much disparity of coverage across samples. To detect shared variability, samples can be annotated and ordered according to sample features that may be linked to sample batch processing. Here we the samples have been ordered by group and sample id (an alias of geoAcc). par(mar = c(1, 3, 2, 3)) boxplot(brcaRna_geneExpression, ylim = c(-4, 4), ylab=&#39;log2 Count&#39;, staplewex = 0, # remove horizontal whisker lines staplecol = &quot;white&quot;, # just to be totally sure :) outline = F, # remove outlying points whisklty = 0, # remove vertical whisker lines las = 2, horizontal = F, xaxt = &quot;n&quot;, border = brcaRna_groupCol[brcaRna_sampDesc$group] ) legend(&quot;top&quot;, legend = names(brcaRna_groupCol), text.col = brcaRna_groupCol, ncol = 2, bty = &quot;n&quot;) # Add reference lines SampleMedian &lt;- apply(brcaRna_geneExpression, 2, median) abline(h = median(SampleMedian), col = &quot;grey&quot;) axis(side = 4, at = round(median(SampleMedian), 2), las = 2, col = &quot;grey&quot;, line = 0, tick = F) Figure 6.1: Sample log2 count boxplots 6.2 Differential representation analysis In the remainder of this section, we will process the data and perform differential expression analysis. If we had counts, the analysis pipeline outlined in Law et al. (2018) [18] would be appropriate. The main analysis steps of the pipeline are: remove lowly expressed genes normalize gene expression distributions remove heteroscedascity fit linear models and examine DE results We will adapt this pipeline to the FPKM data that we have to work with - basically skip the 2nd and 3rd steps in the above pipeline. Remove lowly expressed genes To determine a sensible threshold we can begin by examining the shapes of the distributions. par(mar = c(4, 3, 2, 1)) plot(density(brcaRna_geneExpression[, 1]), col = brcaRna_groupCol[brcaRna_sampDesc$group[1]], lwd = 2, ylim = c(0, .30), xlim=c(-5, 10), las = 2, main = &quot;&quot;, xlab = &quot;gene expression&quot; ) abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(brcaRna_geneExpression), size = 100)) { den &lt;- density(brcaRna_geneExpression[, JJ]) lines(den$x, den$y, col = brcaRna_groupCol[brcaRna_sampDesc$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(brcaRna_groupCol), text.col = brcaRna_groupCol, bty = &quot;n&quot;) Figure 6.2: Sample gene expression densities In this analysis we will be using a nominal gene expression value of 0, genes are deeemed to be represented if their expression is above this threshold, and not represented otherwise. For this analysis we will require that genes be represented in at least 25 samples across the entire dataset to be retained for downstream analysis. Remove weakly represented genes and replot densities. Removing 37.1% of genes… ### CLEAR CACHE par(mar = c(4, 3, 2, 1)) plot(density(brcaRna_geneExpression_F[, 1]), col = brcaRna_groupCol[brcaRna_sampDesc$group[1]], lwd = 2, las = 2, main = &quot;&quot;, xlab = &quot;gene expression&quot;, ylim = c(0, .30), xlim=c(-5, 10) ) #abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(brcaRna_geneExpression_F), size = 100)) { den &lt;- density(brcaRna_geneExpression_F[, JJ]) lines(den$x, den$y, col = brcaRna_groupCol[brcaRna_sampDesc$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(brcaRna_groupCol), text.col = brcaRna_groupCol, bty = &quot;n&quot;) Figure 6.3: Sample gene expression densities after removing weak genes As another sanity check, we will look at a multidimensional scaling plot of distances between gene expression profiles. We use plotMDS in limma package [19]), which plots samples on a two-dimensional scatterplot so that distances on the plot approximate the typical log2 fold changes between the samples. ### CLEAR CACHE par(mfcol = c(1, 2), mar = c(4, 4, 2, 1), xpd = NA, oma = c(0, 0, 2, 0)) # wo loss of generality, sample 500 samples # simply a matter of convenience to save time # remove from final version set.seed(1) samp_ndx &lt;- sample(1:ncol(brcaRna_geneExpression_F), size = 500) MDS.out &lt;- limma::plotMDS(brcaRna_geneExpression_F[, samp_ndx], col = brcaRna_groupCol[brcaRna_sampDesc$group[samp_ndx]], pch = 1 ) legend(&quot;topleft&quot;, legend = names(brcaRna_groupCol), text.col = brcaRna_groupCol, bty = &quot;n&quot; ) MDS.out &lt;- limma::plotMDS(brcaRna_geneExpression_F[, samp_ndx], col = brcaRna_groupCol[brcaRna_sampDesc$group[samp_ndx]], pch = 1, dim.plot = 3:4 ) Figure 6.4: MDS plots of gene expression values The MDS plot, which is analogous to a PCA plot adapted to gene exression data, does not indicate strong clustering of samples. Creating a design matrix and contrasts Before proceeding with the statistical modeling used for the differential expression analysis, we need to set up a model design matrix. ### CLEAR CACHE Design_mtx &lt;- model.matrix( ~ -1 + group, data=brcaRna_sampDesc) colnames(Design_mtx) &lt;- sub(&#39;group&#39;, &#39;&#39;, colnames(Design_mtx)) cat(&quot;colSums(Design_mtx):\\n&quot;) ## colSums(Design_mtx): colSums(Design_mtx) ## LumA Other ## 1492 933 Contrasts_mtx &lt;- limma::makeContrasts( LumAvsOther = LumA - Other, levels=colnames(Design_mtx)) cat(&quot;Contrasts:\\n&quot;) ## Contrasts: Contrasts_mtx ## Contrasts ## Levels LumAvsOther ## LumA 1 ## Other -1 Fit linear models and examine the results Having properly filtered and normalized the data, the linear models can be fitted to each gene and the results examined to assess differential expression between the two groups of interest, in our case LumA vs Other. Table ?? displays the counts of genes in each DE category: brcaRna_geneExpression_F_fit &lt;- limma::lmFit(brcaRna_geneExpression_F, Design_mtx) colnames(brcaRna_geneExpression_F_fit$coefficients) &lt;- sub(&quot;\\\\(Intercept\\\\)&quot;, &quot;Intercept&quot;, colnames(brcaRna_geneExpression_F_fit$coefficients) ) brcaRna_geneExpression_F_fit &lt;- limma::contrasts.fit( brcaRna_geneExpression_F_fit, contrasts=Contrasts_mtx) brcaRna_geneExpression_F_efit &lt;- limma::eBayes(brcaRna_geneExpression_F_fit) brcaRna_geneExpression_F_efit_dt &lt;- limma::decideTests(brcaRna_geneExpression_F_efit,adjust.method = &quot;BH&quot;, p.value = 0.05) knitr::kable(t(summary(brcaRna_geneExpression_F_efit_dt)), caption=&quot;DE Results at FDR = 0.05&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 6.2: DE Results at FDR = 0.05 Down NotSig Up LumAvsOther 6301 4498 8607 Graphical representations of DE results: MD Plots To summarise results for all genes visually, mean-difference plots (aka MA plot), which display log-FCs from the linear model fit against the average gene expression values can be generated using the plotMD function, with the differentially expressed genes highlighted. We may also be interested in whether certain gene features are related to gene identification. Gene GC content, for example, might be of interest. We don’t have GC content here - skip this for now. ### CLEAR CACHE par(mfrow=c(1,2), mar=c(4.5,4.5,2,1),oma=c(1,1,2,0)) # log-fold-change vs ave-expr limma::plotMD(brcaRna_geneExpression_F_efit, ylim = c(-0.5, 0.5), column=&#39;LumAvsOther&#39;, status=brcaRna_geneExpression_F_efit_dt[,&#39;LumAvsOther&#39;], hl.pch = 16, hl.col = c(&quot;lightblue&quot;, &quot;pink&quot;), hl.cex = .5, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(brcaRna_geneExpression_F_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F, cex.lab=1.5 ) abline(h = 0, col = &quot;black&quot;) rug(quantile(brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(brcaRna_geneExpression_F_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) # log-fold-change vs identification boxplot(split( brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], brcaRna_geneExpression_F_efit_dt[,&#39;LumAvsOther&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;log-fold-change&#39;, ylim=c(-.4, .4), cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) Figure 6.5: LumA vs Other - Genes Identified at FDR = 0,05 #SKIP - DONT HAVE ACGT SKIP &lt;- function() { # gc vs identification genes_ndx &lt;- match(rownames(brcaRna_geneExpression_F_efit), brcaRna_genes_annotF$geneSymbol) if(sum(is.na(genes_ndx))) stop(&quot;brcaRna_geneExpression_F_efit/brcaRna_genes_annotF: genes mismatch&quot;) GC_vec &lt;- with(brcaRna_genes_annotF[genes_ndx,],(G+C)/(A+C+G+T)) boxplot(split( GC_vec, brcaRna_geneExpression_F_efit_dt[,&#39;LumAvsOther&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;gene-gc&#39;, cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) #mtext(side=3, outer=T, cex=1.25, &quot;Genes identified at adjusted p-value=0.05&quot;) }#SKIP ### CLEAR CACHE brcaRna_geneExpression_F_logFC_sum &lt;- sapply( split( brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], brcaRna_geneExpression_F_efit_dt[,&#39;LumAvsOther&#39;]), quantile, prob = (1:3) / 4) colnames(brcaRna_geneExpression_F_logFC_sum) &lt;- as.character(factor( colnames(brcaRna_geneExpression_F_logFC_sum), levels=c(&quot;-1&quot;, &quot;0&quot;, &quot;1&quot;), labels=c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;) )) knitr::kable(brcaRna_geneExpression_F_logFC_sum, digits = 2, caption = &quot;log FC quartiles by gene identification&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 6.3: log FC quartiles by gene identification down notDE up 25% -0.28 -0.03 0.11 50% -0.17 0.00 0.20 75% -0.10 0.03 0.37 Many genes are identified, and the effect sizes are substantial in comparison to the 5hmc data - compare tables ?? and ??. This will result in a higher signal-to-noise ratio context. See Section 6.3. DE genes at 10% fold change For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The treat method (McCarthy and Smyth 2009 [23]) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement. The number of differentially expressed genes are greatly reduced if we impose a minimal fold-change requirement of 10%. ### CLEAR CACHE brcaRna_geneExpression_F_tfit &lt;- limma::treat(brcaRna_geneExpression_F_fit, lfc=log2(1.10)) brcaRna_geneExpression_F_tfit_dt &lt;- limma::decideTests(brcaRna_geneExpression_F_tfit) cat(&quot;10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05:\\n&quot;) ## 10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05: summary(brcaRna_geneExpression_F_tfit_dt) ## LumAvsOther ## Down 2569 ## NotSig 12519 ## Up 4318 # log-fold-change vs ave-expr limma::plotMD(brcaRna_geneExpression_F_efit, ylim = c(-0.6, 0.6), column=&#39;LumAvsOther&#39;, status=brcaRna_geneExpression_F_tfit_dt[,&#39;LumAvsOther&#39;], hl.pch = 16, hl.col = c(&quot;blue&quot;, &quot;red&quot;), hl.cex = .7, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(brcaRna_geneExpression_F_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F ) abline(h = 0, col = &quot;black&quot;) rug(quantile(brcaRna_geneExpression_F_efit$coefficients[, &#39;LumAvsOther&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(brcaRna_geneExpression_F_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) Figure 6.6: LumA vs Other - Identified Genes at FDR = 0,05 and logFC &gt; 10% 6.3 Signal-to-noise ratio regime In Hastie et al. (2017) [7]) results from lasso fits are compared with best subset and forward selection fits and it is argued that while best subset is optimal for high signal-to-noise regimes, the lasso gains some competitive advantage when the prevailing signal-to-noise ratio of the dataset is lowered. ### CLEAR CACHE Effect &lt;- abs(brcaRna_geneExpression_F_efit$coefficients[,&#39;LumAvsOther&#39;]) Noise &lt;- brcaRna_geneExpression_F_efit$sigma SNR &lt;- Effect/Noise plot(spatstat::CDF(density(SNR)), col = 1, lwd = 2, ylab = &quot;Prob(SNR&lt;x)&quot;, xlim = c(0, 1.5) ) SNR_quant &lt;- quantile(SNR, prob=c((1:3)/4,.9)) rug(SNR_quant, lwd = 2, ticksize = 0.05, col = 1 ) Figure 6.7: Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile knitr::kable(t(SNR_quant), digits = 3, caption = paste( &quot;SNR Quantiles&quot;) ) %&gt;% kableExtra::kable_styling(full_width = F) Table 6.4: SNR Quantiles 25% 50% 75% 90% 0.094 0.217 0.384 0.56 See NCBI GEO GSE96058↩︎ "],
["brca-rnaseseq-explore-sparsity.html", "Section 7 BrCa RNA-Seq: Exploring Sparsity 7.1 Cross-validation analysis setup 7.2 Fit and compare models 7.3 The relaxed lasso and blended mix models 7.4 Examination of sensitivity vs specificity 7.5 Compare predictions at misclassified samples 7.6 Compare coefficient profiles 7.7 Examine feature selection", " Section 7 BrCa RNA-Seq: Exploring Sparsity In this section we explore various models fitted to the Breast Cancer RNA-Seq data set explored in Section 6. We focus our analyses on lasso fits which tend to favor sparse models. 7.1 Cross-validation analysis setup We use the same CV set-up as was used with the HCC 5hmC-Seq data set (Section 4). K_FOLD &lt;- 10 trainP &lt;- 0.8 First we divide the analysis dataset into train and test in a \\(4\\):1 ratio. ### CLEAR CACHE set.seed(1) brcaRna_train_sampID_vec &lt;- with(brcaRna_sampDesc, brcaRna_sampDesc$sampID[caret::createDataPartition(y=group, p=trainP, list=F)] ) brcaRna_test_sampID_vec &lt;- with(brcaRna_sampDesc, setdiff(sampID, brcaRna_train_sampID_vec) ) brcaRna_train_group_vec &lt;- factor(brcaRna_sampDesc[brcaRna_train_sampID_vec, &#39;group&#39;], levels=c(&#39;Other&#39;, &#39;LumA&#39;)) names(brcaRna_train_group_vec) &lt;- brcaRna_sampDesc[brcaRna_train_sampID_vec, &#39;sampID&#39;] brcaRna_test_group_vec &lt;- factor(brcaRna_sampDesc[brcaRna_test_sampID_vec, &#39;group&#39;], levels=c(&#39;Other&#39;, &#39;LumA&#39;)) names(brcaRna_test_group_vec) &lt;- brcaRna_sampDesc[brcaRna_test_sampID_vec, &#39;sampID&#39;] knitr::kable(table(brcaRna_train_group_vec), caption=&quot;Train set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.1: Train set brcaRna_train_group_vec Freq Other 747 LumA 1194 knitr::kable(table(brcaRna_test_group_vec), caption=&quot;Test set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.1: Test set brcaRna_test_group_vec Freq Other 186 LumA 298 brcaRna_train_geneExpr_mtx &lt;- t(brcaRna_geneExpression_F[,brcaRna_train_sampID_vec]) brcaRna_test_geneExpr_mtx &lt;- t(brcaRna_geneExpression_F[,brcaRna_test_sampID_vec]) We explore some glmnet fits and the “bet on sparsity”. We consider three models, specified by the value of the alpha parameter in the elastic net parametrization: - lasso: \\(\\alpha = 1.0\\) - sparse models - ridge \\(\\alpha = 0\\) - shrunken coefficients models - elastic net: \\(\\alpha = 0.5\\) - semi sparse model In this analysis, we will only evaluate models in terms of model sparsity, stability and performance. We leave the question of significance testing of hypotheses about model parameters completely out. See Lockhart et al. (2014) [24] and Wassermam (2014) [25] for a discussion of this topic. In this section we look at the relative performance and sparsity of the models considered. The effect of the size of the sample set on the level and stability of performance will be investigated in the next section. First we create folds for \\(10\\)-fold cross-validation of models fitted to training data. We’ll use caret::createFolds to assign samples to folds while keeping the outcome ratios constant across folds. # This is too variable, both in terms of fold size And composition #foldid_vec &lt;- sample(1:10, size=length(brcaRna_train_group_vec), replace=T) ### CLEAR CACHE set.seed(1) brcaRna_train_foldid_vec &lt;- caret::createFolds( factor(brcaRna_train_group_vec), k=K_FOLD, list=F) # brcaRna_train_foldid_vec contains the left-out IDs # the rest are kept fold_out_tbl &lt;- sapply(split(brcaRna_train_group_vec, brcaRna_train_foldid_vec), table) rownames(fold_out_tbl) &lt;- paste(rownames(fold_out_tbl), &#39;- Out&#39;) fold_in_tbl &lt;- do.call(&#39;cbind&#39;, lapply(sort(unique(brcaRna_train_foldid_vec)), function(FOLD) table(brcaRna_train_group_vec[brcaRna_train_foldid_vec != FOLD]))) rownames(fold_in_tbl) &lt;- paste(rownames(fold_in_tbl), &#39;- In&#39;) colnames(fold_in_tbl) &lt;- as.character(sort(unique(brcaRna_train_foldid_vec))) knitr::kable(rbind(fold_in_tbl, fold_out_tbl[,colnames(fold_in_tbl)]), caption=&quot;training samples fold composition&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.2: training samples fold composition 1 2 3 4 5 6 7 8 9 10 Other - In 673 673 672 672 672 672 672 672 673 672 LumA - In 1074 1075 1075 1074 1075 1075 1074 1075 1074 1075 Other - Out 74 74 75 75 75 75 75 75 74 75 LumA - Out 120 119 119 120 119 119 120 119 120 119 Note that the folds identify samples that are left-out of the training data for each fold fit. 7.2 Fit and compare models ### CLEAR CACHE start_time &lt;- proc.time() brcaRna_cv_lasso &lt;- glmnet::cv.glmnet( x=brcaRna_train_geneExpr_mtx, y=brcaRna_train_group_vec, foldid=brcaRna_train_foldid_vec, alpha=1, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;lasso time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lasso time: 31.48s ### CLEAR CACHE start_time &lt;- proc.time() brcaRna_cv_ridge &lt;- glmnet::cv.glmnet( x=brcaRna_train_geneExpr_mtx, y=brcaRna_train_group_vec, foldid=brcaRna_train_foldid_vec, alpha=0, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;ridge time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## ridge time: 258.93s ### CLEAR CACHE start_time &lt;- proc.time() brcaRna_cv_enet &lt;- glmnet::cv.glmnet( x=brcaRna_train_geneExpr_mtx, y=brcaRna_train_group_vec, foldid=brcaRna_train_foldid_vec, alpha=0.5, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;enet time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## enet time: 32.59s ### CLEAR CACHE plot_cv_f &lt;- function(cv_fit, Nzero=T, ...) { suppressPackageStartupMessages(require(glmnet)) # No nonger used #lambda.1se_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se] #lambda.min_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min] # Get oof error - cv errors produced by extraction method ARE oof!!! ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) train_oofPred_1se_vec &lt;- ifelse( logistic_f(cv_fit$fit.preval[,ndx_1se]) &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) train_oofPred_1se_error &lt;- mean(train_oofPred_1se_vec != brcaRna_train_group_vec) ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) train_oofPred_min_vec &lt;- ifelse( logistic_f(cv_fit$fit.preval[,ndx_min]) &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) train_oofPred_min_error &lt;- mean(train_oofPred_min_vec != brcaRna_train_group_vec) # Get test set error test_pred_1se_vec &lt;- predict( cv_fit, newx=brcaRna_test_geneExpr_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_pred_1se_error &lt;- mean(test_pred_1se_vec != brcaRna_test_group_vec) test_pred_min_vec &lt;- predict( cv_fit, newx=brcaRna_test_geneExpr_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_pred_min_error &lt;- mean(test_pred_min_vec != brcaRna_test_group_vec) plot( log(cv_fit$lambda), cv_fit$cvm, pch=16,col=&quot;red&quot;, xlab=&#39;&#39;,ylab=&#39;&#39;, ... ) abline(v=log(c(cv_fit$lambda.1se, cv_fit$lambda.min))) if(Nzero) axis(side=3, tick=F, at=log(cv_fit$lambda), labels=cv_fit$nzero, line = -1 ) LL &lt;- 2 #mtext(side=1, outer=F, line = LL, &quot;log(Lambda)&quot;) #LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;1se p =&quot;, lambda.1se_p),&#39;&#39;), &quot;1se: train =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], 1), ##&quot;oof =&quot;, round(100*train_oofPred_1se_error, 1), ### REDUNDANT &quot;test =&quot;, round(100*test_pred_1se_error, 1) )) LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;min p =&quot;, lambda.min_p),&#39;&#39;), &quot;min: train =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], 1), ##&quot;oof =&quot;, round(100*train_oofPred_min_error, 1), ### REDUNDANT &quot;test =&quot;, round(100*test_pred_min_error, 1) )) tmp &lt;- cbind( error_1se = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se], train = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], #train_oof = 100*train_oofPred_1se_error, ### REDUNANT test = 100*test_pred_1se_error), error_min = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min], train = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], #train_oof = 100*train_oofPred_min_error, ### REDUNDSANT test = 100*test_pred_min_error) ) # Need to fix names rownames(tmp) &lt;- c(&#39;p&#39;, &#39;train&#39;, &#39;test&#39;) tmp } Examine model performance. ### CLEAR CACHE par(mfrow=c(1,3), mar=c(5, 2, 3, 1), oma=c(3,2,0,0)) lasso_errors_mtx &lt;- plot_cv_f(brcaRna_cv_lasso, ylim=c(0,.5)) title(&#39;lasso&#39;) rifge_errors_mtx &lt;- plot_cv_f(brcaRna_cv_ridge, Nzero=F, ylim=c(0,.5)) title(&#39;ridge&#39;) enet_errors_mtx &lt;- plot_cv_f(brcaRna_cv_enet, ylim=c(0,.5)) title(&#39;enet&#39;) mtext(side=1, outer=T, cex=1.25, &#39;log(Lambda)&#39;) mtext(side=2, outer=T, cex=1.25, brcaRna_cv_lasso$name) Figure 7.1: compare fits ### CLEAR CACHE errors_frm &lt;- data.frame( lasso = lasso_errors_mtx, ridge = rifge_errors_mtx, enet = enet_errors_mtx ) colnames(errors_frm) &lt;- sub(&#39;\\\\.error&#39;,&#39;&#39;, colnames(errors_frm)) knitr::kable(t(errors_frm), caption = &#39;Misclassifiaction error rates&#39;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.3: Misclassifiaction error rates p train test lasso_1se 168 11.9 9.9 lasso_min 408 11.4 12.8 ridge_1se 19406 13.0 13.0 ridge_min 19406 12.6 12.4 enet_1se 315 11.6 11.2 enet_min 671 11.1 13.4 lasso and enet, 1se and min lambda, have comparable performance on the training data. The lasso 1se model gains performance in the test data, but lasso min lambda and enet models stay at the same level or drop in performance on the test set. lasso 1se is most parsimonious by a factor of 2. 7.3 The relaxed lasso and blended mix models Next we look at the so-called relaxed lasso model, and the blended mix which is an optimized shrinkage between the relaxed lasso and the regular lasso. See (2.3) in Section 2. ### CLEAR CACHE library(glmnet) brcaRna_cv_lassoR_sum &lt;- print(brcaRna_cv_lassoR) ## ## Call: glmnet::cv.glmnet(x = brcaRna_train_geneExpr_mtx, y = brcaRna_train_group_vec, type.measure = &quot;class&quot;, foldid = brcaRna_train_foldid_vec, keep = T, relax = T, alpha = 1, family = &quot;binomial&quot;, nlambda = 30) ## ## Measure: Misclassification Error ## ## Gamma Lambda Measure SE Nonzero ## min 1.00 0.005227 0.1139 0.006167 408 ## 1se 0.75 0.021823 0.1180 0.008812 91 plot(brcaRna_cv_lassoR) Figure 7.2: Relaxed lasso fit ### CLEAR CACHE # only report 1se ndx_1se &lt;- match(brcaRna_cv_lassoR$lambda.1se, brcaRna_cv_lassoR$lambda) ndx_min &lt;- match(brcaRna_cv_lassoR$lambda.min, brcaRna_cv_lassoR$lambda) # only show 1se anyway # if(ndx_1se != ndx_min) stop(&quot;lambda.1se != lambda.min&quot;) # train oof data - NOT CLEAR WHY THESE DIFFER FROM CV ERRORS EXTRACTED FROM MODEL # Get relaxed lasso (gamma=0) oof error train_oofPred_relaxed_1se_vec &lt;- ifelse( logistic_f(brcaRna_cv_lassoR$fit.preval[[&quot;g:0&quot;]][, ndx_1se]) &gt; 0.5, &quot;LumA&quot;, &quot;Other&quot; ) train_oofPred_relaxed_1se_error &lt;- mean(train_oofPred_relaxed_1se_vec != brcaRna_train_group_vec) # blended mix (gamma=0.5) train_oofPred_blended_1se_vec &lt;- ifelse( logistic_f(brcaRna_cv_lassoR$fit.preval[[&quot;g:0.5&quot;]][, ndx_1se]) &gt; 0.5, &quot;LumA&quot;, &quot;Other&quot; ) train_oofPred_blended_1se_error &lt;- mean(train_oofPred_blended_1se_vec != brcaRna_train_group_vec) # Test set error - relaxed test_pred_relaxed_1se_vec &lt;- predict( brcaRna_cv_lassoR, newx = brcaRna_test_geneExpr_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0 ) test_pred_relaxed_1se_error &lt;- mean(test_pred_relaxed_1se_vec != brcaRna_test_group_vec) # Test set error - blended test_pred_blended_1se_vec &lt;- predict( brcaRna_cv_lassoR, newx = brcaRna_test_geneExpr_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0.5 ) test_pred_blended_1se_error &lt;- mean(test_pred_blended_1se_vec != brcaRna_test_group_vec) brcaRna_cv_lassoR_1se_error &lt;- brcaRna_cv_lassoR$cvm[brcaRna_cv_lassoR$lambda==brcaRna_cv_lassoR$lambda.min] cv_blended_statlist &lt;- brcaRna_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] cv_blended_1se_error &lt;- cv_blended_statlist$cvm[cv_blended_statlist$lambda== brcaRna_cv_lassoR$relaxed$lambda.1se] knitr::kable(t(data.frame( train_relaxed = brcaRna_cv_lassoR_1se_error, train_blended = cv_blended_1se_error, #train_relaxed_oof = train_oofPred_relaxed_1se_error, #train_blended_oof = train_oofPred_blended_1se_error, test_relaxed = test_pred_relaxed_1se_error, test_blended = test_pred_blended_1se_error )) * 100, digits = 1, caption = &quot;Relaxed lasso and blended mix error rates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.4: Relaxed lasso and blended mix error rates train_relaxed 11.4 train_blended 11.8 test_relaxed 11.4 test_blended 10.3 The relaxed lasso and blended mix error rates are comparable to the regular lasso fit error rate. We see here too that the reported cv error rates are comparable to test set error rates. The 1se lambda rule applied to the relaxed lasso fit selected a model with \\(168\\) features, while for the blended mix model (See (2.3) in Section 2) the 1se lambda rule selected \\(91\\) features (vertical dotted reference line in Figure ??). This feature is pointed out in the glmnet 3.0 vignette: The debiasing will potentially improve prediction performance, and CV will typically select a model with a smaller number of variables. 7.4 Examination of sensitivity vs specificity In the results above we reported error rates without inspecting the sensitivity versus specificity trade-off. ROC curves can be examined to get a sense of the trade-off. 7.4.1 Training data out-of-fold ROC curves ### CLEAR CACHE ### CLEAR CACHE # train # lasso ndx_1se &lt;- match(brcaRna_cv_lasso$lambda.1se,brcaRna_cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(brcaRna_cv_lasso$fit.preval[,ndx_1se]) train_lasso_roc &lt;- pROC::roc( response = as.numeric(brcaRna_train_group_vec==&#39;LumA&#39;), predictor = train_lasso_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # enet ndx_1se &lt;- match(brcaRna_cv_enet$lambda.1se,brcaRna_cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(brcaRna_cv_enet$fit.preval[,ndx_1se]) train_enet_roc &lt;- pROC::roc( response = as.numeric(brcaRna_train_group_vec==&#39;LumA&#39;), predictor = train_enet_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # lasso - relaxed ndx_1se &lt;- match(brcaRna_cv_lassoR$lambda.1se,brcaRna_cv_lassoR$lambda) train_relaxed_oofProb_vec &lt;- logistic_f(brcaRna_cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_relaxed_roc &lt;- pROC::roc( response = as.numeric(brcaRna_train_group_vec==&#39;LumA&#39;), predictor = train_relaxed_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # blended mix (gamma=0.5) ndx_1se &lt;- match(brcaRna_cv_lassoR$lambda.1se,brcaRna_cv_lassoR$lambda) train_blended_oofProb_vec &lt;- logistic_f(brcaRna_cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_roc &lt;- pROC::roc( response = as.numeric(brcaRna_train_group_vec==&#39;LumA&#39;), predictor = train_blended_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(train_lasso_roc, col = col_vec[1]) lines(train_enet_roc, col = col_vec[2]) lines(train_relaxed_roc, col = col_vec[3]) lines(train_blended_roc, col = col_vec[4]) legend(&#39;bottomright&#39;, title=&#39;AUC&#39;, legend=c( paste(&#39;lasso =&#39;, round(train_lasso_roc[[&#39;auc&#39;]],3)), paste(&#39;enet =&#39;, round(train_enet_roc[[&#39;auc&#39;]],3)), paste(&#39;relaxed =&#39;, round(train_relaxed_roc[[&#39;auc&#39;]],3)), paste(&#39;blended =&#39;, round(train_blended_roc[[&#39;auc&#39;]],3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 7.3: Train data out-of-sample ROCs Compare thresholds for 90% Specificity: ### CLEAR CACHE lasso_ndx &lt;- with(as.data.frame(pROC::coords(train_lasso_roc, transpose=F)), min(which(specificity &gt;= 0.9))) enet_ndx &lt;- with(as.data.frame(pROC::coords(train_enet_roc, transpose=F)), min(which(specificity &gt;= 0.9))) lassoR_ndx &lt;- with(as.data.frame(pROC::coords(train_relaxed_roc, transpose=F)), min(which(specificity &gt;= 0.9))) blended_ndx &lt;- with(as.data.frame(pROC::coords(train_blended_roc, transpose=F)), min(which(specificity &gt;= 0.9))) spec90_frm &lt;- data.frame(rbind( lasso=as.data.frame(pROC::coords(train_lasso_roc, transpose=F))[lasso_ndx,], enet=as.data.frame(pROC::coords(train_enet_roc, transpose=F))[enet_ndx,], relaxed=as.data.frame(pROC::coords(train_relaxed_roc, transpose=F))[lassoR_ndx,], blended=as.data.frame(pROC::coords(train_blended_roc, transpose=F))[blended_ndx,] )) knitr::kable(spec90_frm, digits=3, caption=&quot;Specificity = .90 Coordinates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.5: Specificity = .90 Coordinates threshold specificity sensitivity lasso 0.672 0.901 0.850 enet 0.661 0.901 0.876 relaxed 1.000 0.901 0.625 blended 0.999 0.901 0.659 This is strange. ### CLEAR CACHE par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(train_lasso_oofProb_vec[brcaRna_train_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_lasso_oofProb_vec[brcaRna_train_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Other&quot;, &quot;LumA&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(train_enet_oofProb_vec[brcaRna_train_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_enet_oofProb_vec[brcaRna_train_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # lassoR plot(density(train_relaxed_oofProb_vec[brcaRna_train_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_relaxed_oofProb_vec[brcaRna_train_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;lassoR&quot;) # blended plot(density(train_blended_oofProb_vec[brcaRna_train_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_blended_oofProb_vec[brcaRna_train_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;out-of-fold predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 7.4: Train data out-of-fold predicted probabilities The relaxed lasso fit results in essentially dichotomized predicted probability distribution - predicted probabilities are very close to 0 or 1. Look at test data ROC curves. ### CLEAR CACHE ### CLEAR CACHE # plot all plot(test_lasso_roc, col = col_vec[1]) lines(test_enet_roc, col = col_vec[2]) lines(test_relaxed_roc, col = col_vec[3]) lines(test_blended_roc, col = col_vec[4]) legend(&quot;bottomright&quot;, title = &quot;AUC&quot;, legend = c( paste(&quot;lasso =&quot;, round(test_lasso_roc[[&quot;auc&quot;]], 3)), paste(&quot;enet =&quot;, round(test_enet_roc[[&quot;auc&quot;]], 3)), paste(&quot;relaxed =&quot;, round(test_relaxed_roc[[&quot;auc&quot;]], 3)), paste(&quot;blended =&quot;, round(test_blended_roc[[&quot;auc&quot;]], 3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 7.5: Test data out-of-sample ROCs Look at densities of predicted probabilities. ### CLEAR CACHE par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(test_lasso_predProb_vec[brcaRna_test_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_lasso_predProb_vec[brcaRna_test_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Other&quot;, &quot;LumA&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(test_enet_predProb_vec[brcaRna_test_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_enet_predProb_vec[brcaRna_test_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # relaxed plot(density(test_relaxed_predProb_vec[brcaRna_test_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_relaxed_predProb_vec[brcaRna_test_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;relaxed&quot;) #sapply(split(test_relaxed_predProb_vec, brcaRna_test_group_vec), summary) # blended plot(density(test_blended_predProb_vec[brcaRna_test_group_vec == &quot;Other&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_blended_predProb_vec[brcaRna_test_group_vec == &quot;LumA&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;test set predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 7.6: Test data out-of-fold predicted probabilities ### CLEAR CACHE # Define plotting function bxpPredProb_f &lt;- function(cv_fit, Gamma=NULL) { # Train - preval is out-of-fold linear predictor for training design points onese_ndx &lt;- match(cv_fit$lambda.1se, cv_fit$lambda) if(is.null(Gamma)) train_1se_preval_vec &lt;- cv_fit$fit.preval[, onese_ndx] else train_1se_preval_vec &lt;- cv_fit$fit.preval[[Gamma]][, onese_ndx] train_1se_predProb_vec &lt;- logistic_f(train_1se_preval_vec) # Test test_1se_predProb_vec &lt;- predict( cv_fit, newx = brcaRna_test_geneExpr_mtx, s = &quot;lambda.1se&quot;, type = &quot;resp&quot; ) tmp &lt;- c( train = split(train_1se_predProb_vec, brcaRna_train_group_vec), test = split(test_1se_predProb_vec, brcaRna_test_group_vec) ) names(tmp) &lt;- paste0(&quot;\\n&quot;, sub(&quot;\\\\.&quot;, &quot;\\n&quot;, names(tmp))) boxplot(tmp) } par(mfrow = c(2, 2), mar = c(5, 3, 2, 1), oma = c(2, 2, 2, 2)) bxpPredProb_f(brcaRna_cv_lasso) title(&#39;lasso&#39;) bxpPredProb_f(brcaRna_cv_enet) title(&#39;enet&#39;) bxpPredProb_f(brcaRna_cv_lassoR, Gamma=&#39;g:0&#39;) title(&#39;relaxed&#39;) bxpPredProb_f(brcaRna_cv_lassoR, Gamma=&#39;g:0.5&#39;) title(&#39;blended&#39;) Figure 7.7: Predicted Probabilities - Train and Test 7.5 Compare predictions at misclassified samples It is useful to examine classification errors more carefully. If models have different failure modes, one might get improved performance by combining model predictions. Note that the models considered here are not expected to compliment each other usefully as they are too similar in nature. ### CLEAR CACHE # NOTE: here we use computred oofClass rather than predClass # as predClass extracted from predict() are fitted values. # train - oof ndx_1se &lt;- match(brcaRna_cv_lasso$lambda.1se,brcaRna_cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(brcaRna_cv_lasso$fit.preval[,ndx_1se]) train_lasso_oofClass_vec &lt;- ifelse( train_lasso_oofProb_vec &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) ndx_1se &lt;- match(brcaRna_cv_enet$lambda.1se,brcaRna_cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(brcaRna_cv_enet$fit.preval[,ndx_1se]) train_enet_oofClass_vec &lt;- ifelse( train_enet_oofProb_vec &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) # RECALL: brcaRna_cv_lassoR$nzero[brcaRna_cv_lassoR$lambda==brcaRna_cv_lassoR$lambda.1se] # train - oof ndx_1se &lt;- match(brcaRna_cv_lassoR$lambda.1se,brcaRna_cv_lassoR$lambda) train_relaxed_oofProb_vec &lt;- logistic_f(brcaRna_cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_relaxed_oofClass_vec &lt;- ifelse( train_relaxed_oofProb_vec &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) # RECALL $`r brcaRna_cv_lassoR$relaxed$nzero.1se`$ features (vertical # cv_blended_statlist &lt;- brcaRna_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] # cv_blended_1se_error &lt;- cv_blended_statlist$cvm[cv_blended_statlist$lambda== #brcaRna_cv_lassoR$relaxed$lambda.1se] # train - oof cv_blended_statlist &lt;- brcaRna_cv_lassoR$relaxed$statlist[[&#39;g:0.5&#39;]] ndx_1se &lt;- match(brcaRna_cv_lassoR$relaxed$lambda.1se, cv_blended_statlist$lambda) train_blended_oofProb_vec &lt;- logistic_f(brcaRna_cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_oofClass_vec &lt;- ifelse( train_blended_oofProb_vec &gt; 0.5, &#39;LumA&#39;, &#39;_Other&#39;) misclass_id_vec &lt;- unique(c( names(train_lasso_oofClass_vec)[train_lasso_oofClass_vec != brcaRna_train_group_vec], names(train_enet_oofClass_vec)[train_enet_oofClass_vec != brcaRna_train_group_vec], names(train_relaxed_oofClass_vec)[train_relaxed_oofClass_vec != brcaRna_train_group_vec], names(train_blended_oofClass_vec)[train_blended_oofClass_vec != brcaRna_train_group_vec] ) ) missclass_oofProb_mtx &lt;- cbind( train_lasso_oofProb_vec[misclass_id_vec], train_enet_oofProb_vec[misclass_id_vec], train_relaxed_oofProb_vec[misclass_id_vec], train_blended_oofProb_vec[misclass_id_vec] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(brcaRna_train_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(brcaRna_train_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Other&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 7.8: out-of-fold predicted probabilities at miscassified samples As we’ve seen above, predictions from lassoR and the blended mix model are basically dichotomous; 0 or 1. Samples have been order by group, and median P(LumA) within group. For the Other (green), predicted probabilities less than 0.5 are considered correct here. For the LumA (red) samples, predicted probabilities greater than 0.5 are considered correct here. Now look at the same plot on the test data set. ### CLEAR CACHE test_lasso_predClass_vec &lt;- predict( brcaRna_cv_lasso, newx=brcaRna_test_geneExpr_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_enet_predClass_vec &lt;- predict( brcaRna_cv_enet, newx=brcaRna_test_geneExpr_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_relaxed_predClass_vec &lt;- predict( brcaRna_cv_lassoR, g=0, newx=brcaRna_test_geneExpr_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) test_blended_predClass_vec &lt;- predict( brcaRna_cv_lassoR, g=0.5, newx=brcaRna_test_geneExpr_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) misclass_id_vec &lt;- unique(c( names(test_lasso_predClass_vec[,1])[test_lasso_predClass_vec != brcaRna_test_group_vec], names(test_enet_predClass_vec[,1])[test_enet_predClass_vec != brcaRna_test_group_vec], names(test_relaxed_predClass_vec[,1])[test_relaxed_predClass_vec != brcaRna_test_group_vec], names(test_blended_predClass_vec[,1])[test_blended_predClass_vec != brcaRna_test_group_vec] ) ) missclass_oofProb_mtx &lt;- cbind( test_lasso_predProb_vec[misclass_id_vec,], test_enet_predProb_vec[misclass_id_vec,], test_relaxed_predProb_vec[misclass_id_vec,], test_blended_predProb_vec[misclass_id_vec,] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(brcaRna_test_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(brcaRna_test_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Other&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 7.9: Test data predicted probabilities at miscassified samples The relaxed lasso fit results in essentially dichotomized predicted probability distribution - predicted probabilities are very close to 0 or 1. We see that for design points in the training set, the predicted probabilies from the relaxed lasso are essentially dichotomized to be tightly distributed at the extremes of the response range. For design points in the test set, the predicted probabilies from the relaxed lasso are comparable to the lasso model predicted porbabilities. This seems to indicate over-fitting in the relaxed lasso fit. 7.6 Compare coefficient profiles ### CLEAR CACHE # lasso ########################## # train - cv predicted lasso_coef &lt;- coef( brcaRna_cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) # enet ########################## enet_coef &lt;- coef( brcaRna_cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) # THESE ARE NOT CORRECT - SKIP # relaxed lasso (gamma=0) ########################## SKIP &lt;- function() { lassoR_coef &lt;- coef( brcaRna_cv_lassoR, s=&#39;lambda.1se&#39;, g=0 ) lassoR_coef_frm &lt;- data.frame( gene=lassoR_coef@Dimnames[[1]][c(1, lassoR_coef@i[-1])], lassoR=lassoR_coef@x) } # blended mix (gamma=0.5) ############################### blended_coef &lt;- coef( brcaRna_cv_lassoR, s=&#39;lambda.1se&#39;, g=0.5 ) blended_coef_frm &lt;- data.frame( gene=blended_coef@Dimnames[[1]][c(1, blended_coef@i[-1])], blended=blended_coef@x) # put it all together all_coef_frm &lt;- base::merge( x = lasso_coef_frm, y = base::merge( x = enet_coef_frm, y = blended_coef_frm, by=&#39;gene&#39;, all=T), by=&#39;gene&#39;, all=T) # SKIPPED #base::merge( #x = lassoR_coef_frm, #y = blended_coef_frm, #by=&#39;gene&#39;, all=T), all_coef_frm[,-1][is.na(all_coef_frm[,-1])] &lt;- 0 par(mfrow=c(ncol(all_coef_frm)-1,1), mar=c(0,5,0,1), oma=c(3,1,2,0)) for(CC in 2:ncol(all_coef_frm)) { plot( x=1:(nrow(all_coef_frm)-1), xlab=&#39;&#39;, y=all_coef_frm[-1, CC], ylab=colnames(all_coef_frm)[CC], type=&#39;h&#39;, xaxt=&#39;n&#39;) } Figure 7.10: Coefficient Profiles Note that there is little difference between the elastic net and the lasso in the selected features, and when the coefficient is zero in one set, it is smaell in the other. By contrast, the blended fit produces more shrinkage. ### CLEAR CACHE knitr::kable( with(all_coef_frm[,-1], table(lassoZero=lasso==0, enetZero=enet==0)), caption=&#39;Zero Ceofficient: rows are lasso, columns enet&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 7.6: Zero Ceofficient: rows are lasso, columns enet FALSE TRUE FALSE 169 0 TRUE 147 6 Coefficients in the blended fit are larger than those in the lasso fit, or zero. We can also examine these with a scatter plot matrix. ### CLEAR CACHE pairs(all_coef_frm[-1,-1], lower.panel = NULL, panel = function(x, y) { points(x, y, pch = 16, col = &quot;blue&quot;) } ) Figure 7.11: Coefficients from fits 7.7 Examine feature selection Recall from glmnet vignette: It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an $\\alpha$=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of $\\alpha$ is for numerical stability; for example, the *elastic net with $\\alpha = 1 - \\epsilon$ for some small $\\epsilon$&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations*. To see how this plays out in this dataset, we can look at feature expression heat maps. Reader notes: Heat maps are rarely useful other than to display the obvious. Here too heat maps fail to yield any insights, or confirmation of the relationship between feature correlation and lasso vs enet feature selection. ### CLEAR CACHE suppressPackageStartupMessages(require(gplots)) # train - cv predicted lasso_coef &lt;- coef( brcaRna_cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(brcaRna_train_geneExpr_mtx[,lasso_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=lasso_coef_frm$gene, labCol=brcaRna_train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(brcaRna_train_group_vec==&#39;_Other&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;lasso genes - N =&#39;, nrow(lasso_coef_frm)-1)) Figure 7.12: Lasso Model Genes ### CLEAR CACHE suppressPackageStartupMessages(require(gplots)) # train - cv predicted enet_coef &lt;- coef( brcaRna_cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(brcaRna_train_geneExpr_mtx[,enet_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=enet_coef_frm$gene, labCol=brcaRna_train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(brcaRna_train_group_vec==&#39;_Other&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;enet genes - N =&#39;, nrow(enet_coef_frm)-1)) Figure 7.13: Enet Model Genes "],
["brca-rnaseq-model-suite.html", "Section 8 BrCa RNA-Seq: Sample size investigation 8.1 Full data set fit 8.2 Selected feature list stability 8.3 Simulation Design 8.4 Setup simulation 8.5 Run simulations 8.6 Simulation results", " Section 8 BrCa RNA-Seq: Sample size investigation In this section we repeat the analyses ran on the HCC 5hmC data in Section 5 on the breast cancer RNA-Seq data set to provide a contrasting SNR regime context. 8.1 Full data set fit We begin by fitting a model to the entire data set in order to: obtain a baseline clssification performance against which to judge the performance obtained from the fits to smaller sample sets, obtain sample consistency scores which can be used to explain variability in the performance of model fitted to data sets of a fixed size, and produce a full model gene signature which can be used to evaluate the stability of selected features in models fitted to data sets of diffferent sizes. First assemble the data set. This entails simply re-combining the train and test data. ### CLEAR CACHE # combine train and test brcaRna_all_geneExpr_mtx &lt;- rbind(brcaRna_train_geneExpr_mtx, brcaRna_test_geneExpr_mtx) # we have to be careful with factors! # We&#39;ll keep as a character and change to factor when needed brcaRna_all_group_vec &lt;- c( as.character(brcaRna_train_group_vec), as.character(brcaRna_test_group_vec) ) # I suspect adding names to vectors breaks one of the tidy commandments, # but then again I am sure I have already offended the creed beyond salvation names(brcaRna_all_group_vec) &lt;- c( names(brcaRna_train_group_vec), names(brcaRna_test_group_vec) ) knitr::kable(table(group = brcaRna_all_group_vec), caption = &quot;samples by group&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 8.1: samples by group group Freq LumA 1492 Other 933 Now fit the losso model through cross-validation. Note that the results of a cv fit are random due to the random allocation of samples to folds. We can reduce this varibility by properly averaging results over repeated cv fits. Here we will obtain sample consistency scores by averaging results over 30 cv runs. set.seed(1) start_time &lt;- proc.time() brcaRna_cv_lassoAll_lst &lt;- lapply(1:30, function(REP) { glmnet::cv.glmnet( x = brcaRna_all_geneExpr_mtx, y = factor(brcaRna_all_group_vec,levels = c(&#39;Other&#39;, &#39;LumA&#39;)), alpha = 1, family = &#39;binomial&#39;, type.measure = &quot;class&quot;, keep = T, nlambda = 100 ) } ) message(&quot;lassoAll time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) load(file=file.path(&quot;RData&quot;,&#39;brcaRna_cv_lassoAll_lst&#39;)) Examine the fits. ### CLEAR CACHE plot( log(brcaRna_cv_lassoAll_lst[[1]]$lambda), brcaRna_cv_lassoAll_lst[[1]]$cvm, lwd=2, xlab=&#39;log(Lambda)&#39;, ylab=&#39;CV Misclassification Error&#39;, type=&#39;l&#39;, ylim=c(0, .5) ) for(JJ in 2:length(brcaRna_cv_lassoAll_lst)) lines( log(brcaRna_cv_lassoAll_lst[[JJ]]$lambda), brcaRna_cv_lassoAll_lst[[JJ]]$cvm, lwd=2 ) Figure 8.1: Repeated cv lasso models fitted to all samples These cv curves are remarkably consistent meaning that the determination of the size or sparsity of the model fitted through cross validation to the full data set is fairly precise: library(magrittr) par(mfrow=c(1,2), mar=c(3,4, 2, 1)) # nzero nzero_1se_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_min_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) boxplot(list(`1se`=nzero_1se_vec, min = nzero_min_vec), ylab=&quot;Selected Features&quot;) # error error_1se_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_min_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) boxplot( list(`1se`=error_1se_vec, min = error_min_vec), ylab=brcaRna_cv_lassoAll_lst[[1]]$name, ylim=c(0.10, .13) ) Figure 8.2: Feature selection and estimated error by repeated cv lasso models # tabular format tmp &lt;- data.frame(rbind( `features_1se` = summary(nzero_1se_vec), features_min = summary(nzero_min_vec), `features:min-1se` = summary(nzero_min_vec - nzero_1se_vec), `cv_error_1se` = summary(100*error_1se_vec), cv_error_min = summary(100*error_min_vec), `cv_error:1se-min` = summary(100*(error_1se_vec-error_min_vec)) )) knitr::kable(tmp %&gt;% dplyr::select(-Mean), caption = &quot;Number of selected features&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 8.2: Number of selected features Min. X1st.Qu. Median X3rd.Qu. Max. features_1se 97.0 118.5 144.0 162.0 208.0 features_min 144.0 221.2 258.0 291.0 451.0 features:min-1se 27.0 59.0 94.5 153.2 344.0 cv_error_1se 10.9 11.3 11.5 11.7 12.0 cv_error_min 10.6 10.8 10.9 11.1 11.5 cv_error:1se-min 0.2 0.4 0.5 0.7 0.8 The number of features selected by the minimum lambda models are larger than the number selected by the “one standard error” rule models by a median of 94.5. The cv error rates obtained from the minimum lambda models are lower then “one standard error” rule models error rates by a median of 0.5%. The cv error rates observed in this set are comparable to the rates oberved in the lasso models fitted to the training sample set which consisted of 80% of the samples in this set. In other words, there is no obvious gain in performance in moving from a data set with 1194 vs 746 samples to a data set with 1492 vs 933 samples. See Table ??. It’s not clear at this point whether the minimum lambda model is truly better than the “one standard error” rule model. We would need and external validation set to make this determination. We can compare the two sets of out-of-fold predicted values, averaged across cv replicates, to see if there is a meaningful difference between the two. # predicted probs - 1se lassoAll_predResp_1se_mtx &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) { ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_1se]) }) lassoAll_predResp_1se_vec &lt;- rowMeans(lassoAll_predResp_1se_mtx) # predicted probs - min lassoAll_predResp_min_mtx &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) { ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_min]) }) lassoAll_predResp_min_vec &lt;- rowMeans(lassoAll_predResp_min_mtx) # plot par(mfrow=c(1,2), mar=c(5,5,2,1)) tmp &lt;- c( `1se` = split(lassoAll_predResp_1se_vec, brcaRna_all_group_vec), min = split(lassoAll_predResp_min_vec, brcaRna_all_group_vec) ) names(tmp) &lt;- sub(&#39;\\\\.&#39;, &#39;\\n&#39;, names(tmp)) boxplot( tmp, ylab=&#39;Predicted oof probability&#39;, border=c(&#39;green&#39;, &#39;red&#39;), xaxt=&#39;n&#39; ) axis(side=1, at=1:length(tmp), tick=F, names(tmp)) # compare the two plot( x = lassoAll_predResp_1se_vec, xlab=&#39;1se model oof Prob&#39;, y = lassoAll_predResp_min_vec, ylab=&#39;min lambda model oof Prob&#39;, col = ifelse(brcaRna_all_group_vec == &#39;LumA&#39;, &#39;red&#39;, &#39;green&#39;) ) # Add reference lines at 10% false positive thres_1se &lt;- quantile(lassoAll_predResp_1se_vec[brcaRna_all_group_vec == &#39;Other&#39;], prob=.9) thres_min &lt;- quantile(lassoAll_predResp_min_vec[brcaRna_all_group_vec == &#39;Other&#39;], prob=.9) abline(v = thres_1se, h = thres_min, col=&#39;grey&#39;) Figure 8.3: Predicted probabilities - averaged over cv replicates We see that there isn’t a big difference in out-of-fold predicted probabilities between the one-standard-error rule and the minimum lamda models. One way to quantify the difference in classification errors is to classify samples according to each vector of predicted probabilities, setting the thresholds to achieve a fixed false positive rate, 10% say. These thresholds are indicated by the grey lines in the scatter plot on the right side of Figure ??. ### NOTE THAT HERE MODEL PRODUCES PROB(Other) lassoAll_predClass_1se_vec &lt;- ifelse( lassoAll_predResp_1se_vec &gt; thres_1se, &#39;LumA&#39;, &#39;Other&#39;) lassoAll_predClass_min_vec &lt;- ifelse( lassoAll_predResp_min_vec &gt; thres_min, &#39;LumA&#39;, &#39;Other&#39;) tmp &lt;- cbind( table(truth=brcaRna_all_group_vec, `1se-pred`=lassoAll_predClass_1se_vec), table(truth=brcaRna_all_group_vec, `min-pred`=lassoAll_predClass_min_vec) ) # HARD-WIRED Hack for printing!!!! colnames(tmp) &lt;- c(&#39;1se-LumA&#39;, &#39;1se-Other&#39;, &#39;min-LumA&#39;, &#39;min-Other&#39;) knitr::kable(tmp, caption = &quot;Classifications: rows are truth&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 8.3: Classifications: rows are truth 1se-LumA 1se-Other min-LumA min-Other LumA 1288 204 1304 188 Other 94 839 94 839 When we fix the false positive rate at 10% (ie. the other samples error rates are fixed), the 1se model makes 204 false negative calls whereas the minimum lambda model makes 188. A difference of 1.1% Get sample consistency scores To compute consistency scores, we will use the out-of-fold predicted probabilities. # get qual scores y &lt;- as.numeric(brcaRna_all_group_vec == &#39;LumA&#39;) # 1se p &lt;- lassoAll_predResp_1se_vec brcaRna_sample_1se_qual_vec &lt;- p^y*(1-p)^(1-y) # min p &lt;- lassoAll_predResp_min_vec brcaRna_sample_min_qual_vec &lt;- p^y*(1-p)^(1-y) We can examine consistency scores as a function of classification bin. y &lt;- as.numeric(brcaRna_all_group_vec == &#39;LumA&#39;) # 1se lassoAll_1se_conf_vec &lt;- paste( y, as.numeric(lassoAll_predClass_1se_vec==&#39;LumA&#39;), sep = &#39;:&#39; ) # min lassoAll_min_conf_vec &lt;- paste( y, as.numeric(lassoAll_predClass_min_vec==&#39;LumA&#39;), sep = &#39;:&#39; ) tmp &lt;- c( split(brcaRna_sample_1se_qual_vec, lassoAll_1se_conf_vec), split(brcaRna_sample_min_qual_vec, lassoAll_min_conf_vec) ) par(mfrow=c(1,2), mar=c(4,3,3,2), oma=c(2,2,2,0)) gplots::boxplot2(split(brcaRna_sample_1se_qual_vec, lassoAll_1se_conf_vec), outline=F, ylab = &#39;&#39;, border=c(&#39;green&#39;, &#39;green&#39;, &#39;red&#39;, &#39;red&#39;), ylim=c(0,1)) title(&#39;1se Model&#39;) gplots::boxplot2(split(brcaRna_sample_min_qual_vec, lassoAll_min_conf_vec), outline=F, ylab = &#39;&#39;, border=c(&#39;green&#39;, &#39;green&#39;, &#39;red&#39;, &#39;red&#39;), ylim=c(0,1)) title(&#39;min lambda Model&#39;) mtext(side=1, outer=T, cex=1.5, &#39;Classification - Truth:Predicted&#39;) mtext(side=2, outer=T, cex=1.5, &#39;Consistency Score&#39;) mtext(side=3, outer=T, cex=1.5, &#39;Sample Quality vs Classification Outcome&#39;) Figure 8.4: consistency scores by classification - Other=0, LumA=1 This figure shows that for false positive cases (0:1 or classifying a other as an LumA case), the algorithm is more certain of its predicted outcome than for the false negative cases (1:0 or classifying an LumA case as Other). ie. the misclassified Other samples are quite similar to LumA sample, whereas there is more ambiguity in the misclassified LumA samples. We will use the minimum lambda model to provide the fitted probabilities used to compute consistency scores, but we could have used either one. brcaRna_sample_qual_vec &lt;- brcaRna_sample_min_qual_vec 8.2 Selected feature list stability Before moving on to the simulation, let’s examine gene selection stability on the full data set. We have two sets of sellected features - one for the one standard deviation rule model, and one for the mimimum lambda model. We saw in Table ?? that the number of features selected by the minimum lambda models had an IQR of 221.25-291, while the one standard error rule models had an IQR of 118.5-162. Let’s examine the stability of the gene lists across cv replicates. ### CLEAR CACHE # 1se lassoAll_coef_1se_lst &lt;- lapply(brcaRna_cv_lassoAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix lassoAll_coef_1se_all &lt;- Reduce(union, lassoAll_coef_1se_lst) lassoAll_coef_1se_mtx &lt;- sapply(lassoAll_coef_1se_lst, function(LL) is.element(lassoAll_coef_1se_all, LL) ) rownames(lassoAll_coef_1se_mtx) &lt;- lassoAll_coef_1se_all genes_by_rep_1se_tbl &lt;- table(rowSums(lassoAll_coef_1se_mtx)) barplot( genes_by_rep_1se_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 8.5: Feature list stability for one standard error rule models We see that 83 features are included in every cv replicate. These make up between 51% and 70% (Q1 and Q3) of the cv replicate one standard error rule models feature lists. ### CLEAR CACHE # min lassoAll_coef_min_lst &lt;- lapply(brcaRna_cv_lassoAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix lassoAll_coef_min_all &lt;- Reduce(union, lassoAll_coef_min_lst) lassoAll_coef_min_mtx &lt;- sapply(lassoAll_coef_min_lst, function(LL) is.element(lassoAll_coef_min_all, LL) ) rownames(lassoAll_coef_min_mtx) &lt;- lassoAll_coef_min_all genes_by_rep_min_tbl &lt;- table(rowSums(lassoAll_coef_min_mtx)) barplot( genes_by_rep_min_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 8.6: Feature list stability for minimum lambda models We see that 110 features are included in every cv replicate. These make up between 38% and 50% (Q1 and Q3) of the cv replicate min feature lists. We will consider the genes that are selected in all cv replicates as a gene signature produced by each model. lasso_gene_sign_1se_vec &lt;- rownames(lassoAll_coef_1se_mtx)[rowSums(lassoAll_coef_1se_mtx)==30] lasso_gene_sign_min_vec &lt;- rownames(lassoAll_coef_min_mtx)[rowSums(lassoAll_coef_min_mtx)==30] 68 out of 83 of the genes in the 1se model gene signature are contained in the min lambda model gene signature. 8.3 Simulation Design We are now ready to run the simulations. SIM &lt;- 30 SIZE &lt;- c(25, 50, 100, 200, 300) CV_REP &lt;- 30 Simluation parameters: Number of simulations : SIM = 30 Sample sizes: SIZE = 25, 50, 100, 200, 300 Number of CV Replicates: CV_REP = 30 We will repeat the simulation process SIM = 30 times. For each simulation iteration, we will select 300 Other and 300 LumA samples at random. Models will be fitted and analyzed to balanced subsets of SIZE = 25, 50, 100, 200, 300, in a Matryoshka doll manner to emulate a typical sample accrual process. For a given simulation and a given sample size, we will obtain CV_REP = 30 cross-validated lasso fits. From these fits, we can obtain 30 out-of-fold assessments of classification accuracy to get a sense if its variability. From each cv replicate, we also obtain an estimated model size and a set of selected features. We will want to examine how these stabilize as the sample size increases. Note that we limit the simulations to a maximum of sample size of 300 in order to to have simulations with low overlap. With 300 randomly selected LumA samples, the expected overlap between two randomly selected sets of LumA samples is 4%. For Others the expected overlap is 10.3%. 8.4 Setup simulation To setup the simulation, we only need two master tables: one for the selection of Others and one for the selection of LumA samples. brcaRna_all_other_vec &lt;- names(brcaRna_all_group_vec[brcaRna_all_group_vec==&#39;Other&#39;]) brcaRna_all_LumA_vec &lt;- names(brcaRna_all_group_vec[brcaRna_all_group_vec==&#39;LumA&#39;]) We have 933 other sample IDs stored in brcaRna_all_other_vec and 1492 LumA sample IDs stored in brcaRna_all_LumA_vec. To create a suite of random samples from these, we only need to randomly select indices from each vector. ### CLEAR CACHE set.seed(12379) brcaRna_sim_other_mtx &lt;- sapply( 1:SIM, function(dummy) sample(1:length(brcaRna_all_other_vec), size = max(SIZE)) ) brcaRna_sim_LumA_mtx &lt;- sapply( 1:SIM, function(dummy) sample(1:length(brcaRna_all_LumA_vec), size = max(SIZE)) ) Each simulation is specified by a given column of the simulation design matrices: brcaRna_sim_other_mtx and brcaRna_sim_LumA_mtx, each with domensions 300, 30. Within each simulation, we can run the analyses of size 25, 50, 100, 200, 300 by simply selecting samples specified in the appropriate rows of each design matrix. We can examine how much variability we have in the consistency scores of the selected samples. Here we show results for the smalle sample sizes where variability will be the greatest. brcaRna_all_other_qual_vec &lt;- brcaRna_sample_qual_vec[brcaRna_all_other_vec] brcaRna_sim_other_qual_mtx &lt;- sapply( 1:ncol(brcaRna_sim_other_mtx), function(CC) brcaRna_all_other_qual_vec[brcaRna_sim_other_mtx[,CC]] ) brcaRna_all_LumA_qual_vec &lt;- brcaRna_sample_qual_vec[brcaRna_all_LumA_vec] brcaRna_sim_LumA_qual_mtx &lt;- sapply( 1:ncol(brcaRna_sim_LumA_mtx), function(CC) brcaRna_all_LumA_qual_vec[brcaRna_sim_LumA_mtx[,CC]] ) # ONLY LOOK AT SAMPLE SIZE == 50 NN &lt;- 50 # PLOT par(mfrow=c(2,1), mar = c(2,5,2,1)) # other boxplot( brcaRna_sim_other_qual_mtx[1:NN,], outline = T, ylab = &#39;Quality Score&#39;, xaxt = &#39;n&#39; ) title(&quot;Other sample consistency across simulations&quot;) # LumA boxplot( brcaRna_sim_LumA_qual_mtx[1:NN,], outline = T, ylab = &#39;Quality Score&#39; ) title(&quot;LumA sample consistency across simulations&quot;) Figure 8.7: sample consistency by simulation run for size = 50 In this figure, we are summarizing the consistency measures of 50 samples per group acress 30 simulations, or random selections of other and LumA samples. We see significant variability in sample consistency, especially in the Other cases. This may lead an unwary observer to be overly optimistic, or overly pessimistic, in the early accrual stages of a study. 8.5 Run simulations As these take a while to run, we will save the results of each similation to a different object and store to disk. These can be easily read from disk when needed for analysis. The simulation results are saved to the file system and only needs to be run once. The simulation takes \\(\\approx\\) 14 minutes per iteration, or \\(\\approx\\) 7 hours of run time on a laptop. (Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Mojave 10.14.6) start_time &lt;- proc.time() # Get stage from SIZE stage_vec &lt;- cut(1:nrow(brcaRna_sim_other_qual_mtx), c(0, SIZE), include.lowest = T) for (SIMno in 1:ncol(brcaRna_sim_other_qual_mtx)) { #cat(&quot;Running simulation &quot;, SIMno, &quot;\\n&quot;) brca_sim_cv_lst &lt;- lapply(1:length(levels(stage_vec)), function(STGno) { Stage_rows_vec &lt;- which(stage_vec %in% levels(stage_vec)[1:STGno]) #cat(&quot;Stage &quot;, STGno, &quot;- analyzing&quot;, length(Stage_rows_vec), &quot;paired samples.\\n&quot;) sim_stage_samples_vec &lt;- c( brcaRna_all_other_vec[brcaRna_sim_other_mtx[Stage_rows_vec, SIMno]], brcaRna_all_LumA_vec[brcaRna_sim_LumA_mtx[Stage_rows_vec, SIMno]] ) sim_stage_geneExpr_mtx &lt;- brcaRna_all_geneExpr_mtx[sim_stage_samples_vec, ] sim_stage_group_vec &lt;- brcaRna_all_group_vec[sim_stage_samples_vec] #print(table(sim_stage_group_vec)) sim_stage_cv_lst &lt;- lapply(1:CV_REP, function(CV) { cv_fit &lt;- glmnet::cv.glmnet( x = sim_stage_geneExpr_mtx, y = sim_stage_group_vec, alpha = 1, family = &quot;binomial&quot;, type.measure = &quot;class&quot;, keep = T, nlambda = 30 ) # Extract 1se metrics from cv_fit ####################### ndx_1se &lt;- which(cv_fit$lambda == cv_fit$lambda.1se) nzero_1se &lt;- cv_fit$nzero[ndx_1se] cvm_1se &lt;- cv_fit$cvm[ndx_1se] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(brcaRna_all_geneExpr_mtx), sim_stage_samples_vec) sim_stage_brcaRna_test_geneExpr_mtx &lt;- brcaRna_all_geneExpr_mtx[sim_stage_test_samples_vec,] sim_stage_brcaRna_test_group_vec &lt;- brcaRna_all_group_vec[sim_stage_test_samples_vec] test_pred_1se_vec &lt;- predict( cv_fit, newx=sim_stage_brcaRna_test_geneExpr_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_1se_error &lt;- mean(test_pred_1se_vec != sim_stage_brcaRna_test_group_vec) # genes coef_1se &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) genes_1se &lt;- coef_1se@Dimnames[[1]][coef_1se@i[-1]] # Extract min metrics from cv_fit ####################### ndx_min &lt;- which(cv_fit$lambda == cv_fit$lambda.min) nzero_min &lt;- cv_fit$nzero[ndx_min] cvm_min &lt;- cv_fit$cvm[ndx_min] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(brcaRna_all_geneExpr_mtx), sim_stage_samples_vec) sim_stage_brcaRna_test_geneExpr_mtx &lt;- brcaRna_all_geneExpr_mtx[sim_stage_test_samples_vec,] sim_stage_brcaRna_test_group_vec &lt;- brcaRna_all_group_vec[sim_stage_test_samples_vec] test_pred_min_vec &lt;- predict( cv_fit, newx=sim_stage_brcaRna_test_geneExpr_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_min_error &lt;- mean(test_pred_min_vec != sim_stage_brcaRna_test_group_vec) # genes coef_min &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) genes_min &lt;- coef_min@Dimnames[[1]][coef_min@i[-1]] # return cv_fit summary metrics list( p_1se = nzero_1se, p_min = nzero_min, cv_1se = cvm_1se, cv_min = cvm_min, test_1se=test_1se_error, test_min=test_min_error, genes_1se = genes_1se, genes_min = genes_min) }) sim_stage_cv_lst }) # save brcaRna_sim_cv_lst fName &lt;- paste0(&quot;brcaRna_sim_&quot;, SIMno, &quot;_cv_lst&quot;) assign(fName, brca_sim_cv_lst) save(list = fName, file=file.path(&quot;RData&quot;, fName)) } 8.6 Simulation results Recall the we have 30 simulations, or randomly selected sets of LumA and Other samples, analyzed in inreasing sizes of 25, 50, 100, 200, 300, with 30 repeated cross-validated lasso fits: Sample sizes: SIZE = 25, 50, 100, 200, 300 Number of CV Replicates: CV_REP = 30 First we extract simluation results and store into one big table (only showing the top of table shere): brcaRna_sim_files_vec &lt;- list.files(&#39;RData&#39;, &#39;^brcaRna_sim_&#39;) # define extraction methods # Each sumulation is a list of cv results ## nested in a list of replicates ############################################## # cvList2frm_f makes a frame out of the inner list cvList2frm_f &lt;- function(cv_lst) { frm1 &lt;- as.data.frame(t(sapply(cv_lst, function(x) x))) frm2 &lt;- data.frame( unlist(frm1[[1]]), unlist(frm1[[2]]), unlist(frm1[[3]]), unlist(frm1[[4]]), unlist(frm1[[5]]), unlist(frm1[[6]]), frm1[7], frm1[8]) names(frm2) &lt;- names(frm1) data.frame(Rep=1:nrow(frm2), frm2)} # cv_lst_to_frm loop over replicates, concatenating the inner list frames cv_lst_to_frm &lt;- function(brcaRna_sim_cv_lst) { do.call(&#39;rbind&#39;, lapply(1:length(brcaRna_sim_cv_lst), function(JJ) { siz_frm &lt;- cvList2frm_f(brcaRna_sim_cv_lst[[JJ]]) data.frame(Size=SIZE[JJ], siz_frm) })) } # we loop across simulations to combine all results into one big table brcaRna_lasso_sim_results_frm &lt;- do.call(&#39;rbind&#39;, lapply(1:length(brcaRna_sim_files_vec), function(SIM_NO) { load(file=file.path(&#39;RData&#39;, brcaRna_sim_files_vec[SIM_NO])) assign(&#39;brcaRna_sim_cv_lst&#39;, get(brcaRna_sim_files_vec[SIM_NO])) rm(list=brcaRna_sim_files_vec[SIM_NO]) data.frame(SimNo=paste0(&#39;Sim_&#39;,formatC(SIM_NO,width = 2,flag = 0)), cv_lst_to_frm(brcaRna_sim_cv_lst)) } )) 8.6.1 Simulation Results - look at one simulation 8.6.1.1 Model Accuracy Assessment First examine results for one simulation run. In the figures that follow, each boxplot summarized 30 repreated cross validation runs performed on a fixed random selection of Other and Affeced samples. Recall that as we move from 25 to 50, etc., the sample sets are growing to emulate an accrual of samples over time. ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(brcaRna_sim_other_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(brcaRna_lasso_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) # errors par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) ################### # 1se #################### cv_1se_lst &lt;- with(simNo_results_frm, split(cv_1se, Size)) names(cv_1se_lst) &lt;- paste0(stringr::str_pad(names(cv_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_1se_lst &lt;- with(simNo_results_frm, split(test_1se, Size)) names(test_1se_lst) &lt;- paste0(stringr::str_pad(names(test_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_1se_lst &lt;- c(cv_1se_lst, test_1se_lst) error_1se_lst &lt;- error_1se_lst[order(names(error_1se_lst))] boxplot(error_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 0.5), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_1se_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation other_qual_vec &lt;- sapply(split(brcaRna_sim_other_qual_mtx[,SimNum], stage_vec), median) LumA_qual_vec &lt;- sapply(split(brcaRna_sim_LumA_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(other_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(LumA_qual_vec, 2) ) }#SKIP # min #################### cv_min_lst &lt;- with(simNo_results_frm, split(cv_min, Size)) names(cv_min_lst) &lt;- paste0(stringr::str_pad(names(cv_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_min_lst &lt;- with(simNo_results_frm, split(test_min, Size)) names(test_min_lst) &lt;- paste0(stringr::str_pad(names(test_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_min_lst &lt;- c(cv_min_lst, test_min_lst) error_min_lst &lt;- error_min_lst[order(names(error_min_lst))] boxplot(error_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 0.5), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation other_qual_vec &lt;- sapply(split(brcaRna_sim_other_qual_mtx[,SimNum], stage_vec), median) LumA_qual_vec &lt;- sapply(split(brcaRna_sim_LumA_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(other_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(LumA_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 8.8: lasso Model Errors by Sample Size In this one simulation, we see: Model accuracy steadily increases with sample size CV error rates tend to be pessimistic, expecially for the small sample sizes and more so for the 1se models. There isn’t much to chose from between the one standard error and the minimum lambda models. 8.6.1.2 Feature Selection ### CLEAR CACHE # get full model nzero ref nzero_1se_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_1se_q2 &lt;- quantile(nzero_1se_vec, prob=c(2)/4) nzero_min_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) nzero_min_q2 &lt;- quantile(nzero_min_vec, prob=c(2)/4) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(brcaRna_sim_other_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(brcaRna_lasso_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) ################### # 1se #################### # selected feature counts p_1se_lst &lt;- with(simNo_results_frm, split(p_1se, Size)) names(p_1se_lst) &lt;- paste0(stringr::str_pad(names(p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of lasso_gene_sign_1se_vec # - the signature selected genes sign_genes_1se_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_1se&#39;]), lasso_gene_sign_1se_vec)) sign_p_1se_lst &lt;- split(sapply(sign_genes_1se_lst, length), simNo_results_frm$Size) names(sign_p_1se_lst) &lt;- paste0(stringr::str_pad(names(sign_p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_1se_lst &lt;- c(p_1se_lst, sign_p_1se_lst) p_singP_1se_lst &lt;- p_singP_1se_lst[order(names(p_singP_1se_lst))] boxplot(p_singP_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;number of selected features&#39;) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;) ) SKIP &lt;- function() { # Add qual annotation other_qual_vec &lt;- sapply(split(brcaRna_sim_other_qual_mtx[,SimNum], stage_vec), median) LumA_qual_vec &lt;- sapply(split(brcaRna_sim_LumA_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(other_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(LumA_qual_vec, 2) ) }#SKIP ################### # min #################### # selected feature counts p_min_lst &lt;- with(simNo_results_frm, split(p_min, Size)) names(p_min_lst) &lt;- paste0(stringr::str_pad(names(p_min_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of lasso_gene_sign_min_vec # - the signature selected genes sign_genes_min_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_min&#39;]), lasso_gene_sign_min_vec)) sign_p_min_lst &lt;- split(sapply(sign_genes_min_lst, length), simNo_results_frm$Size) names(sign_p_min_lst) &lt;- paste0(stringr::str_pad(names(sign_p_min_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_min_lst &lt;- c(p_min_lst, sign_p_min_lst) p_singP_min_lst &lt;- p_singP_min_lst[order(names(p_singP_min_lst))] boxplot(p_singP_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 200), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) SKIP &lt;- function() { # Add qual annotation other_qual_vec &lt;- sapply(split(brcaRna_sim_other_qual_mtx[,SimNum], stage_vec), median) LumA_qual_vec &lt;- sapply(split(brcaRna_sim_LumA_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(other_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(LumA_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 8.9: lasso Models Selected Features by Sample Size Compare with Figures ?? and ?? for the HCC 5hmC data. Reader Note IN the case of discriminating between LumA and other breast cancer subtypes, we could use features in the LumA breast cancer subtype signature as positive controls for feature selection. To Do. 8.6.2 Summarize results across simulation runs. 8.6.2.1 Model Accuracy Assessment Now look acoss all simulations. In the figures that follow, each boxplot summarizes the results of 30 simulations. For a give sample size and a given simulation, each data point is the median across 30 repeated cv runs. ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(brcaRna_cv_lassoAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(brcaRna_sim_other_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) # 1se ######################################### ## cv cv_1se_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_1se_lst &lt;- with(sizeVal_results_frm, split(cv_1se, SimNo)) sapply(sizeVal_cv_1se_lst, median) }) names(cv_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_1se_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_1se_lst &lt;- with(sizeVal_results_frm, split(test_1se, SimNo)) sapply(sizeVal_test_1se_lst, median) }) names(test_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_1se_Bysize_lst &lt;- c(cv_1se_Bysize_lst, test_1se_Bysize_lst) error_1se_Bysize_lst &lt;- error_1se_Bysize_lst[order(names(error_1se_Bysize_lst))] boxplot(error_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, .5), outline=F, xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;Misclassification Error&#39;) for(JJ in 1:length(error_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(error_1se_Bysize_lst[[JJ]])), amount=0.25), y=error_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda models&#39;)) # min ######################################### ## cv cv_min_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_min_lst &lt;- with(sizeVal_results_frm, split(cv_min, SimNo)) sapply(sizeVal_cv_min_lst, median) }) names(cv_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_min_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_min_lst &lt;- with(sizeVal_results_frm, split(test_min, SimNo)) sapply(sizeVal_test_min_lst, median) }) names(test_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_min_Bysize_lst &lt;- c(cv_min_Bysize_lst, test_min_Bysize_lst) error_min_Bysize_lst &lt;- error_min_Bysize_lst[order(names(error_min_Bysize_lst))] boxplot(error_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, .5), outline=F, xaxt=&#39;n&#39; ) for(JJ in 1:length(error_min_Bysize_lst)) points( x=jitter(rep(JJ, length(error_min_Bysize_lst[[JJ]])), amount=0.25), y=error_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;lasso fit error rates summarized across simulations&#39;)) Figure 8.10: lasso Model Errors by Sample Size As in the case of the analysis of the HCC 5hmC data, we see that at lower samples sizes the assessed performance is quite variable. In this example cohorts of N=25 LumA and Other subtype samples can yield classification error estimates ranging from less than %10 to over 50%. To appreciate how much variability can be encountered as samples are accrued over time we need to look at a typical path the assessed model accuracy estimates might take. ### CLEAR CACHE error_1se_Bysize_mtx &lt;- do.call(&#39;cbind&#39;, lapply(error_1se_Bysize_lst, function(LL) LL)) cv_error_1se_Bysize_mtx &lt;- error_1se_Bysize_mtx[,grep(&#39;_cv&#39;, colnames(error_1se_Bysize_mtx))] plot(x=c(1, ncol(cv_error_1se_Bysize_mtx)), y=c(0,0.6), xlab=&#39;sample size&#39;, ylab=&#39;Misclassification Error&#39;, type=&#39;n&#39;, xaxt=&#39;n&#39;) axis(side=1, at=1:ncol(cv_error_1se_Bysize_mtx), labels=sub(&#39;_cv&#39;,&#39;&#39;,colnames(cv_error_1se_Bysize_mtx))) for(JJ in 1:15) lines(x=1:ncol(cv_error_1se_Bysize_mtx), y=cv_error_1se_Bysize_mtx[JJ,], type=&#39;b&#39;, pch=JJ, col=JJ) title(&#39;Example Misclassification Error Paths&#39;) Figure 8.11: lasso Model Error Paths We see how erratic the assessed model accuracy can be when sample sizes are small, and that it would be hard to guess the ultimate level of accuracy the is achievable, or the number of samples required to get a reasonable estimate of the achievable level of accuracy. 8.6.2.2 Feature Selection ### CLEAR CACHE # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(brcaRna_sim_other_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0)) # 1se ######################################### # selected features p_1se_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_1se_lst &lt;- with(sizeVal_results_frm, split(p_1se, SimNo)) sapply(sizeVal_p_1se_lst, median) }) names(p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_1se_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_1se_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_1se&#39;]), lasso_gene_sign_1se_vec)) sizeVal_sign_p_1se_lst &lt;- split(sapply(sizeVal_sign_genes_1se_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_1se_lst, median) }) names(sign_p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_1se_Bysize_lst &lt;- c(p_1se_Bysize_lst, sign_p_1se_Bysize_lst) p_singP_1se_Bysize_lst &lt;- p_singP_1se_Bysize_lst[order(names(p_singP_1se_Bysize_lst))] boxplot(p_singP_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 300), xaxt=&#39;n&#39; ) mtext(side=2, outer=T, &#39;number of selected features&#39;) for(JJ in 1:length(p_singP_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_1se_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lamdba models&#39;)) # min ######################################### # selected features p_min_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_min_lst &lt;- with(sizeVal_results_frm, split(p_min, SimNo)) sapply(sizeVal_p_min_lst, median) }) names(p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_min_Bysize_lst &lt;- lapply(unique(brcaRna_lasso_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- brcaRna_lasso_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_min_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_min&#39;]), lasso_gene_sign_min_vec)) sizeVal_sign_p_min_lst &lt;- split(sapply(sizeVal_sign_genes_min_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_min_lst, median) }) names(sign_p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(brcaRna_lasso_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_min_Bysize_lst &lt;- c(p_min_Bysize_lst, sign_p_min_Bysize_lst) p_singP_min_Bysize_lst &lt;- p_singP_min_Bysize_lst[order(names(p_singP_min_Bysize_lst))] boxplot(p_singP_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0, 300), xaxt=&#39;n&#39; ) for(JJ in 1:length(p_singP_min_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_min_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda models&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;lasso fit feature selection summarized across simulations&#39;)) Figure 8.12: lasso Models Selected Features by Sample Size Compare with Figures ?? and ?? for the HCC 5hmC data. ### CLEAR CACHE other_samp25_qual_vec &lt;- apply(brcaRna_sim_other_qual_mtx[1:25, ], 2, median) LumA_samp25_qual_vec &lt;- apply(brcaRna_sim_LumA_qual_mtx[1:25, ], 2, median) samp25_qual_error_mtx &lt;- cbind( `1se_cv` = error_1se_Bysize_lst[[&#39;025_cv&#39;]], `1se_test` = error_1se_Bysize_lst[[&#39;025_test&#39;]], `other_Q` = other_samp25_qual_vec, `LumA_Q` = LumA_samp25_qual_vec) # Correlation panel panel.cor &lt;- function(x, y){ usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- round(cor(x, y), digits=2) txt &lt;- paste0(&quot;R = &quot;, r) cex.cor &lt;- 0.8/strwidth(txt) text(0.5, 0.5, txt, cex = 1.5) ###cex.cor * r) } pairs(samp25_qual_error_mtx, lower.panel = panel.cor) "],
["conclusions.html", "Section 9 Conclusions", " Section 9 Conclusions We have found that … Other questions … "],
["references.html", "References", " References "],
["appendix-1.html", "Appendix 1 - Sample size in elastic net fits to HCC 5hmC-Seq Data Selected feature list stability Run simulations - enet enet Simulation results", " Appendix 1 - Sample size in elastic net fits to HCC 5hmC-Seq Data Repeat the analyses from Section ??, but using the elastic net as classfication model. set.seed(1) start_time &lt;- proc.time() hcc5hmC_cv_enetAll_lst &lt;- lapply(1:30, function(REP) { glmnet::cv.glmnet( x = all_lcpm_mtx, y = factor(hcc5hmC_hcc5hmC,levels = c(&#39;Control&#39;, &#39;HCC&#39;)), alpha = 0.5, family = &#39;binomial&#39;, type.measure = &quot;class&quot;, keep = T, nlambda = 100 ) } ) message(&quot;enetAll time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) Examine the fits. ### CLEAR CACHE plot( log(hcc5hmC_cv_enetAll_lst[[1]]$lambda), hcc5hmC_cv_enetAll_lst[[1]]$cvm, lwd=2, xlab=&#39;log(Lambda)&#39;, ylab=&#39;CV Misclassification Error&#39;, type=&#39;l&#39;, ylim=c(0, .5) ) for(JJ in 2:length(hcc5hmC_cv_enetAll_lst)) lines( log(hcc5hmC_cv_enetAll_lst[[JJ]]$lambda), hcc5hmC_cv_enetAll_lst[[JJ]]$cvm, lwd=2 ) Figure 9.1: Repeated cv enet models fitted to all samples These cv curves are again remarkably consistent meaning that the determination of the size or sparsity of the model through cross validation is fairly precise: library(magrittr) par(mfrow=c(1,2), mar=c(3,4, 2, 1)) # nzero nzero_1se_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_min_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) boxplot(list(`1se`=nzero_1se_vec, min = nzero_min_vec), ylab=&quot;Full Model cv Summary&quot;) # error error_1se_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_min_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) boxplot( list(`1se`=error_1se_vec, min = error_min_vec), ylab=hcc5hmC_cv_enetAll_lst[[1]]$name, ylim=c(0.06, .10) ) Figure 9.2: Feature selection and estimated error by repeated cv enet models # tabular format tmp &lt;- data.frame(rbind( `features_1se` = summary(nzero_1se_vec), features_min = summary(nzero_min_vec), `features:min-1se` = summary(nzero_min_vec - nzero_1se_vec), `cv_error_1se` = summary(100*error_1se_vec), cv_error_min = summary(100*error_min_vec), `cv_error:1se-min` = summary(100*(error_1se_vec-error_min_vec)) )) knitr::kable(tmp %&gt;% dplyr::select(-Mean), caption = &quot;Number of selected features&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 9.1: Number of selected features Min. X1st.Qu. Median X3rd.Qu. Max. features_1se 94.0 174.0 183.0 215.0 406.0 features_min 183.0 271.5 357.5 457.2 501.0 features:min-1se 23.0 81.5 168.5 237.5 331.0 cv_error_1se 7.1 7.4 7.6 7.7 8.3 cv_error_min 6.5 6.8 7.0 7.2 7.5 cv_error:1se-min 0.2 0.5 0.6 0.7 0.9 The number of features selected by the minimum lambda model is larger than the number selected by the “one standard error” rule by a median of \\(168.5\\) and results on a median reduction in cv error rates of \\(0.6\\)%. The cv error rates observed in this set are comparable to the rates oberved in the enet models fitted to the training sample set which consisted of 80% of the samples in this set. See Table ??. It’s not clear at this point whether the minimum lambda model is better than the “one standard error” rule model. We would need and external validation set to make this determination. We can compare the two sets of out-of-fold predicted values, averaged across cv replicates, to see if there is a meaningful difference between the two. # predicted probs - 1se enetAll_predResp_1se_mtx &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) { ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_1se]) }) enetAll_predResp_1se_vec &lt;- rowMeans(enetAll_predResp_1se_mtx) # predicted probs - min enetAll_predResp_min_mtx &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) { ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) logistic_f(cv_fit$fit.preval[,ndx_min]) }) enetAll_predResp_min_vec &lt;- rowMeans(enetAll_predResp_min_mtx) # plot par(mfrow=c(1,2), mar=c(5,5,2,1)) tmp &lt;- c( `1se` = split(enetAll_predResp_1se_vec, hcc5hmC_all_group_vec), min = split(enetAll_predResp_min_vec, hcc5hmC_all_group_vec) ) names(tmp) &lt;- sub(&#39;\\\\.&#39;, &#39;\\n&#39;, names(tmp)) boxplot( tmp, ylab=&#39;Predicted oof probability&#39;, border=c(&#39;green&#39;, &#39;red&#39;), xaxt=&#39;n&#39; ) axis(side=1, at=1:length(tmp), tick=F, names(tmp)) # compare the two plot( x = enetAll_predResp_1se_vec, xlab=&#39;1se model oof Prob&#39;, y = enetAll_predResp_min_vec, ylab=&#39;min lambda model oof Prob&#39;, col = ifelse(hcc5hmC_all_group_vec == &#39;HCC&#39;, &#39;red&#39;, &#39;green&#39;) ) # Add referecne lines at 10% false positive thres_1se &lt;- quantile(enetAll_predResp_1se_vec[hcc5hmC_all_group_vec == &#39;Control&#39;], prob=.9) thres_min &lt;- quantile(enetAll_predResp_min_vec[hcc5hmC_all_group_vec == &#39;Control&#39;], prob=.9) abline(v = thres_1se, h = thres_min, col=&#39;grey&#39;) Figure 9.3: Predicted probabilities - averaged over cv replicates We see that there isn’t a big difference in out-of-fold predicted probabilities between the one-standard-error rule ans minimum lamda models. One way to quantify the difference in classification errors is to classify samples according to each vector of predicted probabilities, setting the thresholds to achieve a fixed false positive rate, 10% say. These thresholds are indicated by the grey lines in the scatter plot on the right side of Figure ??. enetAll_predClass_1se_vec &lt;- ifelse( enetAll_predResp_1se_vec &gt; thres_1se, &#39;HCC&#39;, &#39;Control&#39;) enetAll_predClass_min_vec &lt;- ifelse( enetAll_predResp_min_vec &gt; thres_min, &#39;HCC&#39;, &#39;Control&#39;) tmp &lt;- cbind( table(truth=hcc5hmC_all_group_vec, `1se-pred`=enetAll_predClass_1se_vec), table(truth=hcc5hmC_all_group_vec, `min-pred`=enetAll_predClass_min_vec) ) # Hack for printing colnames(tmp) &lt;- c(&#39;1se-Control&#39;, &#39;1se-HCC&#39;, &#39;min-Control&#39;, &#39;min-HCC&#39;) knitr::kable(tmp, caption = &quot;Classifications: rows are truth&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 9.2: Classifications: rows are truth 1se-Control 1se-HCC min-Control min-HCC Control 700 78 700 78 HCC 34 521 25 530 When we fix the false positive rate at 10%, the 1se model makes 39 false negative calls whereas the minimum lambda model makes 32. A difference of \\(1.3\\)% Selected feature list stability Before moving on to the simulation, let’s examine gene selection stability on the full data set. We have two sets of sellected features - one for the one standard deviation rile model, and one for the mimimum lambda model. We saw in Table ?? that the number of features selected by the minimum lambda models had an IQR of \\(271.5-457.25\\), while the one standard error rule models had an IQR of \\(174-215\\). Let’s examine the stability of the gene lists across cv replicates. ### CLEAR CACHE # 1se enetAll_coef_1se_lst &lt;- lapply(hcc5hmC_cv_enetAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix enetAll_coef_1se_all &lt;- Reduce(union, enetAll_coef_1se_lst) enetAll_coef_1se_mtx &lt;- sapply(enetAll_coef_1se_lst, function(LL) is.element(enetAll_coef_1se_all, LL) ) rownames(enetAll_coef_1se_mtx) &lt;- enetAll_coef_1se_all genes_by_rep_1se_tbl &lt;- table(rowSums(enetAll_coef_1se_mtx)) barplot( genes_by_rep_1se_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 9.4: Feature list stability for one standard error rule models We see that \\(76\\) features are included in every cv replicate. These make up between \\(35\\)% and \\(44\\)% (Q1 and Q3) of the cv replicate one standard error rule models feature lists. ### CLEAR CACHE # min enetAll_coef_min_lst &lt;- lapply(hcc5hmC_cv_enetAll_lst, function(cv_fit){ cv_fit_coef &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]] }) # put into matrix enetAll_coef_min_all &lt;- Reduce(union, enetAll_coef_min_lst) enetAll_coef_min_mtx &lt;- sapply(enetAll_coef_min_lst, function(LL) is.element(enetAll_coef_min_all, LL) ) rownames(enetAll_coef_min_mtx) &lt;- enetAll_coef_min_all genes_by_rep_min_tbl &lt;- table(rowSums(enetAll_coef_min_mtx)) barplot( genes_by_rep_min_tbl, xlab=&#39;Number of Replicates&#39;, ylab=&#39;Number of features&#39; ) Figure 9.5: Feature list stability for minimum lambda models We see that \\(168\\) features are included in every cv replicate. These make up between \\(37\\)% and \\(62\\)% (Q1 and Q3) of the cv replicate min feature lists. We will consider the genes that are selected in all cv replicates as a gene signature produced by each model. enet_gene_sign_1se_vec &lt;- rownames(enetAll_coef_1se_mtx)[rowSums(enetAll_coef_1se_mtx)==30] enet_gene_sign_min_vec &lt;- rownames(enetAll_coef_min_mtx)[rowSums(enetAll_coef_min_mtx)==30] 76 out of 76 of the genes in the 1se model gene signature are contained in the min lambda model gene signature. Run simulations - enet As these make take a while to run, we will save the results of each similation to a different object and store to disk. These can be easily read from disk when needed for analysis. The simulation saves results to the file system and only needs to be run once. The simulation takes \\(\\approx\\) 8 minutes per iteration, or 4 hours of run time on a laptop. (Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Mojave 10.14.6) ### CLEAR CACHE start_time &lt;- proc.time() # Get stage from SIZE stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0, SIZE), include.lowest = T) # ran in two runs 1:7, 8:ncol for (SIMno in 8:ncol(sim_control_qual_mtx)) { #cat(&quot;Running simulation &quot;, SIMno, &quot;\\n&quot;) sim_cv_lst &lt;- lapply(1:length(levels(stage_vec)), function(STGno) { Stage_rows_vec &lt;- which(stage_vec %in% levels(stage_vec)[1:STGno]) #cat(&quot;Stage &quot;, STGno, &quot;- analyzing&quot;, length(Stage_rows_vec), &quot;paired samples.\\n&quot;) sim_stage_samples_vec &lt;- c( all_control_vec[sim_control_mtx[Stage_rows_vec, SIMno]], all_affected_vec[sim_affected_mtx[Stage_rows_vec, SIMno]] ) sim_stage_lcpm_mtx &lt;- all_lcpm_mtx[sim_stage_samples_vec, ] sim_stage_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_samples_vec] #print(table(sim_stage_group_vec)) sim_stage_cv_lst &lt;- lapply(1:CV_REP, function(CV) { cv_fit &lt;- glmnet::cv.glmnet( x = sim_stage_lcpm_mtx, y = sim_stage_group_vec, alpha = 1, family = &quot;binomial&quot;, type.measure = &quot;class&quot;, keep = T, nlambda = 30 ) # Extract 1se metrics from cv_fit ####################### ndx_1se &lt;- which(cv_fit$lambda == cv_fit$lambda.1se) nzero_1se &lt;- cv_fit$nzero[ndx_1se] cvm_1se &lt;- cv_fit$cvm[ndx_1se] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(all_lcpm_mtx), sim_stage_samples_vec) sim_stage_test_lcpm_mtx &lt;- all_lcpm_mtx[sim_stage_test_samples_vec,] sim_stage_test_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_test_samples_vec] test_pred_1se_vec &lt;- predict( cv_fit, newx=sim_stage_test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_1se_error &lt;- mean(test_pred_1se_vec != sim_stage_test_group_vec) # genes coef_1se &lt;- coef( cv_fit, s = &quot;lambda.1se&quot; ) genes_1se &lt;- coef_1se@Dimnames[[1]][coef_1se@i[-1]] # Extract min metrics from cv_fit ####################### ndx_min &lt;- which(cv_fit$lambda == cv_fit$lambda.min) nzero_min &lt;- cv_fit$nzero[ndx_min] cvm_min &lt;- cv_fit$cvm[ndx_min] # test error sim_stage_test_samples_vec &lt;- setdiff(rownames(all_lcpm_mtx), sim_stage_samples_vec) sim_stage_test_lcpm_mtx &lt;- all_lcpm_mtx[sim_stage_test_samples_vec,] sim_stage_test_group_vec &lt;- hcc5hmC_all_group_vec[sim_stage_test_samples_vec] test_pred_min_vec &lt;- predict( cv_fit, newx=sim_stage_test_lcpm_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_min_error &lt;- mean(test_pred_min_vec != sim_stage_test_group_vec) # genes coef_min &lt;- coef( cv_fit, s = &quot;lambda.min&quot; ) genes_min &lt;- coef_min@Dimnames[[1]][coef_min@i[-1]] # return cv_fit summary metrics list( p_1se = nzero_1se, p_min = nzero_min, cv_1se = cvm_1se, cv_min = cvm_min, test_1se=test_1se_error, test_min=test_min_error, genes_1se = genes_1se, genes_min = genes_min) }) sim_stage_cv_lst }) # save sim_cv_lst fName &lt;- paste0(&quot;enet_sim_&quot;, SIMno, &quot;_cv_lst&quot;) assign(fName, sim_cv_lst) save(list = fName, file=file.path(&quot;RData&quot;, fName)) } message(&quot;simulation time: &quot;, round((proc.time() - start_time)[3], 2), &quot;s&quot;) enet Simulation results Simulation Results - look at one simulation First examine results for one simulation run. ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(enet_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) # errors par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0)) ################### # 1se #################### cv_1se_lst &lt;- with(simNo_results_frm, split(cv_1se, Size)) names(cv_1se_lst) &lt;- paste0(stringr::str_pad(names(cv_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_1se_lst &lt;- with(simNo_results_frm, split(test_1se, Size)) names(test_1se_lst) &lt;- paste0(stringr::str_pad(names(test_1se_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_1se_lst &lt;- c(cv_1se_lst, test_1se_lst) error_1se_lst &lt;- error_1se_lst[order(names(error_1se_lst))] boxplot(error_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0.05, .4), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_1se_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda - error rates&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_lst)), round(affected_qual_vec, 2) ) }#SKIP # min #################### cv_min_lst &lt;- with(simNo_results_frm, split(cv_min, Size)) names(cv_min_lst) &lt;- paste0(stringr::str_pad(names(cv_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) test_min_lst &lt;- with(simNo_results_frm, split(test_min, Size)) names(test_min_lst) &lt;- paste0(stringr::str_pad(names(test_min_lst), width=3, pad=&#39;0&#39;),&#39;_cv&#39;) error_min_lst &lt;- c(cv_min_lst, test_min_lst) error_min_lst &lt;- error_min_lst[order(names(error_min_lst))] boxplot(error_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0.05, .4), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda - error rates&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_lst)), round(affected_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 9.6: enet Model Errors by Sample Size ### CLEAR CACHE # get full model nzero ref nzero_1se_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se]) nzero_1se_q2 &lt;- quantile(nzero_1se_vec, prob=c(2)/4) nzero_min_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min]) nzero_min_q2 &lt;- quantile(nzero_min_vec, prob=c(2)/4) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) #SIM &lt;- &quot;Sim_01&quot; for(SIM in unique(enet_sim_results_frm$SimNo)[1]){ SimNum &lt;- as.numeric(sub(&#39;Sim_&#39;,&#39;&#39;,SIM)) simNo_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(SimNo==SIM) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0)) ################### # 1se #################### # selected feature counts p_1se_lst &lt;- with(simNo_results_frm, split(p_1se, Size)) names(p_1se_lst) &lt;- paste0(stringr::str_pad(names(p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of enet_gene_sign_1se_vec # - the signature selected genes sign_genes_1se_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_1se&#39;]), enet_gene_sign_1se_vec)) sign_p_1se_lst &lt;- split(sapply(sign_genes_1se_lst, length), simNo_results_frm$Size) names(sign_p_1se_lst) &lt;- paste0(stringr::str_pad(names(sign_p_1se_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_1se_lst &lt;- c(p_1se_lst, sign_p_1se_lst) p_singP_1se_lst &lt;- p_singP_1se_lst[order(names(p_singP_1se_lst))] boxplot(p_singP_1se_lst, border=c(&#39;blue&#39;,&#39;green&#39;), #ylim=c(0, 300), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lamdba - selected gene counts&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_lst)), round(affected_qual_vec, 2) ) }#SKIP ################### # min #################### # selected feature counts p_min_lst &lt;- with(simNo_results_frm, split(p_min, Size)) names(p_min_lst) &lt;- paste0(stringr::str_pad(names(p_min_lst), width=3, pad=&#39;0&#39;),&#39;_p&#39;) # get selected features that are part of enet_gene_sign_min_vec # - the signature selected genes sign_genes_min_lst &lt;- lapply(1:nrow(simNo_results_frm), function(RR) intersect(unlist(simNo_results_frm[RR, &#39;genes_min&#39;]), enet_gene_sign_min_vec)) sign_p_min_lst &lt;- split(sapply(sign_genes_min_lst, length), simNo_results_frm$Size) names(sign_p_min_lst) &lt;- paste0(stringr::str_pad(names(sign_p_min_lst), width=3, pad=&#39;0&#39;),&#39;_signP&#39;) p_singP_min_lst &lt;- c(p_min_lst, sign_p_min_lst) p_singP_min_lst &lt;- p_singP_min_lst[order(names(p_singP_min_lst))] boxplot(p_singP_min_lst, border=c(&#39;blue&#39;,&#39;green&#39;), #ylim=c(0, 300), xaxt=&#39;n&#39; ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda - selected gene counts&#39;)) SKIP &lt;- function() { # Add qual annotation control_qual_vec &lt;- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median) affected_qual_vec &lt;- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(control_qual_vec, 2) ) LL &lt;- LL + 1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_lst)), round(affected_qual_vec, 2) ) }#SKIP mtext(side=3, outer=T, cex=1.25, paste(&#39;Sim =&#39;, SIM)) } # for(SIM Figure 9.7: enet Models Selected Features by Sample Size Summarize results across simulation runs ### CLEAR CACHE # get full model cv error ref error_1se_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se]) error_1se_q2 &lt;- quantile(error_1se_vec, prob=1/2) error_min_vec &lt;- sapply(hcc5hmC_cv_enetAll_lst, function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min]) error_min_q2 &lt;- quantile(error_min_vec, prob=1/2) # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0)) # 1se ######################################### ## cv cv_1se_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_1se_lst &lt;- with(sizeVal_results_frm, split(cv_1se, SimNo)) sapply(sizeVal_cv_1se_lst, median) }) names(cv_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_1se_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_1se_lst &lt;- with(sizeVal_results_frm, split(test_1se, SimNo)) sapply(sizeVal_test_1se_lst, median) }) names(test_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_1se_Bysize_lst &lt;- c(cv_1se_Bysize_lst, test_1se_Bysize_lst) error_1se_Bysize_lst &lt;- error_1se_Bysize_lst[order(names(error_1se_Bysize_lst))] boxplot(error_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0.05, .5), outline=F, xaxt=&#39;n&#39; ) for(JJ in 1:length(error_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(error_1se_Bysize_lst[[JJ]])), amount=0.25), y=error_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lambda - error rates&#39;)) # min ######################################### ## cv cv_min_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_cv_min_lst &lt;- with(sizeVal_results_frm, split(cv_min, SimNo)) sapply(sizeVal_cv_min_lst, median) }) names(cv_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_cv&#39;) ## test test_min_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_test_min_lst &lt;- with(sizeVal_results_frm, split(test_min, SimNo)) sapply(sizeVal_test_min_lst, median) }) names(test_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_test&#39;) error_min_Bysize_lst &lt;- c(cv_min_Bysize_lst, test_min_Bysize_lst) error_min_Bysize_lst &lt;- error_min_Bysize_lst[order(names(error_min_Bysize_lst))] boxplot(error_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), ylim=c(0.05, .5), outline=F, xaxt=&#39;n&#39; ) for(JJ in 1:length(error_min_Bysize_lst)) points( x=jitter(rep(JJ, length(error_min_Bysize_lst[[JJ]])), amount=0.25), y=error_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;cv&#39;, names(error_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_cv&#39;),names(error_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) abline(h= error_min_q2, col = &#39;red&#39;) legend(&#39;topright&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;,&#39;green&#39;), legend = c(&#39;cv error&#39;, &#39;test set&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda - error rates&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;enet fit error rates summarized across simulations&#39;)) Figure 9.8: enet Model Errors by Sample Size ### CLEAR CACHE error_1se_Bysize_sum_frm &lt;- t(sapply(error_1se_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4))) colnames(error_1se_Bysize_sum_frm) &lt;- paste0(&#39;1se_&#39;, colnames(error_1se_Bysize_sum_frm)) error_min_Bysize_sum_frm &lt;- t(sapply(error_min_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4))) colnames(error_min_Bysize_sum_frm) &lt;- paste0(&#39;min_&#39;, colnames(error_min_Bysize_sum_frm)) knitr::kable(cbind(`1se`=error_1se_Bysize_sum_frm, min=error_min_Bysize_sum_frm), caption = paste(&quot;elastic net error rates by sample size across simulations&quot;), digits=2) %&gt;% kableExtra::kable_styling(full_width = F) Table 9.3: elastic net error rates by sample size across simulations 1se_25% 1se_50% 1se_75% min_25% min_50% min_75% 025_cv 0.30 0.32 0.40 0.26 0.29 0.38 025_test 0.27 0.30 0.36 0.28 0.32 0.37 050_cv 0.20 0.23 0.28 0.18 0.21 0.25 050_test 0.22 0.24 0.26 0.21 0.24 0.26 100_cv 0.16 0.18 0.20 0.15 0.17 0.18 100_test 0.17 0.18 0.19 0.16 0.17 0.18 200_cv 0.12 0.13 0.13 0.11 0.12 0.12 200_test 0.11 0.12 0.13 0.11 0.12 0.12 300_cv 0.10 0.10 0.11 0.09 0.10 0.10 300_test 0.09 0.10 0.10 0.08 0.09 0.10 Now look at feature selection. ### CLEAR CACHE # Utility objects SIZE0 &lt;- stringr::str_pad(SIZE, width=3, pad=&#39;0&#39;) stage_vec &lt;- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T) par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0)) # 1se ######################################### # selected features p_1se_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_1se_lst &lt;- with(sizeVal_results_frm, split(p_1se, SimNo)) sapply(sizeVal_p_1se_lst, median) }) names(p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_1se_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_1se_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_1se&#39;]), enet_gene_sign_1se_vec)) sizeVal_sign_p_1se_lst &lt;- split(sapply(sizeVal_sign_genes_1se_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_1se_lst, median) }) names(sign_p_1se_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_1se_Bysize_lst &lt;- c(p_1se_Bysize_lst, sign_p_1se_Bysize_lst) p_singP_1se_Bysize_lst &lt;- p_singP_1se_Bysize_lst[order(names(p_singP_1se_Bysize_lst))] boxplot(p_singP_1se_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), #ylim=c(0, 300), xaxt=&#39;n&#39; ) for(JJ in 1:length(p_singP_1se_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_1se_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_1se_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_1se_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_1se_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_1se_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;1se errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;one se lamdba - selected gene counts&#39;)) # min ######################################### # selected features p_min_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_p_min_lst &lt;- with(sizeVal_results_frm, split(p_min, SimNo)) sapply(sizeVal_p_min_lst, median) }) names(p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_p&#39;) # selected signatue features sign_p_min_Bysize_lst &lt;- lapply(unique(enet_sim_results_frm$Size), function(SizeVal) { sizeVal_results_frm &lt;- enet_sim_results_frm %&gt;% dplyr::filter(Size==SizeVal) sizeVal_sign_genes_min_lst &lt;- lapply(1:nrow(sizeVal_results_frm), function(RR) intersect(unlist(sizeVal_results_frm[RR, &#39;genes_min&#39;]), enet_gene_sign_min_vec)) sizeVal_sign_p_min_lst &lt;- split(sapply(sizeVal_sign_genes_min_lst, length), sizeVal_results_frm$SimNo) sapply(sizeVal_sign_p_min_lst, median) }) names(sign_p_min_Bysize_lst) &lt;- paste0( stringr::str_pad(unique(enet_sim_results_frm$Size), width=3, pad=&#39;0&#39;), &#39;_signP&#39;) p_singP_min_Bysize_lst &lt;- c(p_min_Bysize_lst, sign_p_min_Bysize_lst) p_singP_min_Bysize_lst &lt;- p_singP_min_Bysize_lst[order(names(p_singP_min_Bysize_lst))] boxplot(p_singP_min_Bysize_lst, col=0, border=c(&#39;blue&#39;,&#39;green&#39;), #ylim=c(0, 300), xaxt=&#39;n&#39; ) for(JJ in 1:length(p_singP_min_Bysize_lst)) points( x=jitter(rep(JJ, length(p_singP_min_Bysize_lst[[JJ]])), amount=0.25), y=p_singP_min_Bysize_lst[[JJ]], cex=0.5, col=ifelse(grepl(&#39;_p&#39;, names(p_singP_min_Bysize_lst)[JJ]),&#39;blue&#39;, &#39;green&#39;) ) LL &lt;- -1 axis(side=1, tick=F, line = LL, at = match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst)), SIZE0 ) abline(v= match(paste0(SIZE0,&#39;_p&#39;),names(p_singP_min_Bysize_lst))[-1] - 0.5, col=&#39;grey&#39;) #abline(h= nzero_min_q2, col = &#39;red&#39;) legend(&#39;topleft&#39;, #title=&#39;min errors&#39;, title.col = &#39;black&#39;, text.col = c(&#39;blue&#39;, &#39;green&#39;), legend= c(&#39;selected genes&#39;,&#39;signature genes&#39;), bty=&#39;n&#39; ) title(paste(&#39;min lambda - selected gene counts&#39;)) mtext(side=3, outer=T, cex=1.25, paste(&#39;enet fit feature selection summarized across simulations&#39;)) Figure 9.9: enet Models Selected Features by Sample Size ### CLEAR CACHE p_sing_1se_Bysize_sum_frm &lt;- t(sapply(p_singP_1se_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4))) colnames(p_sing_1se_Bysize_sum_frm) &lt;- paste0(&#39;1se_&#39;, colnames(p_sing_1se_Bysize_sum_frm)) p_sing_min_Bysize_sum_frm &lt;- t(sapply(p_singP_min_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4))) colnames(p_sing_min_Bysize_sum_frm) &lt;- paste0(&#39;min_&#39;, colnames(p_sing_min_Bysize_sum_frm)) knitr::kable(cbind(p_sing_1se_Bysize_sum_frm, p_sing_min_Bysize_sum_frm), caption = paste(&quot;elastic net feature selection by sample size across simulations&quot;), digits=2) %&gt;% kableExtra::kable_styling(full_width = F) Table 9.4: elastic net feature selection by sample size across simulations 1se_25% 1se_50% 1se_75% min_25% min_50% min_75% 025_p 3.00 8.50 14.75 6.25 15.0 27.75 025_signP 1.00 1.25 2.75 2.00 3.0 3.75 050_p 5.00 8.50 16.00 18.25 31.0 44.12 050_signP 3.00 4.00 5.75 5.00 7.5 9.38 100_p 17.25 22.50 36.00 42.25 55.0 65.88 100_signP 8.12 10.50 14.00 15.25 16.0 19.00 200_p 38.00 54.50 63.25 95.00 105.0 128.75 200_signP 20.00 22.50 24.88 34.00 37.0 41.38 300_p 50.00 67.75 71.88 95.88 111.5 152.75 300_signP 30.00 32.50 36.75 48.12 55.0 61.50 "]
]
