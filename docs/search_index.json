[
["index.html", "DNA Hydroxymethylation in Hepatocellular Carcinoma Preamble License", " DNA Hydroxymethylation in Hepatocellular Carcinoma Francois Collin 2020-09-08 Preamble This vignette offers some exploratory data analyses of DNA Hydroxymethylation data available from NCBI GEO Series GSE112679. These data can be conveniently accessed through an R data package. See GSE112679 R Data Package page. License This work by Francois Collin is licensed under a Creative Commons Attribution 4.0 International License "],
["intro.html", "Section 1 Introduction", " Section 1 Introduction The goal of detecting cancer at the earliest stage of development with a non-invasive procedure has busied many groups with the task of perfecting techniques to support what has become commonly known as a liquid biopsy - the analysis of biomarkers circulating in fluids such as blood, saliva or urine. Epigenetic biomarkers present themselves as good candidates for this application (Gai and Sun (2019) [1]). In particular, given their prevalence in the human genome, close correlation with gene expression and high chemical stability, DNA modifications such as 5-methylcytosine (5mC) and 5-hydroxymethylcytosine (5hmC) are DNA epigenetic marks that provide much promise as cancer diagnosis biomarkers that could be profitably analyzed in liquid biopsies [2–5]. Li et al. (2017) [3] used a sensitive and selective chemical labeling technology to extract genome-wide 5hmC profiles from circulating cell-free DNA (cfDNA) as well as from genomic DNA (gDNA) collected from a cohort of 260 patients recently diagnosed with colorectal, gastric, pancreatic, liver or thyroid cancer and normal tissues from 90 healthy individuals They found 5hmC-based biomarkers of circulating cfDNA to be highly predictive of some cancer types. Similar small sample size findings were reported in Song et al. (2017) [4]. Focusing on hepatocellular carcinoma, Cai et al. (2019) [2] assembled a sizable dataset to demonstrate the feasibility of using features derived from 5-hydroxymethylcytosines marks in circulating cell-free DNA as a non-invasive approach for the early detection of hepatocellular carcinoma. The data that are the basis of that report are available on the NCBI GEO web site (Series GSE112679). The data have also been bundled in a R data package which can be installed from github: if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;12379Monty/GSE112679&quot;) An important question in the early development of classifiers of the sorts that are the basis of any liquid biopsy diagnostic tool is how many samples should be collected to make properly informed decisions. In this report we will explore the GSE112679 data to shed some light on the relationship between sample size and model performance in the context classifying samples based on 5hmC data. The layout of the paper is the following: In Section 2 we provide some background on modeling genomic data, ie. \\(n &lt;&lt; p\\) data. In Section 3 we preprocess the 5hmC data that we will use for the classification analysis and perform some light QC analyses. In Section 4 we explore some glmnet fits that discriminate between early stage HCC and control samples. In Section 5 we examine the results of fitting a suite of models to investigate the effect of sample size on model performance. In Section 6 look at the question of assessing variable importance. Concluding remarks are in Section 7. "],
["modeling-background.html", "Section 2 Modeling - Background 2.1 Predictive modeling for genomic data 2.2 glmnet", " Section 2 Modeling - Background Refer to first pass study for relevant exploratory data analyis results. 2.1 Predictive modeling for genomic data The main challenge in calibrating predictive models to genomic data is that there are many more features than there are example cases to fit to; the now classic \\(n &lt;&lt; p\\) problem. In this scenario, fitting methods tend to over fit. The problem can be addressed by selecting variables, regularizing the fit or both. 2.1.1 caret for model evaluation The caret Package provide a set of functions that streamline the process for fitting and evaluating a large number of predictive models in parallel. The package contains tools for: data splitting pre-processing feature selection model tuning using re-sampling variable importance estimation The tools facilitate the process of automating randomly splitting data sets into training, testing and evaluating so that predictive models can be evaluated on a comparable and exhaustive basis. Especially useful is the functionality that is provided to repeatedly randomly stratify samples into train and test set so that any sample selection bias is removed. What makes the caret package extremely useful is that it provides a common interface to an exhaustive collection of fitting procedures. Without this common interface one has to learn the specific syntax that used in each fitting procedure to be included in a comparative analysis, which can be quite burdensome. Some of the models which can be evaluated with caret include: (only some of these can be used with multinomial responses) FDA - Flexible Discriminant Analysis stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection knn - k nearest neighbors pam - Nearest shrunken centroids rf - Random forests svmRadial - Support vector machines (RBF kernel) gbm - Boosted trees xgbLinear - eXtreme Gradient Boosting xgbTree - eXtreme Gradient Boosting neuralnet - neural network Many more models can be implemented and evaluated with caret, including some deep learning methods, Simulated Annealing Feature Selection and Genetic Algorithms. Many other methods found here are also worth investigating. We only mention caret here because it is an extremely useful tool for anyone interested in comparing many predictive models. We have done that in the past and have found that regularized regression models perform as well as any in the context of classification based on genomic scale data and will focus on the particular set of tools for fitting and analyzing regularized regression models provided by the glmnet R package. 2.2 glmnet In this investigation we will focus on models that can be analyzed with the the glmnet R package [6]. Several factors favor this choice: the glmnet package is a well supported package providing extensive functionality for regularized regression and classification models. the hyper-parameters of the elastic net enable us to explore the relationship between model size, or sparsity, and predictive accuracy. ie. we can investigate the “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. in our experience building classifiers from genomic scale data, regularized classification models using the elastic net penalty do as well as any other, and are more economical in terms of computing time, especially in comparison to the more exotic boosting algorithms. the lasso has been shown to be near optimal for the \\(n&lt;&lt;p\\) problem over a wide range of signal-to-noise regiments (Hastie et al. (2017) [7]). Much of the following comes from the Glmnet Vignette. Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elastic net penalty at a grid of values for the regularization parameter lambda ([6,8–10]). glmnet solves the following problem: \\[\\min_{\\beta_0,\\beta} \\frac{1}{N} \\sum_{i=1}^{N} w_i l(y_i,\\beta_0+\\beta^T x_i) + \\lambda\\left[(1-\\alpha)||\\beta||_2^2/2 + \\alpha ||\\beta||_1\\right],\\] over a grid of values of \\(\\lambda\\). Here \\(l(y,\\eta)\\) is the negative log-likelihood contribution for observation i; e.g. for the Gaussian case it is \\(\\frac{1}{2}(y-\\eta)^2\\). alpha hyper-parameter The elastic-net penalty is controlled by \\(\\alpha\\), and bridges the gap between lasso (\\(\\alpha\\)=1, the default) and ridge (\\(\\alpha\\)=0). The tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an \\(\\alpha\\)=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of \\(\\alpha\\) is for numerical stability; for example, the elastic net with \\(\\alpha = 1 - \\epsilon\\) for some small \\(\\epsilon\\)&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations. Signal-to-noise Ratio A key characteristic of classification problems is the prevailing signal-to-noise ratio (SNR) of the problem at hand. To define SNR, let \\((x_0, y_0) \\in \\mathbb{R}^p \\times \\mathbb{R}\\) be a pair of predcitor and response variables and define \\(f(x_0) = \\mathbb{E}(y_0|x_0)\\) and \\(\\epsilon = y_0 - f(x_0)\\) so that \\[y_0 = f(x_0) + \\epsilon_0.\\] The signal-to-noise ratio in this model is defined as \\[SNR=\\frac{var(f(x_0))}{var(\\epsilon_0)}.\\] It is useful to relate the SNR of a model to the proportion of variance explained (PVE). For a given prediction function g — eg. one trained on n samples \\((x_i, y_i) i = 1, \\dots, n\\) that are i.i.d. to \\((x_0, y_0)\\) — its associated proportion of variance explained is defined as \\[PVE(g)=1 - \\frac{\\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}.\\] This is maximized when we take \\(g\\) to be the mean function \\(f\\) itself, in which case \\[PVE(f) = 1 - \\frac{Var(\\epsilon_0)}{Var(y_0)} = \\frac{SNR}{1+SNR}.\\] Or equivalently, \\[SNR = \\frac{PVE}{1-PVE}.\\] Hastie, Tibshirani, and Tibshirani (2017) [7], point out that PVE is typically in the 0.2 range, and much lower in financial data. It is also much lower in 5hmC data, as we will see in the next section. Note that the SNR is a different characterization of noise level than the coefficient of variation: \\[c_v = \\frac{\\sigma}{\\mu}=\\frac{Var(y)}{\\mathbb{E}(y)}\\] Note that for small SNR, SNR \\(\\approx\\) PVE. See Xiang et al. (2020) [11], Lozoya et al. (2018) [12], Simonson et al. (2018) [13] and Rapaport et al. (2013) [14] for SNR in RNA-Seq Lasso vs Best Subset Best subset selection finds the subset of k predictors that produces the best fit in terms of squared error, solving the nonconvex problem: \\[\\begin{equation} \\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_0 \\leq k \\tag{2.1} \\end{equation}\\] The lasso solves a covex relaxation of (2.1) where we replace the \\(l_0\\) norm by the \\(l_1\\) norm, namely \\[\\begin{equation} \\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_1 \\leq t \\tag{2.2} \\end{equation}\\] where \\(||\\beta||_1 = \\sum_{i=1}^{p} |\\beta_i|\\), and \\(t \\geq 0\\) is a tuning parameter. Bertsimas et al. (2016) [15] presented a mixed integer optimization (MIO) formulation for the best subset selection problem. Using these MIO solvers, one can solve problems with p in the hundreds and even thousands. Bertsimas et al. showed evidence that best subset selection generally gives superior prediction accuracy compared to forward stepwise selection and the lasso, over a variety of problem setups. In Hastie et al. (2017) [7], the authors countered by arguing that neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes. Best subset selection and forward stepwise perform quite similarly over a range of SNR contexts, but the relaxed lasso is the overall best option, performing just about as well as the lasso in low SNR scenarios, and as well as best subset selection in high SNR scenarios. Hastie et al. conclude that a blended mix of lasso and relaxed lasso estimator, the shrunken relaxed lasso fit, is able to use its auxiliary shrinkage parameter (\\(\\gamma\\)) to get the “best of both worlds”: it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not. Suppose the glmnet fitted linear predictor at \\(\\lambda\\) is \\(\\hat{\\eta}_\\lambda(x)\\) and the relaxed version is \\(\\tilde{\\eta}_\\lambda(x)\\), then the shrunken relaxed lasso fit is \\[\\begin{equation} \\tilde{\\eta}_{\\lambda,\\gamma}(x)=(1-\\gamma)\\tilde{\\eta}_\\lambda(x) + \\gamma \\hat{\\eta}_\\lambda(x) \\tag{2.3} \\end{equation}\\] \\(\\gamma \\in [0,\\, 1]\\) is an additional tuning parameter which can be selected by cross validation. The de-biasing will potentially improve prediction performance, and cross-validation will typically select a model with a smaller number of variables. This procedure is very competitive with forward-stepwise and best-subset regression, and has a considerable speed advantage when the number of variables is large. This is especially true for best-subset, but even so for forward stepwise. The latter has to plod through the variables one-at-a-time, while glmnet will just plunge in and find a good active set. Further details may be found in Friedman, Hastie, and Tibshirani (2010), Tibshirani et al. (2012), Simon et al. (2011), Simon, Friedman, and Hastie (2013) and Hastie, Tibshirani, and Tibshirani (2017) ([6–10]). "],
["preproc.html", "Section 3 Preprocessing 3.1 Load the data 3.2 Differential representation analysis 3.3 Signal-to-noise ratio regime", " Section 3 Preprocessing 3.1 Load the data The data that are available from NCBI GEO Series GSE112679 can be conveniently accessed through an R data package. Attaching the GSE112679 package makes the count data tables available as well as a gene annotation table and a sample description table. See GSE112679 R Data Package page. In the Cai et al. [2] paper, samples were separated into Train and Val-1 subsets for model fitting and analysis. Val-2 was used as an external validation set. if (!(&quot;GSE112679&quot; %in% rownames(installed.packages()))) { if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;12379Monty/GSE112679&quot;) } library(GSE112679) sampDesc$DxStage &lt;- with(sampDesc, ifelse(outcome==&#39;HCC&#39;, paste0(outcome,&#39;:&#39;,stage), outcome)) with( sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot;), knitr::kable(table(DxStage, trainValGroup, exclude = NULL), caption=&quot;GSE112679 Samples by Dx Group and Subset&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) Table 3.1: GSE112679 Samples by Dx Group and Subset Train Val-1 Val-2 Benign 253 132 3 CHB 190 96 0 Cirrhosis 73 33 0 HCC:Early 335 220 24 HCC:Late 0 442 13 HCC:NA 0 147 23 Healthy 269 124 177 For this analysis, we will consider early stage cancer samples and healthy or benign samples from the Train or Val-1 subsets. The appropriate outcome variable will be renamed or aliased group # Use suffix &#39;A&#39; for Analysis samples sampDescA &lt;- sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot; &amp; (trainValGroup %in% c(&quot;Train&quot;, &quot;Val-1&quot;)) &amp; ((outcome2 == &quot;BenignHealthy&quot;) | (outcome2 == &quot;HCC&quot; &amp; stage == &quot;Early&quot;))) %&gt;% dplyr::rename(group = outcome2) %&gt;% dplyr::arrange(group, sampID) # Recode group sampDescA$group &lt;- with( sampDescA, ifelse(group == &quot;BenignHealthy&quot;, &quot;Control&quot;, group) ) # set groupCol for later groupCol &lt;- c(&quot;#F3C300&quot;, &quot;#875692&quot;) names(groupCol) &lt;- unique(sampDescA$group) with(sampDescA, knitr::kable(table(group, exclude = NULL), caption=&quot;Samples used in this analysis&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) Table 3.2: Samples used in this analysis group Freq Control 778 HCC 555 The features are counts of reads captured by chemical labeling, and indicate the level of 5-hydroxymethylcytosines within each gene body. Cai et al. (2019), Li et al. (2017) and Song et al. (2017) [2–4] all analyze 5hmC gene body counts using standard RNA-Seq methodologies, and we will do the same here. Note that before conducting any substantive analyses, the data would normally be very carefully examined for any sign of quality variation between groups of samples. This analysis would integrate sample meta data - where and when were the blood samples collected - as well as library preparation and sequencing metrics in order to detect any sign of processing artifacts that may be present in the dataset. This is particularly important when dealing with blood samples as variable DNA quality degradation is a well known challenge that is encountered when dealing with such samples [16]. Although blood specimen handling protocols can be put in place to minimize quality variation [17], variability can never be completely eradicated, especially in the context of blood samples collected by different groups, working in different environments. The problem of variable DNA quality becomes paricularly pernicuous when it is compounded with a confounding factor that sneaks in when the control sample collection events are separated in time and space from the cancer sample collection events; an all too common occurence. As proper data QC requires an intimate familiarity with the details of data collection and processing, such a task cannot be untertaken here. We will simply run a minimal set of QC sanity checks to make sure that there are no apparent systematic effects in the data. featureCountsA &lt;- cbind( Train_featureCount, Val1_featureCount, Val2_featureCount )[, rownames(sampDescA)] We first look at coverage - make sure there isn’t too much disparity of coverage across samples. To detect shared variability, samples can be annotated and ordered according to sample features that may be linked to sample batch processing. Here we the samples have been ordered by group and sample id (an alias of geoAcc). par(mar = c(1, 3, 2, 1)) boxplot(log2(featureCountsA + 1), ylim = c(3, 11), ylab=&#39;log2 Count&#39;, staplewex = 0, # remove horizontal whisker lines staplecol = &quot;white&quot;, # just to be totally sure :) outline = F, # remove outlying points whisklty = 0, # remove vertical whisker lines las = 2, horizontal = F, xaxt = &quot;n&quot;, border = groupCol[sampDescA$group] ) legend(&quot;top&quot;, legend = names(groupCol), text.col = groupCol, ncol = 2, bty = &quot;n&quot;) # Add reference lines SampleMedian &lt;- apply(log2(featureCountsA + 1), 2, median) abline(h = median(SampleMedian), col = &quot;grey&quot;) axis(side = 4, at = round(median(SampleMedian), 1), las = 2, col = &quot;grey&quot;, line = -1, tick = F) Figure 3.1: Sample log2 count boxplots featureCountsA_quant &lt;- apply(featureCountsA, 2, function(CC) { c(quantile(CC, prob = c(.15, (1:3) / 4)), totCovM = sum(CC) / 1e6) }) featureCountsA_quant2 &lt;- apply(featureCountsA_quant, 1, function(RR) { quantile(RR, prob = (1:3) / 4) }) knitr::kable(featureCountsA_quant2, digits = 1, caption = paste( &quot;Coverage Summary - Columns are sample coverage quantiles and total coverage&quot;, &quot;\\nRows are quartiles across samples&quot; ) ) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.3: Coverage Summary - Columns are sample coverage quantiles and total coverage Rows are quartiles across samples 15% 25% 50% 75% totCovM 25% 4 24 111 321 5.5 50% 5 30 135 391 6.7 75% 6 35 162 468 8.0 From this table, we see that 25% of the samples have total coverage exceeding 8M reads, 25% of samples have a 15 percentile of coverage lower than 4, etc. 3.2 Differential representation analysis In the remainder of this section, we will process the data and perform differential expression analysis as outlined in Law et al. (2018) [18]. The main analysis steps are: remove lowly expressed genes normalize gene expression distributions remove heteroscedascity fit linear models and examine DE results It is good practice to perform this differential expression analysis prior to fitting models to get an idea of how difficult it will be to discriminate between samples belonging to the different subgroups. The pipeline outlined in Law et al. (2018) [18] also provides some basic quality assessment opportunities. Remove lowly expressed genes Genes that are not expressed at a biologically meaningful level in any condition should be discarded to reduce the subset of genes to those that are of interest, and to reduce the number of tests carried out downstream when looking at differential expression. Carrying un-informative genes may also be a hindrance to classification and other downtream analyses. To determine a sensible threshold we can begin by examining the shapes of the distributions. par(mar = c(4, 3, 2, 1)) plot(density(lcpm_mtx[, 1]), col = groupCol[sampDescA$group[1]], lwd = 2, ylim = c(0, .25), las = 2, main = &quot;&quot;, xlab = &quot;log2 CPM&quot; ) abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(lcpm_mtx), size = 100)) { den &lt;- density(lcpm_mtx[, JJ]) lines(den$x, den$y, col = groupCol[sampDescA$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(groupCol), text.col = groupCol, bty = &quot;n&quot;) Figure 3.2: Sample \\(log_2\\) CPM densities As is typically the case with RNA-Seq data, we notice many weakly represented genes in this dataset. A cpm value of 1 appears to adequatly separate the expressed from the un-expressed genes, but we will be slightly more strict here and require a CPM threshold of 3 . Using a nominal CPM value of 3, genes are deeemed to be represented if their expression is above this threshold, and not represented otherwise. For this analysis we will require that genes be represented in at least 25 samples across the entire dataset to be retained for downstream analysis. Here, a CPM value of 3 means that a gene is represented if it has at least 9 reads in the sample with the lowest sequencing depth (library size 2.9 million). Note that the thresholds used here are arbitrary as there are no hard and fast rules to set these by. The voom-plot, which is part of analyses done to remove heteroscedasticity, can be examined to verify that the filtering performed is adequate. Remove weakly represented genes and replot densities. Removing 17.5% of genes… par(mar = c(4, 3, 2, 1)) plot(density(lcpm_mtx[, 1]), col = groupCol[sampDescA$group[1]], lwd = 2, ylim = c(0, .25), las = 2, main = &quot;&quot;, xlab = &quot;log2 CPM&quot; ) #abline(v = 0, col = 3) # After verifying no outliers, can plot a random subset for (JJ in sample(2:ncol(lcpm_mtx), size = 100)) { den &lt;- density(lcpm_mtx[, JJ]) lines(den$x, den$y, col = groupCol[sampDescA$group[JJ]], lwd = 2) } # for(JJ legend(&quot;topright&quot;, legend = names(groupCol), text.col = groupCol, bty = &quot;n&quot;) Figure 3.3: Sample \\(log_2\\) CPM densities after removing weak genes As another sanity check, we will look at a multidimensional scaling plot of distances between gene expression profiles. We use plotMDS in limma package [19]), which plots samples on a two-dimensional scatterplot so that distances on the plot approximate the typical log2 fold changes between the samples. Before producing the MDS plot we will normalize the distributions. We will store the data into s DGEList object as this is convenient when running many of the analyses implemented in the edgeR and limma packages. Call the set ‘AF’, for set ‘A’, ‘Filtered’. AF_dgel &lt;- edgeR::DGEList( counts = featureCountsAF, genes = genes_annotAF, samples = sampDescA, group = sampDescA$group ) AF_dgel &lt;- edgeR::calcNormFactors(AF_dgel) AF_lcmp_mtx &lt;- edgeR::cpm(AF_dgel, log = T) # Save AF_dgel to facilitate restarting # remove from final version save(list = &quot;AF_dgel&quot;, file = &quot;RData/AF_dgel&quot;) Verify that the counts are properly normalized. par(mar = c(1, 3, 2, 1)) boxplot(AF_lcmp_mtx, ylim = c(1, 8), ylab=&#39;Normalized Log CPM&#39;, staplewex = 0, # remove horizontal whisker lines staplecol = &quot;white&quot;, # just to be totally sure :) outline = F, # remove outlying points whisklty = 0, # remove vertical whisker lines las = 2, horizontal = F, xaxt = &quot;n&quot;, border = groupCol[sampDescA$group] ) legend(&quot;top&quot;, legend = names(groupCol), text.col = groupCol, ncol = 2, bty = &quot;n&quot;) # Add reference lines SampleMedian &lt;- apply(AF_lcmp_mtx, 2, median) abline(h = median(SampleMedian), col = &quot;grey&quot;) axis(side = 4, at = round(median(SampleMedian), 1), las = 2, col = &quot;grey&quot;, line = -1, tick = F) Figure 3.4: Sample log2 count boxplots Proceed with MDS plots. par(mfcol = c(1, 2), mar = c(4, 4, 2, 1), xpd = NA, oma = c(0, 0, 2, 0)) # wo loss of generality, sample 500 samples # simply a matter of convenience to save time # remove from final version set.seed(1) samp_ndx &lt;- sample(1:ncol(AF_lcmp_mtx), size = 500) MDS.out &lt;- limma::plotMDS(AF_lcmp_mtx[, samp_ndx], col = groupCol[sampDescA$group[samp_ndx]], pch = 1 ) legend(&quot;topleft&quot;, legend = names(groupCol), text.col = groupCol, bty = &quot;n&quot; ) MDS.out &lt;- limma::plotMDS(AF_lcmp_mtx[, samp_ndx], col = groupCol[sampDescA$group[samp_ndx]], pch = 1, dim.plot = 3:4 ) Figure 3.5: MDS plots of log-CPM values The MDS plot, which is analogous to a PCA plot adapted to gene exression data, does not indicate strong clustering of samples. The fanning pattern observed in the first two dimensions indicates that a few samples are drifting way from the core set, but in no particular direction. There is some structure in the 3rd and 4th dimension plot which should be investigated. glMDSPlot from package Glimma provides an interactive MDS plot that can extremely usful for exploration Glimma::glMDSPlot(AF_dgel[, samp_ndx], groups = AF_dgel$samples[ samp_ndx, c(&quot;group&quot;, &quot;trainValGroup&quot;, &quot;sampType&quot;, &quot;tissue&quot;, &quot;title&quot;, &quot;stage&quot;) ], main = paste(&quot;MDS plot: filtered counts&quot;), #### , Excluding outlier samples&quot;), path = &quot;.&quot;, folder = figures_DIR, html = paste0(&quot;GlMDSplot&quot;), launch = F ) Link to glMDSPlot: Here No obvious factor links the samples in the 3 clusters observed on the 4th MDS dimensions. The percent of variance exaplained by this dimension or \\(\\approx\\) 4%. The glMDSPlot indicates further segregation along the 6th dimension. The percent of variance exaplained by this dimension or \\(\\approx\\) 2%. Tracking down this source of variability may be quite challenging, especially without having the complete information about the sample attributes and provenance. Unwanted variability is a well-documented problem in the analysis of RNA-Seq data (see Peixoto et al. (2015) [20]), and many procedures have been proposed to reduce the effect of unwanted variation on RNA-Seq analsys results ([20–22]). There are undoubtedly some similar sources of systematic variation in the 5hmC data, but it is beyond the scope of this work to investigate these in this particular dataset. Given that the clustering of samples occurs in MDS dimensions that explain a small fraction of variability, and that these is no assocation with the factor of interest, HCC vs Control, these sources of variability should not interfere too much with our classification analysis. It would nonetheless be interesting to assess whether downstream results can be improved by removing this variability. Creating a design matrix and contrasts Before proceeding with the statistical modeling used for the differential expression analysis, we need to set up a model design matrix. Design_mtx &lt;- model.matrix( ~ -1 + group, data=AF_dgel$samples) colnames(Design_mtx) &lt;- sub(&#39;group&#39;, &#39;&#39;, colnames(Design_mtx)) cat(&quot;colSums(Design_mtx):\\n&quot;) ## colSums(Design_mtx): colSums(Design_mtx) ## Control HCC ## 778 555 Contrasts_mtx &lt;- limma::makeContrasts( HCCvsControl = HCC - Control, levels=colnames(Design_mtx)) cat(&quot;Contrasts:\\n&quot;) ## Contrasts: Contrasts_mtx ## Contrasts ## Levels HCCvsControl ## Control -1 ## HCC 1 Removing heteroscedasticity from the count data As for RNA-Seq data, for 5hmC count data the variance is not independent of the mean. In limma, the R package we are using for our analyses, linear modeling is carried out on the log-CPM values which are assumed to be normally distributed and the mean-variance relationship is accommodated using precision weights calculated by the voom function. We apply this transformation next. par(mfrow=c(1,1)) filteredCountsAF_voom &lt;- limma::voom(AF_dgel, Design_mtx, plot=T) Figure 3.6: Removing heteroscedascity Note that the voom-plot provides a visual check on the level of filtering performed upstream. If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be observed at the low end of the expression scale due to very small counts. Fit linear models and examine the results Having properly filtered and normalized the data, the linear models can be fitted to each gene and the results examined to assess differential expression between the two groups of interest, in our case HCC vs Control. Table 3.4 displays the counts of genes in each DE category: filteredCountsAF_voom_fit &lt;- limma::lmFit(filteredCountsAF_voom, Design_mtx) colnames(filteredCountsAF_voom_fit$coefficients) &lt;- sub(&quot;\\\\(Intercept\\\\)&quot;, &quot;Intercept&quot;, colnames(filteredCountsAF_voom_fit$coefficients) ) filteredCountsAF_voom_fit &lt;- limma::contrasts.fit( filteredCountsAF_voom_fit, contrasts=Contrasts_mtx) filteredCountsAF_voom_efit &lt;- limma::eBayes(filteredCountsAF_voom_fit) filteredCountsAF_voom_efit_dt &lt;- limma::decideTests(filteredCountsAF_voom_efit,adjust.method = &quot;BH&quot;, p.value = 0.05) knitr::kable(t(summary(filteredCountsAF_voom_efit_dt)), caption=&quot;DE Results at FDR = 0.05&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.4: DE Results at FDR = 0.05 Down NotSig Up HCCvsControl 5214 5280 5258 Graphical representations of DE results: MD Plots To summarise results for all genes visually, mean-difference plots (aka MA plot), which display log-FCs from the linear model fit against the average log-CPM values can be generated using the plotMD function, with the differentially expressed genes highlighted. We may also be interested in whether certain gene features are related to gene identification. Gene GC content, for example, might be of interest. par(mfrow=c(1,3), mar=c(4.5,4.5,2,1),oma=c(1,1,2,0)) # log-fold-change vs ave-expr limma::plotMD(filteredCountsAF_voom_efit, ylim = c(-0.4, 0.4), column=&#39;HCCvsControl&#39;, status=filteredCountsAF_voom_efit_dt[,&#39;HCCvsControl&#39;], hl.pch = 16, hl.col = c(&quot;lightblue&quot;, &quot;pink&quot;), hl.cex = .5, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F, cex.lab=1.5 ) abline(h = 0, col = &quot;black&quot;) rug(quantile(filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) # log-fold-change vs identification boxplot(split( filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], filteredCountsAF_voom_efit_dt[,&#39;HCCvsControl&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;log-fold-change&#39;, ylim=c(-.4, .4), cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) # gc vs identification genes_ndx &lt;- match(rownames(filteredCountsAF_voom_efit), genes_annotAF$Symbol) if(sum(is.na(genes_ndx))) stop(&quot;filteredCountsAF_voom_efit/genes_annotAF: genes mismatch&quot;) GC_vec &lt;- with(genes_annotAF[genes_ndx,],(G+C)/(A+C+G+T)) boxplot(split( GC_vec, filteredCountsAF_voom_efit_dt[,&#39;HCCvsControl&#39;]), outline=F, border=c(&quot;pink&quot;, &quot;grey&quot;, &quot;lightblue&quot;), xaxt=&#39;n&#39;, ylab=&#39;gene-gc&#39;, cex.lab=1.5 ) axis(side=1, at=1:3, c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;), cex.axis=1.5) Figure 3.7: HCC vs Control - Genes Identified at FDR = 0,05 #mtext(side=3, outer=T, cex=1.25, &quot;Genes identified at adjusted p-value=0.05&quot;) featureCountsAF_logFC_sum &lt;- sapply( split( filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], filteredCountsAF_voom_efit_dt[,&#39;HCCvsControl&#39;]), quantile, prob = (1:3) / 4) colnames(featureCountsAF_logFC_sum) &lt;- as.character(factor( colnames(featureCountsAF_logFC_sum), levels=c(&quot;-1&quot;, &quot;0&quot;, &quot;1&quot;), labels=c(&#39;down&#39;, &#39;notDE&#39;, &#39;up&#39;) )) knitr::kable(featureCountsAF_logFC_sum, digits = 2, caption = &quot;log FC quartiles by gene identification&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.5: log FC quartiles by gene identification down notDE up 25% -0.07 -0.01 0.04 50% -0.05 0.00 0.06 75% -0.03 0.01 0.09 While many genes are identified, the effect sizes are quite small, which results in a low signal-to-noise ratio context. See Section 3.3 below. The log-fold-change distribution for up-represented genes is long-tailed, with many high log fold-change values. By contrast, log-fold-change distribution for down-represented genes closer to symmetric and has few genes with low log fold-change values. We will see how this affects the results of identifying genes with an effect size requirement. The GC content of down regulated genes tends to be slightly lower than the rest of the genes. A statistical test would find that the difference between the mean of the down regulated gene population is singificantly different than the mean of the other gene population even though the difference is quite small (-0.028). These asymmetries are minor, but it would still be good to establish that they relfect biology rather than processing artifacts. DE genes at 10% fold change For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The treat method (McCarthy and Smyth 2009 [23]) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement. The number of differentially expressed genes are greatly reduced if we impose a minimal fold-change requirement of 10%. filteredCountsAF_voom_tfit &lt;- limma::treat(filteredCountsAF_voom_fit, lfc=log2(1.10)) filteredCountsAF_voom_tfit_dt &lt;- limma::decideTests(filteredCountsAF_voom_tfit) cat(&quot;10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05:\\n&quot;) ## 10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05: summary(filteredCountsAF_voom_tfit_dt) ## HCCvsControl ## Down 3 ## NotSig 15550 ## Up 199 # log-fold-change vs ave-expr limma::plotMD(filteredCountsAF_voom_efit, ylim = c(-0.5, 0.5), column=&#39;HCCvsControl&#39;, status=filteredCountsAF_voom_tfit_dt[,&#39;HCCvsControl&#39;], hl.pch = 16, hl.col = c(&quot;blue&quot;, &quot;red&quot;), hl.cex = .7, bg.pch = 16, bg.col = &quot;grey&quot;, bg.cex = 0.5, main = &#39;&#39;, xlab = paste0( &quot;Average log-expression: IQR=&quot;, paste(round(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), ylab = paste0( &quot;log-fold-change: IQR=&quot;, paste(round(quantile(filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 3) / 4), 2), collapse = &quot;, &quot; ) ), legend = F ) abline(h = 0, col = &quot;black&quot;) rug(quantile(filteredCountsAF_voom_efit$coefficients[, &#39;HCCvsControl&#39;], prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 2, lwd = 2 ) rug(quantile(filteredCountsAF_voom_efit$Amean, prob = c(1, 2, 3) / 4), col = &quot;purple&quot;, ticksize = .03, side = 1, lwd = 2 ) Figure 3.8: HCC vs Control - Identified Genes at FDR = 0,05 and logFC &gt; 10% As noted above, the log-fold-change distribution for the up-represented genes is long-tailes in comparison to log-fold-change distribution for the down-represented genes. As a result fewer down-represented than up-regulated genes are identified when a minimum log-FC requirement is imposed. 3.3 Signal-to-noise ratio regime In Hastie et al. (2017) [7]) results from lasso fits are compared with best subset and forward selection fits and it is argued that while best subset is optimal for high signal-to-noise regimes, the lasso gains some competitive advantage when the prevailing signal-to-noise ratio of the dataset is lowered. We can extract sigma and signal from the fit objects to get SNR values for each gene to see in what SNR regime the 5hmC gene body data are. lib.size &lt;- colSums(AF_dgel$counts) fit &lt;- filteredCountsAF_voom_efit sx &lt;- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06) sy &lt;- sqrt(fit$sigma) CV &lt;- sy/sx Effect &lt;- abs(filteredCountsAF_voom_efit$coefficients[,&#39;HCCvsControl&#39;]) Noise &lt;- filteredCountsAF_voom_efit$sigma SNR &lt;- Effect/Noise plot(spatstat::CDF(density(SNR)), col = 1, lwd = 2, ylab = &quot;Prob(SNR&lt;x)&quot;, xlim = c(0, 0.2) ) SNR_quant &lt;- quantile(SNR, prob=c((1:3)/4,.9)) rug(SNR_quant, lwd = 2, ticksize = 0.05, col = 1 ) Figure 3.9: Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile knitr::kable(t(SNR_quant), digits = 3, caption = paste( &quot;SNR Quantiles&quot;) ) %&gt;% kableExtra::kable_styling(full_width = F) Table 3.6: SNR Quantiles 25% 50% 75% 90% 0.018 0.036 0.06 0.082 These SNR values are in the range where the lasso and relaxed lasso gain some advantage over best subset and forward selection fits (see Hastie et al. (2017) [7]). "],
["explore-sparsity.html", "Section 4 The bet on sparsity 4.1 CV analysis setup 4.2 Fit and compare models 4.3 Relaxed lasso and blended mix 4.4 Examination of sensitivity vs specificity 4.5 Compare predictions at misclassified samples 4.6 Compare coefficient profiles 4.7 Examine feature selection", " Section 4 The bet on sparsity In this section we explore various fits that can be computed and analyzed with tools provided in the glmnet package. Refer to the Glmnet Vignette for a quick reference guide. 4.1 CV analysis setup K_FOLD &lt;- 10 trainP &lt;- 0.8 EPS &lt;- 0.05 # Have no idea what &quot;small&quot; epsilon means First we divide the analysis dataset into train and test in a 4:1 ratio. set.seed(1) train_sampID_vec &lt;- with(AF_dgel$samples, AF_dgel$samples$sampID[caret::createDataPartition(y=group, p=trainP, list=F)] ) test_sampID_vec &lt;- with(AF_dgel$samples, setdiff(sampID, train_sampID_vec) ) train_group_vec &lt;- AF_dgel$samples[train_sampID_vec, &#39;group&#39;] names(train_group_vec) &lt;- AF_dgel$samples[train_sampID_vec, &#39;sampID&#39;] test_group_vec &lt;- AF_dgel$samples[test_sampID_vec, &#39;group&#39;] names(test_group_vec) &lt;- AF_dgel$samples[test_sampID_vec, &#39;sampID&#39;] knitr::kable(table(train_group_vec), caption=&quot;Train set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Train set train_group_vec Freq Control 623 HCC 444 knitr::kable(table(test_group_vec), caption=&quot;Test set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Test set test_group_vec Freq Control 155 HCC 111 train_lcpm_mtx &lt;- t(lcpm_mtx[,train_sampID_vec]) test_lcpm_mtx &lt;- t(lcpm_mtx[,test_sampID_vec]) We explore some glmnet fits and the “bet on sparsity” We consider three models, specified by the value of the alpha parameter in the elastic net parametrization: - lasso: \\(\\alpha = 1.0\\) - sparse models - ridge \\(\\alpha = 0\\) - shrunken coefficients models - elastic net: \\(\\alpha = 0.5\\) - semi sparse model Some questions of interest include: * How sparse is the model undelying best 5hmC classifier for Early HCC vs Control? Does the relaxed lasso improve performance in this case? Does the shrunken relaxed lasso (aka the blended mix) improve performance Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set? In this analysis, we will only evaluate models in terms of model size, stability and performance. We leave the question of significance testing of hypotheses about model parameters completely out. See Lockhart et al. (2014) [24] and Wassermam (2014) [25] for a discussion of this topic. In this section we look at the relative performance and size of the models considered. The effect of the size of the sample set on the level and stability of performance will be investigated in the next section. First we create folds for 10-fold cross-validation of models fitted to training data. We’ll use caret::createFolds to assign samples to folds while keeping the outcome ratios constant across folds. # This is too variable, both in terms of fold size And composition #foldid_vec &lt;- sample(1:10, size=length(train_group_vec), replace=T) set.seed(1) train_foldid_vec &lt;- caret::createFolds( factor(train_group_vec), k=K_FOLD, list=F) knitr::kable(sapply(split(train_group_vec, train_foldid_vec), table), caption=&quot;training samples fold composition&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.2: training samples fold composition 1 2 3 4 5 6 7 8 9 10 Control 62 62 62 63 62 62 63 62 63 62 HCC 45 44 44 45 44 45 44 45 44 44 Note that the folds identify samples that are left-out of the training data for each fold fit. 4.2 Fit and compare models glmnet provides cross-validation methods to pick the parameter lambda which controls to size of the penalty function. The “one standard error rule” produces a model with fewer predictors then the minimum cv error model. On the training data, this usually results in increased MSE and more biased parameter estimates (see Engebretsen et al. (2019) [26] for example). The question of iterest though is the performance on unseen data; not on the training data. In the analysis below, we compare the cv error rates with out-of-fold and test set error rates. The results show that out-of-fold error rates computed from the training data are good indicators of test set error rates, and that the one standard error rule models do as well as the minimim cv error models for the lasso, which has the best overall performance. 4.2.1 Logistic regression in glmnet glmnet provides functionality to extract various predicted of fitted values from calibrated models. Note in passing that some folks make a distinction between fitted or estimated values for sample points in the training data versus predicted values for sample points that are not in the training dataset. glmnet makes no such distinction and the predict function is used to produce both fitted as well as predicted values. When predict is invoked to make predictions for design points that are part of the training dataset, what is returned are fitted values. When predict is invoked to make predictions for design points that are not part of the training dataset, what is returned are predicted values. For logistic regressions, which is the model fitted in a regularized fashion when models are fitted by glmnet with the parameter family='binomial', three fitted or predicted values can be extracted at a given design point. Suppose our response variable Y is either 0 or 1 (Control or HCC in our case). These are specified by the type parameter. type='resp' returns the fitted or predicted probability of \\(Y=1\\). type='class' returns the fitted or predicted class for the design point, which is simply dichotomizing the response: class = 1 if the fitted or predicted probability is greater than 0.5 (check to make sure class is no the Bayes estimate). type='link' returns the fitted or predicted value of the linear predictor \\(\\beta&#39;x\\). The relationship between the linear predictor and the response can be derided from the logistic regression model: \\[P(Y=1|x,\\beta) = g^{-1}(\\beta&#39;x) = h(\\beta&#39;x) = \\frac{e^{\\beta&#39;x}}{1+e^{\\beta&#39;x}}\\] where \\(g\\) is the link function, \\(g^{-1}\\) the mean function. The link function is given by: \\[g(y) = h^{-1}(y) = ln(\\frac{y}{1-y})\\] This link function is called the logit function, and its inverse the logistic function. logistic_f &lt;- function(x) exp(x)/(1+exp(x)) It is important to note that all predicted values extracted from glmnet fitted models by the predict() extraction method yield fitted values for design points that are part of the training data set. This includes the predicted class for training data which are used to estimate misclassification error rates. As a result, the cv error rates quoted in various glmnet summaries are generally optimistic. glmnet fitting functions have a parameter, keep, which instructs the fitting function to keep the out-of-fold, or prevalidated, predictions as part of the returned object. The out-of-fold predictions are predicted values for the samples in the left-out folds, pooled across all cv folds. For each hyper-parameter specification, we get one full set of out-of-fold predictions for the training set samples. Performance assessments based on these values are usually more generalizable - ie. predictive of performance in unseen data - than assessments based on values produced from the full fit, which by default is what glmnet extraction methods provide. See Höfling and Tibshirani (2008) [27] for a description of the use of pre-validation in model assessment. Because the keep=T option will store predicted values for all models evaluated in the cross-validation process, we will limit the number of models tested by setting nlambda=30 when calling the fitting functions. This has no effect on performance in this data set. start_time &lt;- proc.time() cv_lasso &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=1, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;lasso time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lasso time: 13.22s start_time &lt;- proc.time() cv_ridge &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;ridge time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## ridge time: 103.96s start_time &lt;- proc.time() cv_enet &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0.5, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;enet time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## enet time: 12.18s The ridge regression model takes over 10 times longer to compute. plot_cv_f &lt;- function(cv_fit, Nzero=T, ...) { library(glmnet) # No nonger used #lambda.1se_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se] #lambda.min_p &lt;- cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min] # Get oof error ndx_1se &lt;- match(cv_fit$lambda.1se,cv_fit$lambda) train_oofPred_1se_vec &lt;- ifelse( cv_fit$fit.preval[,ndx_1se] &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) train_oofPred_1se_error &lt;- mean(train_oofPred_1se_vec != train_group_vec) ndx_min &lt;- match(cv_fit$lambda.min,cv_fit$lambda) train_oofPred_min_vec &lt;- ifelse( cv_fit$fit.preval[,ndx_min] &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) train_oofPred_min_error &lt;- mean(train_oofPred_min_vec != train_group_vec) # Get test set error test_pred_1se_vec &lt;- predict( cv_fit, newx=test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&quot;class&quot; ) test_pred_1se_error &lt;- mean(test_pred_1se_vec != test_group_vec) test_pred_min_vec &lt;- predict( cv_fit, newx=test_lcpm_mtx, s=&quot;lambda.min&quot;, type=&quot;class&quot; ) test_pred_min_error &lt;- mean(test_pred_min_vec != test_group_vec) plot( log(cv_fit$lambda), cv_fit$cvm, pch=16,col=&quot;red&quot;, xlab=&#39;&#39;,ylab=&#39;&#39;, ... ) abline(v=log(c(cv_fit$lambda.1se, cv_fit$lambda.min))) if(Nzero) axis(side=3, tick=F, at=log(cv_fit$lambda), labels=cv_fit$nzero, line = -1 ) LL &lt;- 2 #mtext(side=1, outer=F, line = LL, &quot;log(Lambda)&quot;) #LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;1se p =&quot;, lambda.1se_p),&#39;&#39;), &quot;1se: cv =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], 1), &quot;oof =&quot;, round(100*train_oofPred_1se_error, 1), &quot;test =&quot;, round(100*test_pred_1se_error, 1) )) LL &lt;- LL+1 mtext(side=1, outer=F, line = LL, paste( #ifelse(Nzero, paste(&quot;min p =&quot;, lambda.min_p),&#39;&#39;), &quot;min: cv =&quot;, round(100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], 1), &quot;oof =&quot;, round(100*train_oofPred_min_error, 1), &quot;test =&quot;, round(100*test_pred_min_error, 1) )) tmp &lt;- cbind( error_1se = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se], train_cv = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se], train_oof = 100*train_oofPred_1se_error, test = 100*test_pred_1se_error), error_min = c( p = cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min], train_cv = 100*cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min], train_oof = 100*train_oofPred_min_error, test = 100*test_pred_min_error) ) # Need to fix names rownames(tmp) &lt;- c(&#39;p&#39;, &#39;train_cv&#39;, &#39;train_oof&#39;, &#39;test&#39;) tmp } Examine model performance. par(mfrow=c(1,3), mar=c(5, 2, 3, 1), oma=c(3,2,0,0)) lasso_errors_mtx &lt;- plot_cv_f(cv_lasso, ylim=c(0,.5)) ## Warning: package &#39;glmnet&#39; was built under R version 4.0.2 ## Loading required package: Matrix ## Loaded glmnet 4.0-2 title(&#39;lasso&#39;) rifge_errors_mtx &lt;- plot_cv_f(cv_ridge, Nzero=F, ylim=c(0,.5)) title(&#39;ridge&#39;) enet_errors_mtx &lt;- plot_cv_f(cv_enet, ylim=c(0,.5)) title(&#39;enet&#39;) mtext(side=1, outer=T, cex=1.25, &#39;log(Lambda)&#39;) mtext(side=2, outer=T, cex=1.25, cv_lasso$name) Figure 4.1: compare fits errors_frm &lt;- data.frame( lasso = lasso_errors_mtx, ridge = rifge_errors_mtx, enet = enet_errors_mtx ) knitr::kable(t(errors_frm), caption = &#39;Misclassifiaction error rates&#39;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.3: Misclassifiaction error rates p train_cv train_oof test lasso.error_1se 99 7.0 9.7 10.2 lasso.error_min 99 7.0 9.7 10.2 ridge.error_1se 15752 13.5 19.0 18.4 ridge.error_min 15752 12.5 16.9 16.2 enet.error_1se 118 7.2 11.1 12.0 enet.error_min 278 6.7 9.2 10.2 We see that the lasso and enet models do better than the ridge model. There is very little difference between the min lambda and the the one standard error (1se) rule lambda models (the two are the same for the lasso in this data set). The 1se lambda lasso fit is only slightly more parsimonious than the 1se elastic net fit, but its test set accuracy is better. The min lambda elastic net fit performs as well as the lasso model, but is much less parsimonious. We also see that the training data out-of-fold estimates of misclassification error rates are much closer to the test set estimates than are the cv estimated rates. This has been our experience with regularized regression models fitted to genomic scale data. It should also be noted that the cv estimates of misclassification rates become more biased as the sample size decreases, as we will show in Section 5. 4.3 Relaxed lasso and blended mix Next we look at the so-called relaxed lasso and the blended mix which is an optimized shrinkage between the relaxed lasso and the regular lasso. See (2.3) in Section 2. library(glmnet) cv_lassoR_sum &lt;- print(cv_lassoR) ## ## Call: glmnet::cv.glmnet(x = train_lcpm_mtx, y = train_group_vec, type.measure = &quot;class&quot;, foldid = train_foldid_vec, keep = T, relax = T, alpha = 1, family = &quot;binomial&quot;, nlambda = 30) ## ## Measure: Misclassification Error ## ## Gamma Lambda Measure SE Nonzero ## min 0.5 0.0379 0.06748 0.005274 35 ## 1se 0.5 0.0379 0.06748 0.005274 35 plot(cv_lassoR) Figure 4.2: lassoR fit # only report 1se ndx_1se &lt;- match(cv_lassoR$lambda.1se, cv_lassoR$lambda) ndx_min &lt;- match(cv_lassoR$lambda.min, cv_lassoR$lambda) # only show 1se anyway # if(ndx_1se != ndx_min) stop(&quot;lambda.1se != lambda.min&quot;) # train oof data # Get relaxed lasso (gamma=0) oof error train_oofPred_relaxed_1se_vec &lt;- ifelse( cv_lassoR$fit.preval[[&quot;g:0&quot;]][, ndx_1se] &gt; 0.5, &quot;HCC&quot;, &quot;Control&quot; ) train_oofPred_relaxed_1se_error &lt;- mean(train_oofPred_relaxed_1se_vec != train_group_vec) # blended mix (gamma=0.5) train_oofPred_blended_1se_vec &lt;- ifelse( cv_lassoR$fit.preval[[&quot;g:0.5&quot;]][, ndx_1se] &gt; 0.5, &quot;HCC&quot;, &quot;Control&quot; ) train_oofPred_blended_1se_error &lt;- mean(train_oofPred_blended_1se_vec != train_group_vec) # Test set error - relaxed test_pred_relaxed_1se_vec &lt;- predict( cv_lassoR, newx = test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0 ) test_pred_relaxed_1se_error &lt;- mean(test_pred_relaxed_1se_vec != test_group_vec) # Test set error - blended test_pred_blended_1se_vec &lt;- predict( cv_lassoR, newx = test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;class&quot;, gamma = 0.5 ) test_pred_blended_1se_error &lt;- mean(test_pred_blended_1se_vec != test_group_vec) cv_lassoR_1se_error &lt;- cv_lassoR_sum[&quot;1se&quot;, &quot;Measure&quot;] cv_lassoR_min_error &lt;- cv_lassoR_sum[&quot;min&quot;, &quot;Measure&quot;] knitr::kable(t(data.frame( train_lassoR_cv = cv_lassoR_1se_error, train_blended_oof = train_oofPred_blended_1se_error, train_relaxed_oof = train_oofPred_relaxed_1se_error, test_blended_oof = test_pred_blended_1se_error, test_relaxed_oof = test_pred_relaxed_1se_error )) * 100, digits = 1, caption = &quot;Relaxed lasso and blended mix error rates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.4: Relaxed lasso and blended mix error rates train_lassoR_cv 6.7 train_blended_oof 10.2 train_relaxed_oof 11.1 test_blended_oof 10.2 test_relaxed_oof 10.9 The relaxed lasso and blended mix error rates are comparable to the regular lasso fit error rate. We see here too that the reported cv error rates are quite optimistic, while out-of-fold error rates continue to be good indicators of unseen data error rates, as captured by the test set. The 1se lambda rule applied to the relaxed lasso fit selected a model with 99 features, while for the blended mix model (See (2.3) in Section 2 the 1se lambda rule selected 35 features (See vertical dotted reference line in Figure 4.2). 4.4 Examination of sensitivity vs specificity In the results above we reported error rates without inspecting the sensitivity versus specificity trade-off. ROC curves can be examined to get a sense of the trade-off. 4.4.1 Training data out-of-fold ROC curves # train # lasso ndx_1se &lt;- match(cv_lasso$lambda.1se,cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(cv_lasso$fit.preval[,ndx_1se]) train_lasso_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_lasso_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # enet ndx_1se &lt;- match(cv_enet$lambda.1se,cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(cv_enet$fit.preval[,ndx_1se]) train_enet_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_enet_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # lasso - relaxed ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_lassoR_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_lassoR_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_lassoR_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # blended mix (gamma=0.5) ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_blended_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_blended_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(train_lasso_roc, col=col_vec[1]) lines(train_enet_roc, col=col_vec[2]) lines(train_lassoR_roc, col=col_vec[3]) lines(train_blended_roc, col=col_vec[4]) legend(&#39;bottomright&#39;, title=&#39;AUC&#39;, legend=c( paste(&#39;lasso =&#39;, round(train_lasso_roc[[&#39;auc&#39;]],3)), paste(&#39;enet =&#39;, round(train_enet_roc[[&#39;auc&#39;]],3)), paste(&#39;lassoR =&#39;, round(train_lassoR_roc[[&#39;auc&#39;]],3)), paste(&#39;blended =&#39;, round(train_blended_roc[[&#39;auc&#39;]],3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 4.3: Train data out-of-sample ROCs Compare thresholds for 90% Specificity: lasso_ndx &lt;- with(as.data.frame(pROC::coords(train_lasso_roc, transpose=F)), min(which(specificity &gt;= 0.9))) enet_ndx &lt;- with(as.data.frame(pROC::coords(train_enet_roc, transpose=F)), min(which(specificity &gt;= 0.9))) lassoR_ndx &lt;- with(as.data.frame(pROC::coords(train_lassoR_roc, transpose=F)), min(which(specificity &gt;= 0.9))) blended_ndx &lt;- with(as.data.frame(pROC::coords(train_blended_roc, transpose=F)), min(which(specificity &gt;= 0.9))) spec90_frm &lt;- data.frame(rbind( lasso=as.data.frame(pROC::coords(train_lasso_roc, transpose=F))[lasso_ndx,], enet=as.data.frame(pROC::coords(train_enet_roc, transpose=F))[enet_ndx,], lassoR=as.data.frame(pROC::coords(train_lassoR_roc, transpose=F))[lassoR_ndx,], blended=as.data.frame(pROC::coords(train_blended_roc, transpose=F))[blended_ndx,] )) knitr::kable(spec90_frm, digits=3, caption=&quot;Specificity = .90 Coordinates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.5: Specificity = .90 Coordinates threshold specificity sensitivity lasso 0.337 0.9 0.932 enet 0.347 0.9 0.937 lassoR 0.003 0.9 0.894 blended 0.031 0.9 0.899 This is strange. par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(train_lasso_oofProb_vec[train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_lasso_oofProb_vec[train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;HCC&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(train_enet_oofProb_vec[train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_enet_oofProb_vec[train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # lassoR plot(density(train_lassoR_oofProb_vec[train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_lassoR_oofProb_vec[train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lassoR&quot;) # blended plot(density(train_blended_oofProb_vec[train_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(train_blended_oofProb_vec[train_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;out-of-fold predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 4.4: Train data out-of-fold predicted probabilities The relaxed lasso fit results in essentially dichotomized predicted probability distribution - predicted probabilities arr very close to 0 or 1. Look at test data ROC curves. # plot all plot(test_lasso_roc, col = col_vec[1]) lines(test_enet_roc, col = col_vec[2]) lines(test_lassoR_roc, col = col_vec[3]) lines(test_blended_roc, col = col_vec[4]) legend(&quot;bottomright&quot;, title = &quot;AUC&quot;, legend = c( paste(&quot;lasso =&quot;, round(test_lasso_roc[[&quot;auc&quot;]], 3)), paste(&quot;enet =&quot;, round(test_enet_roc[[&quot;auc&quot;]], 3)), paste(&quot;lassoR =&quot;, round(test_lassoR_roc[[&quot;auc&quot;]], 3)), paste(&quot;blended =&quot;, round(test_blended_roc[[&quot;auc&quot;]], 3)) ), text.col = col_vec[1:4], bty=&#39;n&#39; ) Figure 4.5: Test data out-of-sample ROCs Look at densities of predicted probabilities. par(mfrow = c(2, 2), mar = c(3, 3, 2, 1), oma = c(2, 2, 2, 2)) # lasso plot(density(test_lasso_predProb_vec[test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_lasso_predProb_vec[test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lasso&quot;) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;HCC&quot;), text.col = c(&quot;green&quot;, &quot;red&quot;)) # enet plot(density(test_enet_predProb_vec[test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_enet_predProb_vec[test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;enet&quot;) # lassoR plot(density(test_lassoR_predProb_vec[test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_lassoR_predProb_vec[test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;lassoR&quot;) #sapply(split(test_lassoR_predProb_vec, test_group_vec), summary) # blended plot(density(test_blended_predProb_vec[test_group_vec == &quot;Control&quot;]), xlim = c(0, 1), main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, col = &quot;green&quot; ) lines(density(test_blended_predProb_vec[test_group_vec == &quot;HCC&quot;]), col = &quot;red&quot; ) title(&quot;blended&quot;) mtext(side = 1, outer = T, &quot;test set predicted probability&quot;, cex = 1.25) mtext(side = 2, outer = T, &quot;density&quot;, cex = 1.25) Figure 4.6: Test data out-of-fold predicted probabilities # Define plotting function bxpPredProb_f &lt;- function(cv_fit, Gamma=NULL) { # Train - preval is out-of-fold linear predictor for training design points onese_ndx &lt;- match(cv_fit$lambda.1se, cv_fit$lambda) if(is.null(Gamma)) train_1se_preval_vec &lt;- cv_fit$fit.preval[, onese_ndx] else train_1se_preval_vec &lt;- cv_fit$fit.preval[[Gamma]][, onese_ndx] train_1se_predProb_vec &lt;- logistic_f(train_1se_preval_vec) # Test test_1se_predProb_vec &lt;- predict( cv_fit, newx = test_lcpm_mtx, s = &quot;lambda.1se&quot;, type = &quot;resp&quot; ) tmp &lt;- c( train = split(train_1se_predProb_vec, train_group_vec), test = split(test_1se_predProb_vec, test_group_vec) ) names(tmp) &lt;- paste0(&quot;\\n&quot;, sub(&quot;\\\\.&quot;, &quot;\\n&quot;, names(tmp))) boxplot(tmp) } par(mfrow = c(2, 2), mar = c(5, 3, 2, 1), oma = c(2, 2, 2, 2)) bxpPredProb_f(cv_lasso) title(&#39;lasso&#39;) bxpPredProb_f(cv_enet) title(&#39;enet&#39;) bxpPredProb_f(cv_lassoR, Gamma=&#39;g:0&#39;) title(&#39;lassoR&#39;) bxpPredProb_f(cv_lassoR, Gamma=&#39;g:0.5&#39;) title(&#39;blended&#39;) Figure 4.7: Predicted Probabilities - Train and Test We have seen above that assessments of model performance based on the out-of-fold predicted values are close to the test set assessments, and that assessments based on prediction extracted from glmnet object are optimistic. Here we look at confusion matrices to see how this affects the classification results. Here we us a threshold of 0.5 to dichotomize the predicted probabilities into a class prediction, as is done in the glmnet predictions. # lasso ########################## # train - cv predicted train_lasso_predClass_vec &lt;- predict( cv_lasso, newx=train_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # train - oof ndx_1se &lt;- match(cv_lasso$lambda.1se,cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(cv_lasso$fit.preval[,ndx_1se]) train_lasso_oofClass_vec &lt;- ifelse( train_lasso_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # test test_lasso_predClass_vec &lt;- predict( cv_lasso, newx=test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # enet ########################## # train - cv predicted train_enet_predClass_vec &lt;- predict( cv_enet, newx=train_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # train - oof ndx_1se &lt;- match(cv_enet$lambda.1se,cv_enet$lambda) train_enet_oofProb_vec &lt;- logistic_f(cv_enet$fit.preval[,ndx_1se]) train_enet_oofClass_vec &lt;- ifelse( train_enet_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # test test_enet_predClass_vec &lt;- predict( cv_enet, newx=test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # lasso - relaxed (gamma=0) ########################## # train - cv predicted train_lassoR_predClass_vec &lt;- predict( cv_lassoR, g=0, newx=train_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # train - oof ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_lassoR_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_lassoR_oofClass_vec &lt;- ifelse( train_lassoR_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # test test_lassoR_predClass_vec &lt;- predict( cv_lassoR, g=0, newx=test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # blended mix (gamma=0.5) ############################### # train - cv predicted train_blended_predClass_vec &lt;- predict( cv_lassoR, g=0.5, newx=train_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # train - oof ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_blended_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_oofClass_vec &lt;- ifelse( train_blended_oofProb_vec &gt; 0.5, &#39;HCC&#39;, &#39;Control&#39;) # test test_blended_predClass_vec &lt;- predict( cv_lassoR, g=0.5, newx=test_lcpm_mtx, s=&#39;lambda.1se&#39;, type=&#39;class&#39; ) # put it all together ######################## all_models_confustion_mtx &lt;- rbind( train_lasso_cv = as.vector(table(train_lasso_predClass_vec, train_group_vec)), train_lasso_oof = as.vector(table(train_lasso_oofClass_vec, train_group_vec)), test_lasso = as.vector(table(test_lasso_predClass_vec, test_group_vec)), train_enet_cv = as.vector(table(train_enet_predClass_vec, train_group_vec)), train_enet_oof = as.vector(table(train_enet_oofClass_vec, train_group_vec)), test_enet = as.vector(table(test_enet_predClass_vec, test_group_vec)), train_lassoR_cv = as.vector(table(train_lassoR_predClass_vec, train_group_vec)), train_lassoR_oof = as.vector(table(train_lassoR_oofClass_vec, train_group_vec)), test_lassoR = as.vector(table(test_lassoR_predClass_vec, test_group_vec)), train_blended_cv = as.vector(table(train_blended_predClass_vec, train_group_vec)), train_blended_oof = as.vector(table(train_blended_oofClass_vec, train_group_vec)), test_blended = as.vector(table(test_blended_predClass_vec, test_group_vec)) ) colnames(all_models_confustion_mtx) &lt;- c(&#39;C:C&#39;,&#39;C:H&#39;,&#39;H:C&#39;, &#39;H:H&#39;) all_models_confustionRates_mtx &lt;- sweep( all_models_confustion_mtx, 1, rowSums(all_models_confustion_mtx), &#39;/&#39;) all_models_confustionRates_mtx &lt;- cbind(all_models_confustionRates_mtx, error = rowSums(all_models_confustionRates_mtx[,2:3])) knitr::kable(100*all_models_confustionRates_mtx, caption=&quot;confusion: Columns are Truth:Predicted&quot;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.6: confusion: Columns are Truth:Predicted C:C C:H H:C H:H error train_lasso_cv 57.6 0.7 2.9 38.7 3.7 train_lasso_oof 56.7 1.7 5.3 36.3 7.0 test_lasso 55.6 2.6 7.5 34.2 10.2 train_enet_cv 57.6 0.7 3.8 37.8 4.6 train_enet_oof 57.1 1.3 5.9 35.7 7.2 test_enet 55.3 3.0 9.0 32.7 12.0 train_lassoR_cv 56.7 1.7 3.0 38.6 4.7 train_lassoR_oof 53.8 4.6 6.4 35.2 11.0 test_lassoR 54.1 4.1 6.8 35.0 10.9 train_blended_cv 57.1 1.3 3.4 38.2 4.7 train_blended_oof 54.3 4.1 6.2 35.4 10.3 test_blended 54.9 3.4 6.8 35.0 10.2 4.5 Compare predictions at misclassified samples It is useful to examine classification errors more carefully. If models have different failure modes, one might get improved performance by combining model predictions. Note that the models considered here are not expected to compliment each other usefully as they are too similar in nature. misclass_id_vec &lt;- unique(c( names(train_lasso_oofClass_vec)[train_lasso_oofClass_vec!=train_group_vec], names(train_enet_oofClass_vec)[train_enet_oofClass_vec!=train_group_vec], names(train_lassoR_oofClass_vec)[train_lassoR_oofClass_vec!=train_group_vec], names(train_blended_oofClass_vec)[train_blended_oofClass_vec!=train_group_vec] ) ) missclass_oofProb_mtx &lt;- cbind( train_lasso_oofProb_vec[misclass_id_vec], train_enet_oofProb_vec[misclass_id_vec], train_lassoR_oofProb_vec[misclass_id_vec], train_blended_oofProb_vec[misclass_id_vec] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(train_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(train_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Control&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 4.8: out-of-fold predicted probabilities at miscassified samples As we’ve seen above, predictions from lassoR and the blended mix model are basically dichotomous; 0 or 1. Samples have been order by group, and median P(HCC) within group. For the Controls (green), predicted probabilities less than 0.5 are considered correct here. For the HCC (red) samples, predicted probabilities greater than 0.5 are considered correct here. Now look at the same plot on the test data set. misclass_id_vec &lt;- unique(c( names(test_lasso_predClass_vec[,1])[test_lasso_predClass_vec!=test_group_vec], names(test_enet_predClass_vec[,1])[test_enet_predClass_vec!=test_group_vec], names(test_lassoR_predClass_vec[,1])[test_lassoR_predClass_vec!=test_group_vec], names(test_blended_predClass_vec[,1])[test_blended_predClass_vec!=test_group_vec] ) ) missclass_oofProb_mtx &lt;- cbind( test_lasso_predProb_vec[misclass_id_vec,], test_enet_predProb_vec[misclass_id_vec,], test_lassoR_predProb_vec[misclass_id_vec,], test_blended_predProb_vec[misclass_id_vec,] ) colnames(missclass_oofProb_mtx) &lt;- c(&#39;lasso&#39;,&#39;enet&#39;, &#39;lassoR&#39;, &#39;blended&#39;) row_med_vec &lt;- apply(missclass_oofProb_mtx, 1, median) missclass_oofProb_mtx &lt;- missclass_oofProb_mtx[ order(test_group_vec[rownames(missclass_oofProb_mtx)], row_med_vec),] plot( x=c(1,nrow(missclass_oofProb_mtx)), xlab=&#39;samples&#39;, y=range(missclass_oofProb_mtx), ylab=&#39;out-of-fold predicted probability&#39;, xaxt=&#39;n&#39;, type=&#39;n&#39;) for(RR in 1:nrow(missclass_oofProb_mtx)) points( rep(RR, ncol(missclass_oofProb_mtx)), missclass_oofProb_mtx[RR,], col=ifelse(test_group_vec[rownames(missclass_oofProb_mtx)[RR]] == &#39;Control&#39;, &#39;green&#39;, &#39;red&#39;), pch=1:ncol(missclass_oofProb_mtx)) legend(&#39;top&#39;, ncol=2, legend=colnames(missclass_oofProb_mtx), pch=1:4, bty=&#39;n&#39;) abline(h=0.5) Figure 4.9: Test data predicted probabilities at miscassified samples 4.6 Compare coefficient profiles # lasso ########################## # train - cv predicted lasso_coef &lt;- coef( cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) # enet ########################## enet_coef &lt;- coef( cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) # lasso - relaxed (gamma=0) ########################## lassoR_coef &lt;- coef( cv_lassoR, s=&#39;lambda.1se&#39;, g=0 ) lassoR_coef_frm &lt;- data.frame( gene=lassoR_coef@Dimnames[[1]][c(1, lassoR_coef@i[-1])], lassoR=lassoR_coef@x) # blended mix (gamma=0.5) ############################### blended_coef &lt;- coef( cv_lassoR, s=&#39;lambda.1se&#39;, g=0.5 ) blended_coef_frm &lt;- data.frame( gene=blended_coef@Dimnames[[1]][c(1, blended_coef@i[-1])], blended=blended_coef@x) # put it all together all_coef_frm &lt;- base::merge( x = lasso_coef_frm, y = base::merge( x = enet_coef_frm, y = base::merge( x = lassoR_coef_frm, y = blended_coef_frm, by=&#39;gene&#39;, all=T), by=&#39;gene&#39;, all=T), by=&#39;gene&#39;, all=T) all_coef_frm[,-1][is.na(all_coef_frm[,-1])] &lt;- 0 par(mfrow=c(ncol(all_coef_frm)-1,1), mar=c(0,5,0,1), oma=c(3,1,2,0)) for(CC in 2:ncol(all_coef_frm)) { plot( x=1:(nrow(all_coef_frm)-1), xlab=&#39;&#39;, y=all_coef_frm[-1, CC], ylab=colnames(all_coef_frm)[CC], type=&#39;h&#39;, xaxt=&#39;n&#39;) } Figure 4.10: Coefficient Profiles Coefficients in the relaxed lasso fit are much larger than those in the lasso fit, or zero. As a consequence, the blended fit coefficients look like a shrunken version of the relaxed lasso fit coefficients. We can also examine these with a scatter plot matrix. pairs(all_coef_frm[-1,-1], lower.panel = NULL, panel = function(x, y) { points(x, y, pch = 16, col = &quot;blue&quot;) } ) Figure 4.11: Coefficients from fits 4.7 Examine feature selection Recall from glmnet vignette: It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an $\\alpha$=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of $\\alpha$ is for numerical stability; for example, the *elastic net with $\\alpha = 1 - \\epsilon$ for some small $\\epsilon$&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations*. To see how this plays out in this dataset, we can look at feature expression heat maps. suppressPackageStartupMessages(require(gplots)) # train - cv predicted lasso_coef &lt;- coef( cv_lasso, s=&#39;lambda.1se&#39; ) lasso_coef_frm &lt;- data.frame( gene=lasso_coef@Dimnames[[1]][c(1, lasso_coef@i[-1])], lasso=lasso_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(train_lcpm_mtx[,lasso_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=lasso_coef_frm$gene, labCol=train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(train_group_vec==&#39;Control&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;lasso genes - N =&#39;, nrow(lasso_coef_frm)-1)) Figure 4.12: Lasso Model Genes suppressPackageStartupMessages(require(gplots)) # train - cv predicted enet_coef &lt;- coef( cv_enet, s=&#39;lambda.1se&#39; ) enet_coef_frm &lt;- data.frame( gene=enet_coef@Dimnames[[1]][c(1, enet_coef@i[-1])], enet=enet_coef@x) Mycol &lt;- colorpanel(1000, &quot;blue&quot;, &quot;red&quot;) heatmap.2( x=t(train_lcpm_mtx[,enet_coef_frm$gene[-1]]), scale=&quot;row&quot;, labRow=enet_coef_frm$gene, labCol=train_group_vec, col=Mycol, trace=&quot;none&quot;, density.info=&quot;none&quot;, #margin=c(8,6), lhei=c(2,10), #lwid=c(0.1,4), #lhei=c(0.1,4) key=F, ColSideColors=ifelse(train_group_vec==&#39;Control&#39;, &#39;green&#39;,&#39;red&#39;), dendrogram=&quot;both&quot;, main=paste(&#39;enet genes - N =&#39;, nrow(enet_coef_frm)-1)) Figure 4.13: Enet Model Genes "],
["model-suite.html", "Section 5 Fitted Model Suite 5.1 Sample quality scores 5.2 Model suite", " Section 5 Fitted Model Suite We examine the results of fitting a suite of models to investigate the effect of sample size on various aspects of model performance: assessed accuracy out-of-fold estimates of precision and variability and cv assessed accuracy bias. selected feature profile stability - to what extent does the feature set implicitly selected by the lasso vary across random sampling and what is the effect of sample size. It is hypothesized that below a certain threshold, sample sizes are too small to provide reliable estimates of performance and stable selected feature profiles. We will attempt to separate variability which is due to sample size from variability due to sample composition. To to this we will track sample quality. Predicted probabilities from fitted model can be transformed into sample quality scores: \\(Q_i = p_i^{y_i}(1-p_i)^{1-y_i}\\), where \\(p_i\\) is the estimated probability of HCC for sample i and \\(y_i\\) is 1 for HCC samples and 0 for Controls. ie. we use the fitted likelihood as a sample quality score. To derive the quality scores, we will use the predicted response from a lasso model fitted to the entire data set. Hard to classify samples will have low quality scores. In the results that we discuss below, when we look at variability across repeated random sampling of different sizes, we can use sample quality scores to investigate how much of the variability is due to sample selection. Note that quality here is not used to say anything about the sample data quality. Low quality here only means that a sample is different from the core of the data set in a way that makes it hard to properly classify. That could happen if the sample were mislabeled, in which case we could think of this sample as being poor quality of course. 5.1 Sample quality scores 5.2 Model suite 5.2.1 Simulation setup "],
["variable-importance.html", "Section 6 Variable importance", " Section 6 Variable importance Cai et al. [2] used the fequency of selection across bootstrap replicates to select features form their final model. We sus[ect that this simply selects features with large coefficients which would correspond the the variable importance metric used in the caret package. In this section we measure variable importance as the loss in performance when the variable is left out of the model. Depending on the correlation structure, it is quite possible for a feature to have a large coefficient and be un-important, in our sense of the word. It is also possible for a feature to be frequently selected in bootstrap samples, and still not un-important. In a sense, this measure of importance is really a measure of single importance. We should therefore also have a measure of group importance. Other questions … "],
["conclusions.html", "Section 7 Conclusions", " Section 7 Conclusions We have found that … Other questions … "],
["references.html", "References", " References "]
]
