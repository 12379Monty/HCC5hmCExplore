[
["index.html", "DNA Hydroxymethylation in Hepatocellular Carcinoma Preamble License", " DNA Hydroxymethylation in Hepatocellular Carcinoma Francois Collin 2020-09-06 Preamble This vignette offers some exploratory data analyses of data available from the NCBI GEO web site. License This work by Francois Collin is licensed under a Creative Commons Attribution 4.0 International License "],
["intro.html", "Section 1 Introduction", " Section 1 Introduction The goal of detecting cancer at the earliest stage of development with a non-invasive procedure has busied many groups with the task of perfecting techniques to support what has become commonly known as a liquid biopsy - the analysis of biomarkers circulating in fluids such as blood, saliva or urine. Epigenetic biomarkers present themselves as good candidates for this application (Gai and Sun (2019) [1]). In particular, given their prevalence in the human genome, close correlation with gene expression and high chemical stability, DNA modifications such as 5-methylcytosine (5mC) and 5-hydroxymethylcytosine (5hmC) are DNA epigenetic marks that provide much promise as cancer diagnosis biomarkers that could be profitably analyzed in liquid biopsies [2–5]. Li et al. (2017) [3] used a sensitive and selective chemical labeling technology to extract genome-wide 5hmC profiles from circulating cell-free DNA (cfDNA) as well as from genomic DNA (gDNA) collected from a cohort of 260 patients recently diagnosed with colorectal, gastric, pancreatic, liver or thyroid cancer and normal tissues from 90 healthy individuals They found 5hmC-based biomarkers of circulating cfDNA to be highly predictive of some cancer types. Similar small sample size findings were reported in Song et al. (2017) [4]. Focusing on hepatocellular carcinoma, Cai et al. (2019) [2] assembled a sizable dataset to demonstrate the feasibility of using features derived from 5-hydroxymethylcytosines marks in circulating cell-free DNA as a non-invasive approach for the early detection of hepatocellular carcinoma. The data that are the basis of that report are available on the NCBI GEO web site (Series GSE112679). The data have also been bundled in a R data package which can be installed from github: if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) install.packages(&quot;devtools&quot;) devtools::install_github(&quot;12379Monty/GSE112679&quot;) An important question in the early development of classifiers of the sorts that are the basis of any liquid biopsy diagnostic tool is how many samples should be collected to make properly informed decisions. In this report we will explore the GSE112679 data to shed some light on the relationship between sample size and model performance in the context classifying samples based on 5hmC data. In Section 2 we preprocess the data that we will use for the classification analysis and perform some light QC analyses. In Section 3 we provide some background to our modeling approach. In Section ?? we explore some glmnet fits that discriminate between early stage HCC and control samples. In Section 6 we examine the results of fitting a suite of models to investigate the effect of sample size on model performance. "],
["preproc.html", "Section 2 Preprocessing 2.1 Load the data 2.2 Differential representation analysis 2.3 Signal-to-noise ratio regime", " Section 2 Preprocessing 2.1 Load the data The data that are available from NCBI GEO Series GSE112679 can be conveniently accessed through an R data package. Attaching the GSE112679 package makes the count data tables available as well as a gene annotation table and a sample description table. See GSE112679 R Data Package page. For the Cai et al. [2] model fitting and analysis, samples were separated into Train and Val-1 subsets. Val-2 was an external validation set. if (!(&quot;GSE112679&quot; %in% rownames(installed.packages()))) { if (!requireNamespace(&quot;devtools&quot;, quietly = TRUE)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;12379Monty/GSE112679&quot;) } library(GSE112679) sampDesc$DxStage &lt;- with(sampDesc, ifelse(outcome==&#39;HCC&#39;, paste0(outcome,&#39;:&#39;,stage), outcome)) with( sampDesc %&gt;% dplyr::filter(sampType == &quot;blood&quot;), knitr::kable(table(DxStage, trainValGroup, exclude = NULL), caption=&quot;GSE112679 Samples by Dx Group and Subset&quot;) %&gt;% kableExtra::kable_styling(full_width = F) ) ## Warning in kableExtra::kable_styling(., full_width = F): Please specify format ## in kable. kableExtra can customize either HTML or LaTeX outputs. See https:// ## haozhu233.github.io/kableExtra/ for details. Table 2.1: GSE112679 Samples by Dx Group and Subset Train Val-1 Val-2 Benign 253 132 3 CHB 190 96 0 Cirrhosis 73 33 0 HCC:Early 335 220 24 HCC:Late 0 442 13 HCC:NA 0 147 23 Healthy 269 124 177 For this analysis, we will consider early stage cancer samples and healthy or benign samples from the Train or Val-1 subsets. The appropriate outcome variable will be renamed or aliased group Table 2.2: Samples used in this analysis group Freq Control 778 HCC 555 The features are counts of reads captured by chemical labeling, and indicate the level of 5-hydroxymethylcytosines within each gene body. Cai et al. (2019), Li et al. (2017) and Song et al. (2017) [2–4] all analyze 5hmC gene body counts using standard RNA-Seq methodologies, and we will do the same here. Note that before conducting any substantive analyses, the data would normally be very carefully examined for any sign of quality variation between groups of samples. This analysis would integrate sample meta data - where and when were the blood samples collected - as well as library preparation and sequencing metrics in order to detect any sign of processing artifacts that may be present in the dataset. This is particularly important when dealing with blood samples as variable DNA quality degradation is a well known challenge that is encountered when dealing with such samples [6]. Although blood specimen handling protocols can be put in place to minimize quality variation [7], variability can never be completely eradicated, especially in the context of blood samples collected by different groups, working in different environments. The problem of variable DNA quality becomes paricularly pernicuous when it is compounded with a confounding factor that sneaks in when the control sample collection events are separated in time and space from the cancer sample collection events; an all too common occurence. As proper data QC requires an intimate familiarity with the details of data collection and processing, such a task cannot be untertaken here. We will simply run a minimal set of QC sanity checks to make sure that there are no apparent systematic effects in the data. We first look at coverage - make sure there isn’t too much disparity of coverage across samples. To detect shared variability, samples can be annotated and ordered according to sample features that may be linked to sample batch processing. Here we the samples have been ordered by group and sample id (an alias of geoAcc). Figure 2.1: Sample log2 count boxplots Table 2.3: Coverage Summary - Columns are sample coverage quantiles and total coverage Rows are quartiles across samples 15% 25% 50% 75% totCovM 25% 4 24 111 321 5.5 50% 5 30 135 391 6.7 75% 6 35 162 468 8.0 From this table, we see that 25% of the samples have total coverage exceeding 8M reads, 25% of samples have a 15 percentile of coverage lower than 4, etc. 2.2 Differential representation analysis In the remainder of this section, we will process the data and perform differential expression analysis as outlined in Law et al. (2018) [8]. The main analysis steps are: remove lowly expressed genes normalize gene expression distributions remove heteroscedascity fit linear models and examine DE results It is good practice to perform this differential expression analysis prior to fitting models to get an idea of how difficult it will be to discriminate between samples belonging to the different subgroups. The pipeline outlined in Law et al. (2018) [8] also provides some basic quality assessment opportunities. Remove lowly expressed genes Genes that are not expressed at a biologically meaningful level in any condition should be discarded to reduce the subset of genes to those that are of interest, and to reduce the number of tests carried out downstream when looking at differential expression. Carrying un-informative genes may also be a hindrance to classification and other downtream analyses. To determine a sensible threshold we can begin by examining the shapes of the distributions. Figure 2.2: Sample \\(log_2\\) CPM densities As is typically the case with RNA-Seq data, we notice many weakly represented genes in this dataset. A cpm value of 1 appears to adequatly separate the expressed from the un-expressed genes, but we will be slightly more strict here and require a CPM threshold of 3 . Using a nominal CPM value of 3, genes are deeemed to be represented if their expression is above this threshold, and not represented otherwise. For this analysis we will require that genes be represented in at least 25 samples across the entire dataset to be retained for downstream analysis. Here, a CPM value of 3 means that a gene is represented if it has at least 9 reads in the sample with the lowest sequencing depth (library size 2.9 million). Note that the thresholds used here are arbitrary as there are no hard and fast rules to set these by. The voom-plot, which is part of analyses done to remove heteroscedasticity, can be examined to verify that the filtering performed is adequate. Remove weakly represented genes and replot densities. Removing 17.5% of genes… Figure 2.3: Sample \\(log_2\\) CPM densities after removing weak genes As another sanity check, we will look at a multidimensional scaling plot of distances between gene expression profiles. We use plotMDS in limma package [9]), which plots samples on a two-dimensional scatterplot so that distances on the plot approximate the typical log2 fold changes between the samples. Before producing the MDS plot we will normalize the distributions. We will store the data into s DGEList object as this is convenient when running many of the analyses implemented in the edgeR and limma packages. Call the set ‘AF’, for set ‘A’, ‘Filtered’. AF_dgel &lt;- edgeR::DGEList( counts = featureCountsAF, genes = genes_annotAF, samples = sampDescA, group = sampDescA$group ) AF_dgel &lt;- edgeR::calcNormFactors(AF_dgel) AF_lcmp_mtx &lt;- edgeR::cpm(AF_dgel, log = T) # Save AF_dgel to facilitate restarting # remove from final version save(list = &quot;AF_dgel&quot;, file = &quot;RData/AF_dgel&quot;) Verify that the counts are properly normalized. Figure 2.4: Sample log2 count boxplots Proceed with MDS plots. Figure 2.5: MDS plots of log-CPM values The MDS plot, which is analogous to a PCA plot adapted to gene exression data, does not indicate strong clustering of samples. The fanning pattern observed in the first two dimensions indicates that a few samples are drifting way from the core set, but in no particular direction. There is some structure in the 3rd and 4th dimension plot which should be investigated. glMDSPlot from package Glimma provides an interactive MDS plot that can extremely usful for exploration Link to glMDSPlot: Here No obvious factor links the samples in the 3 clusters observed on the 4th MDS dimensions. The percent of variance exaplained by this dimension or \\(\\approx\\) 4%. The glMDSPlot indicates further segregation along the 6th dimension. The percent of variance exaplained by this dimension or \\(\\approx\\) 2%. Tracking down this source of variability may be quite challenging, especially without having the complete information about the sample attributes and provenance. Unwanted variability is a well-documented problem in the analysis of RNA-Seq data (see Peixoto et al. (2015) [10]), and many procedures have been proposed to reduce the effect of unwanted variation on RNA-Seq analsys results ([10–12]). There are undoubtedly some similar sources of systematic variation in the 5hmC data, but it is beyond the scope of this work to investigate these in this particular dataset. Given that the clustering of samples occurs in MDS dimensions that explain a small fraction of variability, and that these is no assocation with the factor of interest, HCC vs Control, these sources of variability should not interfere too much with our classification analysis. It would nonetheless be interesting to assess whether downstream results can be improved by removing this variability. Creating a design matrix and contrasts Before proceeding with the statistical modeling used for the differential expression analysis, we need to set up a model design matrix. ## colSums(Design_mtx): ## Control HCC ## 778 555 ## Contrasts: ## Contrasts ## Levels HCCvsControl ## Control -1 ## HCC 1 Removing heteroscedasticity from the count data As for RNA-Seq data, for 5hmC count data the variance is not independent of the mean. In limma, the R package we are using for our analyses, linear modeling is carried out on the log-CPM values which are assumed to be normally distributed and the mean-variance relationship is accommodated using precision weights calculated by the voom function. We apply this transformation next. par(mfrow=c(1,1)) filteredCountsAF_voom &lt;- limma::voom(AF_dgel, Design_mtx, plot=T) Figure 2.6: Removing heteroscedascity Note that the voom-plot provides a visual check on the level of filtering performed upstream. If filtering of lowly-expressed genes is insufficient, a drop in variance levels can be observed at the low end of the expression scale due to very small counts. Fit linear models and examine the results Having properly filtered and normalized the data, the linear models can be fitted to each gene and the results examined to assess differential expression between the two groups of interest, in our case HCC vs Control. Table 2.4 displays the counts of genes in each DE category: Table 2.4: DE Results at FDR = 0.05 Down NotSig Up HCCvsControl 5214 5280 5258 Graphical representations of DE results: MD Plots To summarise results for all genes visually, mean-difference plots (aka MA plot), which display log-FCs from the linear model fit against the average log-CPM values can be generated using the plotMD function, with the differentially expressed genes highlighted. We may also be interested in whether certain gene features are related to gene identification. Gene GC content, for example, might be of interest. Figure 2.7: HCC vs Control - Genes Identified at FDR = 0,05 Table 2.5: log FC quartiles by gene identification down notDE up 25% -0.07 -0.01 0.04 50% -0.05 0.00 0.06 75% -0.03 0.01 0.09 While many genes are identified, the effect sizes are quite small, which results in a low signal-to-noise ratio context. See Section 2.3 below. The log-fold-change distribution for up-represented genes is long-tailed, with many high log fold-change values. By contrast, log-fold-change distribution for down-represented genes closer to symmetric and has few genes with low log fold-change values. We will see how this affects the results of identifying genes with an effect size requirement. The GC content of down regulated genes tends to be slightly lower than the rest of the genes. A statistical test would find that the difference between the mean of the down regulated gene population is singificantly different than the mean of the other gene population even though the difference is quite small (-0.028). These asymmetries are minor, but it would still be good to establish that they relfect biology rather than processing artifacts. DE genes at 10% fold change For a stricter definition on significance, one may require log-fold-changes (log-FCs) to be above a minimum value. The treat method (McCarthy and Smyth 2009 [13]) can be used to calculate p-values from empirical Bayes moderated t-statistics with a minimum log-FC requirement. The number of differentially expressed genes are greatly reduced if we impose a minimal fold-change requirement of 10%. ## 10% FC Gene Identification Summary - voom, adjust.method = BH, p.value = 0.05: ## HCCvsControl ## Down 3 ## NotSig 15550 ## Up 199 Figure 2.8: HCC vs Control - Identified Genes at FDR = 0,05 and logFC &gt; 10% As noted above, the log-fold-change distribution for the up-represented genes is long-tailes in comparison to log-fold-change distribution for the down-represented genes. As a result fewer down-represented than up-regulated genes are identified when a minimum log-FC requirement is imposed. 2.3 Signal-to-noise ratio regime In Hastie et al. (2017) [14]) results from lasso fits are compared with best subset and forward selection fits and it is argued that while best subset is optimal for high signal-to-noise regimes, the lasso gains some competitive advantage when the prevailing signal-to-noise ratio of the dataset is lowered. We can extract sigma and signal from the fit objects to get SNR values for each gene to see in what SNR regime the 5hmC gene body data are. lib.size &lt;- colSums(AF_dgel$counts) fit &lt;- filteredCountsAF_voom_efit sx &lt;- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06) sy &lt;- sqrt(fit$sigma) CV &lt;- sy/sx Figure 2.9: Cumulative Distribution of SNR - rug = 25, 50, 75 and 90th percentile Table 2.6: SNR Quantiles 25% 50% 75% 90% 0.018 0.036 0.06 0.082 These SNR values are in the range where the lasso and relaxed lasso gain some advantage over best subset and forward selection fits (see Hastie et al. (2017) [14]). "],
["modeling-background.html", "Section 3 Modeling - Background 3.1 Predictive modeling for genomic data 3.2 glmnet", " Section 3 Modeling - Background Refer to first pass study for relevant exploratory data analyis results. In the section we look at some models fitted to discriminate between early stage HCC and healthy and benign samples (grouped as Controls here) from the GSE112679 data set. Some questions to addredd with the baseline model how separable are the data: what accuracy do we expect individual sample quality scores: which samples are hard to classify? Compute a score in [0, 1], where 1 is perfectly good classification and 0 is poerfectly bad. 3.1 Predictive modeling for genomic data The main challenge in calibrating predictive models to genomic data is that there are many more features than there are example cases to fit to; the now classic \\(n &lt;&lt; p\\) problem. In this scenario, fitting methods tend to overfit. The problem can be addressed by selecting variables, regularizng the fit or both. See the Trevor Hastie talk: Statistical Learning with Big Data - Trevor Hastie for a good discussion of this problem and potential solutions. 3.1.1 caret for model evaluation The caret Package provide a set of functions that streamline the process for fitting and evalluating a large numbet of predictive models in parallel. The package contains tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation The tools facilitate the process of automating randomly spliting data sets into training, testing and evaluating so that predictive models can be evaluated on a comparable and exhaustive basis. Especially useful is the functionality that is provided to repeatedly randomly stratify samples into train and test set so that any sample selection bias is removed. What makes the caret package extremely useful is that a common interface is provided to an exhaustive collection of fitting procedures. Without this common interface, one has to learn the programming interfaces that are used in all fitting procedures to be included in a comparative analysis, which can be quite burdenful. Some of the models which can be evaluated with caret include: (only some of these can be used with multinomial responses) FDA - Flexible Discriminant Analysis stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection knn - k nearest neighbors pam - Nearest shrunken centroids rf - Random forests svmRadial - Support vector machines (RBF kernel) gbm - Boosted trees xgbLinear - eXtreme Gradient Boosting xgbTree - eXtreme Gradient Boosting neuralnet - neural network Many more models can be implemented and evaluated with caret, including some deep learning methods. Simulated Annealing Feature Selection and Genetic Algorithms. Many methods found here are also worth investigating. We only mention caret here because it is an exteremely useful tool for anyone interested in comparing many predictive models. We have done that in the past and have found that regularized regression models perform as welll as any in the context of classifiction based on genomic scale data. 3.2 glmnet In this investigation we will focus on models that can be analyzed with the the glmnet R package [15]. Several factors favor this choice: the glmnet package is a well supported package providing extensive functionality for regularized regression and classification models. the hyper-parameters of the elastic net enable us to explore the relationship between model size, or sparsity, and predictive accuracy. ie. we can investigate the “bet on sparsity” principle: Use a procedure that does well in sparse problems, since no procedure does well in dense problems. in our experience building classifiers from genomic scale data, regularized classification models using the elastic net penalty do as well as any other, and are more economical in terms of computing time, espacially in comparison to the more exotic boosting algorithms. the lasso has been shown to be near optimal for the \\(n&lt;&lt;p\\) problem over a wide range of signal-to-noise regiments. Much of the following comes from the Glmnet Vignette. Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter lambda ([15–18]). glmnet solves the following problem: \\[\\min_{\\beta_0,\\beta} \\frac{1}{N} \\sum_{i=1}^{N} w_i l(y_i,\\beta_0+\\beta^T x_i) + \\lambda\\left[(1-\\alpha)||\\beta||_2^2/2 + \\alpha ||\\beta||_1\\right],\\] over a grid of values of \\(\\lambda\\). Here \\(l(y,\\eta)\\) is the negative log-likelihood contribution for observation i; e.g. for the Gaussian case it is \\(\\frac{1}{2}(y-\\eta)^2\\). alpha hyper-parameter The elastic-net penalty is controlled by \\(\\alpha\\), and bridges the gap between lasso (\\(\\alpha\\)=1, the default) and ridge (\\(\\alpha\\)=0). The tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an \\(\\alpha\\)=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of \\(\\alpha\\) is for numerical stability; for example, the elastic net with \\(\\alpha = 1 - \\epsilon\\) for some small \\(\\epsilon\\)&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations. Lasso vs Best Subset Best subset selection \\[\\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_0 \\leq k\\] lasso \\[\\min_{\\beta \\in \\mathcal{R}^p} ||Y - X\\beta||^2_2 \\, \\, subject \\, to \\, \\, ||\\beta||_1 \\leq t\\] Bertsimas et al. (2016) [19] presented a mixed integer optimization (MIO) formulation for the best subset selection problem Using these MIO solvers, can solve problems with p in the hundreds and even thousands demonstrated that best subset selection generally gives superior prediction accuracy compared to forward stepwise selection and the lasso, over a variety of problem setups. Hastie et al. (2017) [14] neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes; best subset selection and forward stepwise perform quite similarly throughout; the relaxed lasso is the overall winner, performing just about as well as the lasso in low SNR scenarios, and as well as best subset selection in high SNR scenarios. We conclude that it is able to use its auxiliary shrinkage parameter (\\(\\gamma\\)) to get the “best of both worlds”: it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not. relaxed lasso \\[\\hat{\\beta}^{relax}(\\lambda, \\gamma) = \\gamma \\beta^{lasso}(\\lambda) + (1 - \\gamma)(\\beta^{LS}(\\lambda)\\] shrunken relaxed lasso (aka the blended fit) Suppose the glmnet fitted linear predictor at \\(\\lambda\\) is \\(\\hat{\\eta}_\\lambda(x)\\) and the relaxed version is \\(\\tilde{\\eta}_\\lambda(x)\\), then the shrunken relaxed lasso fit is \\[\\tilde{\\eta}_{\\lambda,\\gamma}(x)=(1-\\gamma)\\tilde{\\eta}_\\lambda(x) + \\gamma \\hat{\\eta}_\\lambda(x)\\] \\(\\gamma \\in [0,\\, 1]\\) is an additional tuning parameter which can be selected by cross validation. The debiasing will potentially improve prediction performance, and CV will typically select a model with a smaller number of variables. This procedure is very competitive with forward-stepwise and best-subset regression, and has a considerable speed advantage when the number of variables is large. This is especially true for best-subset, but even so for forward stepwise. The latter has to plod through the variables one-at-a-time, while glmnet will just plunge in and find a good active set. Further details may be found in Friedman, Hastie, and Tibshirani (2010), Tibshirani et al. (2012), Simon et al. (2011), Simon, Friedman, and Hastie (2013) and Hastie, Tibshirani, and Tibshirani (2017) ([14–18]). SNR \\(y_0=f(x_0) + \\epsilon_0\\) \\(SNR=\\frac{var(f(x_0))}{var(\\epsilon_0)}\\) \\(PVE(g)=1 - \\frac{\\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}\\) \\(PVE(f) = 1 - \\frac{Var(\\epsilon_0)}{Var(y_0)} = \\frac{SNR}{1+SNR}\\) \\(SNR = \\frac{PVE}{1-PVE}\\) \\(c_v = \\frac{\\sigma}{\\mu}=\\frac{Var(y)}{\\mathbb{E}(y)}\\) a PVE of 0.5 is rare for noisy observational data, and 0.2 may be more typical A PVE of 0.86, corresponding to an SNR of 6, is unheard of! For small SNR, SNR \\(\\approx\\) PVE See Xiang et al. (2020) [20], Lozoya et al. (2018) [21], Simonson et al. (2018) [22] and Rapaport et al. (2013) [23] for SNR in RNA-Seq "],
["explore-sparsity.html", "Section 4 The bet on sparsity 4.1 CV analysis setup 4.2 Fit and compare models 4.3 Relaxed lasso and blended mix 4.4 Examination of sensitivity vs specificity 4.5 Refit with “auc” as optimization 4.6 Relaxed lasso and blended mix", " Section 4 The bet on sparsity In this section we explore various fits that can be computed ans analyzed with tools provided in the glmnet package. Refer to the Glmnet Vignette for a quick reference guide. 4.1 CV analysis setup K_FOLD &lt;- 10 trainP &lt;- 0.8 EPS &lt;- 0.02 # Have no idea what &quot;small&quot; epsilon means First we divide the analysis dataset into train and test in a 4:1 ratio. set.seed(1) train_sampID_vec &lt;- with(AF_dgel$samples, AF_dgel$samples$sampID[caret::createDataPartition(y=group, p=trainP, list=F)] ) test_sampID_vec &lt;- with(AF_dgel$samples, setdiff(sampID, train_sampID_vec) ) train_group_vec &lt;- AF_dgel$samples[train_sampID_vec, &#39;group&#39;] test_group_vec &lt;- AF_dgel$samples[test_sampID_vec, &#39;group&#39;] knitr::kable(table(train_group_vec), caption=&quot;Train set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Train set train_group_vec Freq Control 623 HCC 444 knitr::kable(table(test_group_vec), caption=&quot;Test set&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Test set test_group_vec Freq Control 155 HCC 111 train_lcpm_mtx &lt;- t(lcpm_mtx[,train_sampID_vec]) test_lcpm_mtx &lt;- t(lcpm_mtx[,test_sampID_vec]) We explore some glmnet fits and the “bet on sparsity” Consider models: lasso: \\(\\alpha = 1.0\\) - sparse model ridge \\(\\alpha = 0\\) - shrunken coefficients model elastic net: \\(\\alpha = 0.5\\) - semi sparse model Does the relaxed lasso improve performance? Does the shrunken relaxed lasso (aka the blended mix) improve performance How sparse is the model undelying best 5hmC classifier for Early HCC vs Control? Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set? In this analysis, we will only evaluate models in terms of model size, stability and performance. We leave the question of significance testing of hypotheses about model parameters completely out. See Lockhart et al. (2014) [24] and Wassermam (2014) [25] for a discussion of this topic. Next we create folds for 10-fold cross-validation of models fitted to training data. We’ll use caret::createFolds to assign samples to folds while keeping the outcome ratios constant across folds. # This is too variable, both in terms of fold size And composition #foldid_vec &lt;- sample(1:10, size=length(train_group_vec), replace=T) set.seed(1) train_foldid_vec &lt;- caret::createFolds( factor(train_group_vec), k=K_FOLD, list=F) knitr::kable(sapply(split(train_group_vec, train_foldid_vec), table), caption=&quot;training samples fold composition&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.2: training samples fold composition 1 2 3 4 5 6 7 8 9 10 Control 62 62 62 63 62 62 63 62 63 62 HCC 45 44 44 45 44 45 44 45 44 44 Note that the folds identify samples that are left-out of the training data for each fold fit. 4.2 Fit and compare models cross-validated accuracy test set accuracy sparsity for lasso, enet , examine number of selected variables Although “the one standard error rule” can produce a model with fewer predictors, it usually results in increased MSE and more biased parameter estimates (see Engebretsen et al. (2019) [26] for example). We will look at both the minimum cv error and the one standard error rule model preformance. 4.2.1 Logistic regression in glmnet glmnet provides functionality to extract various predicted of fitted values from calibrated models. Note in passsing that some folks make a distinction between fitted or estimated values for sample points in the training data vurses predicted values for sample points that are not in the training dataset. glmnet makes no such distinction and the predict function is used to produce both fitted as well as predicted values. For logistic regressions, which is the model fitted in a regularized fashion when models are fitted by glmnet with the parameter family='binomial', three fitted or predicted values can be extracted at a given design point. Suppose our response variable Y is either 0 or 1 (Control or HCC in our case). These are specified by the type parameter. type='resp' returns the fitted or predicted probability of \\(Y=1\\). type='class' returns the fitted or predicted class for the design point, which is simply dichotomizing the response: class = 1 if the fitted or predicted probability is greater than 0.5 (check to make sure class is no the Bayes estimate). type='link' returns the fitted or predicted vealue of the linear predictor \\(\\beta&#39;x\\). The relationship between the linear predictor and the response can be derided from the logis toc regression model: \\[P(Y=1|x,\\beta) = g^{-1}(\\beta&#39;x) = h(\\beta&#39;x) = \\frac{e^{\\beta&#39;x}}{1+e^{\\beta&#39;x}}\\] where \\(g\\) is the link function, \\(g^{-1}\\) the mean function. The link function is given by: \\[g(y) = h^{-1}(y) = ln(\\frac{y}{1-y})\\] This link function is called the logit function, and its inverse the logistic function. logistic_f &lt;- function(x) exp(x)/(1+exp(x)) It is important to note that all predicted values extracted from glmnet fitted models by the predict() extraction method yield fitted values for design points that are part of the training data set. This includes the predicted class for training data which ae used to estimate misclassification error rates. As a result, the cv error rates quoted in various glmnet summaries are generally optimistic. glmnet fitting functions have a parameter, keep, which instructs the fitting function to keep the out-of-fold, or prevalidated, predictions as part of the returned object. The out-of-fold predictions are predicted values for the samples in the left-out folds, pooled across all cv folds. For each hyper-parameter specification, we get one full set of out-of-fold predictions for the training set samples. Performance assessments based on these values are usually more generalizable - ie. predictive of performance in unseen data - than assessments based on values produced from the full fit, which by default is what glmnet extraction methods provide. See Höfling and Tibshirani (2008) [27] for a description of the use of pre-validation in model assessment. Because the keep=T option will store predicted values for all models evaluated in the cross-validation process, we will limit the number of models tested by setting nlambda=30 when calling the fitting functions. This has no effect on performance in this data set. start_time &lt;- proc.time() cv_lasso &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=1, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;lasso time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lasso time: 13.3s start_time &lt;- proc.time() cv_ridge &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;ridge time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## ridge time: 104.5s start_time &lt;- proc.time() cv_enet &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0.5, family=&#39;binomial&#39;, type.measure = &quot;class&quot;, keep=T, nlambda=30 ) message(&quot;enet time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## enet time: 12.3s The ridge regression model takes over 10 times longer to compute. Examine model performance. ## Warning: package &#39;glmnet&#39; was built under R version 4.0.2 ## Loading required package: Matrix ## Loaded glmnet 4.0-2 Figure 4.1: compare fits errors_frm &lt;- data.frame( lasso = lasso_errors_mtx, ridge = rifge_errors_mtx, enet = enet_errors_mtx ) knitr::kable(t(errors_frm)*100, caption = &#39;Misclassifiaction error rates&#39;, digits=1) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.3: Misclassifiaction error rates train_cv train_oof test lasso.error_1se 7.0 9.7 10.2 lasso.error_min 7.0 9.7 10.2 ridge.error_1se 13.5 19.0 18.4 ridge.error_min 12.5 16.9 16.2 enet.error_1se 7.2 11.1 12.0 enet.error_min 6.7 9.2 10.2 We see that the lasso and enet models do better than the ridge model. These is very little difference between the min lambda and the the one standard error rule lambda models (the two are the same for the lasso in this data set). We also see that the training data out-of-fold estimates of misclassification error rates are much closer to the test set estimates than are the cv estimated rates. This has been our experience with regularized regression models fitted to genomic scale data. It should also be noted thay the cv estimates of misclassification rates become more biased as the sample size decreases, as we will show in Section 6. 4.3 Relaxed lasso and blended mix Next we look at the so-called relaxed lasso and the blended mix which is an optimized shrinkage between the relaxed lasso and the regular lasso. ## ## Call: glmnet::cv.glmnet(x = train_lcpm_mtx, y = train_group_vec, type.measure = &quot;class&quot;, foldid = train_foldid_vec, keep = T, relax = T, alpha = 1, family = &quot;binomial&quot;, nlambda = 30) ## ## Measure: Misclassification Error ## ## Gamma Lambda Measure SE Nonzero ## min 0.5 0.0379 0.06748 0.005274 35 ## 1se 0.5 0.0379 0.06748 0.005274 35 Figure 4.2: lassoR fit Table 4.4: Relaxed lasso and blended mix error rates train_blended_cv 6.7 train_blended_oof 10.2 train_relaxed_oof 11.1 test_blended_oof 10.2 test_relaxed_oof 10.9 The relaxed lasso and blended mix error rates are comparable to the regular lasso fit error rate. We see here too that the reported cv error rates are quite optimistic, while out-of-fold error rates continue to be good indicaters of unseen data error rates. 4.4 Examination of sensitivity vs specificity In the results above we reported error rates without inspecting the sensitivity vurses specificity trade off. Here we look at this question with the help of ROC curves. 4.4.1 Training data out-of-fold ROC curves # train # lasso ndx_1se &lt;- match(cv_lasso$lambda.1se,cv_lasso$lambda) train_lasso_oofProb_vec &lt;- logistic_f(cv_lasso$fit.preval[,ndx_1se]) train_lasso_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_lasso_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # lasso - relaxed ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_lassoR_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0&#39;]][,ndx_1se]) train_lassoR_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_lassoR_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases # blended mix (gamma=0.5) ndx_1se &lt;- match(cv_lassoR$lambda.1se,cv_lassoR$lambda) train_blended_oofProb_vec &lt;- logistic_f(cv_lassoR$fit.preval[[&#39;g:0.5&#39;]][,ndx_1se]) train_blended_roc &lt;- pROC::roc( response = as.numeric(train_group_vec==&#39;HCC&#39;), predictor = train_blended_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(train_lasso_roc) lines(train_lassoR_roc, col=&#39;blue&#39;) lines(train_blended_roc, col=&#39;green&#39;) legend(&#39;bottomright&#39;, title=&#39;AUC&#39;, legend=c( paste(&#39;lasso =&#39;, round(train_lasso_roc[[&#39;auc&#39;]],3)), paste(&#39;lassoR =&#39;, round(train_lassoR_roc[[&#39;auc&#39;]],3)), paste(&#39;blended =&#39;, round(train_blended_roc[[&#39;auc&#39;]],3)) ), text.col = c(&#39;black&#39;, &#39;blue&#39;, &#39;green&#39;)) Figure 4.3: Train data out-of-sample ROCs Compare thresholds for 90% Specificity: lasso_ndx &lt;- with(as.data.frame(pROC::coords(train_lasso_roc, transpose=F)), min(which(specificity &gt;= 0.9))) lassoR_ndx &lt;- with(as.data.frame(pROC::coords(train_lassoR_roc, transpose=F)), min(which(specificity &gt;= 0.9))) blended_ndx &lt;- with(as.data.frame(pROC::coords(train_blended_roc, transpose=F)), min(which(specificity &gt;= 0.9))) spec90_frm &lt;- data.frame(rbind( lasso=as.data.frame(pROC::coords(train_lasso_roc, transpose=F))[lasso_ndx,], lassoR=as.data.frame(pROC::coords(train_lassoR_roc, transpose=F))[lassoR_ndx,], blended=as.data.frame(pROC::coords(train_blended_roc, transpose=F))[blended_ndx,] )) knitr::kable(spec90_frm, digits=3, caption=&quot;Specificity = .90 Coordinates&quot; ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.5: Specificity = .90 Coordinates threshold specificity sensitivity lasso 0.337 0.9 0.932 lassoR 0.003 0.9 0.894 blended 0.031 0.9 0.899 This is strange. par(mfrow=c(1,3)) # lasso plot(density(train_lasso_oofProb_vec[train_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(train_lasso_oofProb_vec[train_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;lasso&quot;) # lassoR plot(density(train_lassoR_oofProb_vec[train_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(train_lassoR_oofProb_vec[train_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;lassoR&quot;) sapply(split(train_lassoR_oofProb_vec,train_group_vec), summary) ## Control HCC ## Min. 1.328376e-69 3.337696e-41 ## 1st Qu. 6.241943e-28 1.000000e+00 ## Median 8.812711e-19 1.000000e+00 ## Mean 7.668147e-02 8.481142e-01 ## 3rd Qu. 4.672541e-10 1.000000e+00 ## Max. 1.000000e+00 1.000000e+00 # blended plot(density(train_blended_oofProb_vec[train_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(train_blended_oofProb_vec[train_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;blended&quot;) Figure 4.4: Train data out-of-fold predicted probabilities This makes no sense. Look at test data ROC. # train # lasso test_lasso_oofProb_vec &lt;- predict( cv_lasso, type=&#39;resp&#39;, lambda=&#39;1se&#39;, newx=test_lcpm_mtx ) test_lasso_roc &lt;- pROC::roc( response = as.numeric(test_group_vec==&#39;HCC&#39;), predictor = test_lasso_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Warning in roc.default(response = as.numeric(test_group_vec == &quot;HCC&quot;), predictor ## = test_lasso_oofProb_vec): Deprecated use a matrix as predictor. Unexpected ## results may be produced, please pass a numeric vector. ## Setting direction: controls &lt; cases # lassoR test_lassoR_oofProb_vec &lt;- predict( cv_lassoR, type=&#39;resp&#39;, lambda=&#39;1se&#39;, newx=test_lcpm_mtx, gamma=0, ) test_lassoR_roc &lt;- pROC::roc( response = as.numeric(test_group_vec==&#39;HCC&#39;), predictor = test_lassoR_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Warning in roc.default(response = as.numeric(test_group_vec == &quot;HCC&quot;), predictor ## = test_lassoR_oofProb_vec): Deprecated use a matrix as predictor. Unexpected ## results may be produced, please pass a numeric vector. ## Setting direction: controls &lt; cases # blended mix (gamma=0.5) test_blended_oofProb_vec &lt;- predict( cv_lassoR, type=&#39;resp&#39;, lambda=&#39;1se&#39;, newx=test_lcpm_mtx, gamma=0.5, ) test_blended_roc &lt;- pROC::roc( response = as.numeric(test_group_vec==&#39;HCC&#39;), predictor = test_blended_oofProb_vec) ## Setting levels: control = 0, case = 1 ## Warning in roc.default(response = as.numeric(test_group_vec == &quot;HCC&quot;), predictor ## = test_blended_oofProb_vec): Deprecated use a matrix as predictor. Unexpected ## results may be produced, please pass a numeric vector. ## Setting direction: controls &lt; cases plot(test_lasso_roc) lines(test_lassoR_roc, col=&#39;blue&#39;) lines(test_blended_roc, col=&#39;green&#39;) legend(&#39;bottomright&#39;, title=&#39;AUC&#39;, legend=c( paste(&#39;lasso =&#39;, round(test_lasso_roc[[&#39;auc&#39;]],3)), paste(&#39;lassoR =&#39;, round(test_lassoR_roc[[&#39;auc&#39;]],3)), paste(&#39;blended =&#39;, round(test_blended_roc[[&#39;auc&#39;]],3)) ), text.col = c(&#39;black&#39;, &#39;blue&#39;, &#39;green&#39;)) Figure 4.5: Test data out-of-sample ROCs Look at desnities of predicted probabilities. par(mfrow=c(1,3)) # lasso plot(density(test_lasso_oofProb_vec[test_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(test_lasso_oofProb_vec[test_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;lasso&quot;) # lassoR plot(density(test_lassoR_oofProb_vec[test_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(test_lassoR_oofProb_vec[test_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;lassoR&quot;) sapply(split(test_lassoR_oofProb_vec,test_group_vec), summary) ## Control HCC ## Min. 2.352635e-09 4.183949e-05 ## 1st Qu. 4.850862e-04 8.373417e-01 ## Median 3.050687e-03 9.925541e-01 ## Mean 9.277576e-02 8.424789e-01 ## 3rd Qu. 3.704167e-02 9.999439e-01 ## Max. 9.999883e-01 1.000000e+00 # blended plot(density(test_blended_oofProb_vec[test_group_vec==&#39;Control&#39;]), xlim=c(0,1),main=&#39;&#39;, xlab=&#39;&#39;, col=&#39;green&#39;) lines(density(test_blended_oofProb_vec[test_group_vec==&#39;HCC&#39;]), co=&#39;red&#39;) title(&quot;blended&quot;) Figure 4.6: Test data out-of-fold predicted probabilities # Train - preval is out-of-fold linear predictor for training design points onese_ndx &lt;- match(cv_lasso$lambda.1se,cv_lasso$lambda) train_1se_preval_vec &lt;- cv_lasso$fit.preval[,onese_ndx] train_1se_predProb_vec &lt;- logistic_f(train_1se_preval_vec) #Test test_1se_predProb_vec &lt;- predict( cv_lasso, newx=test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&#39;resp&#39; ) tmp &lt;- c( train=split(train_1se_predProb_vec, train_group_vec), test=split(test_1se_predProb_vec, test_group_vec)) names(tmp) &lt;- sub(&#39;\\\\.&#39;,&#39;\\t&#39;,names(tmp)) boxplot(tmp) Figure 4.7: Predicted Probabilities - Train and Test Table 4.6: cv lasso confusion matrix: train set Control HCC Control 615 31 HCC 8 413 Table 4.7: cv lassoR confusion matrix: train set Control HCC Control 607 34 HCC 16 410 Table 4.8: cv lasso confusion matrix: test set Control HCC Control 148 22 HCC 7 89 Table 4.9: cv lassoR confusion matrix: test set Control HCC Control 146 17 HCC 9 94 4.5 Refit with “auc” as optimization start_time &lt;- proc.time() cv_lasso2 &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=1, family=&#39;binomial&#39;, type.measure = &quot;auc&quot;) message(&quot;lasso time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lasso time: 23.89s start_time &lt;- proc.time() cv_ridge2 &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0, family=&#39;binomial&#39;, type.measure = &quot;auc&quot;) message(&quot;ridge time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## ridge time: 266.7s start_time &lt;- proc.time() cv_enet2 &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=0.5, family=&#39;binomial&#39;, type.measure = &quot;auc&quot;) message(&quot;enet time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## enet time: 23.54s start_time &lt;- proc.time() cv_lassoC2 &lt;- glmnet::cv.glmnet( x=train_lcpm_mtx, y=train_group_vec, foldid=train_foldid_vec, alpha=1-EPS, family=&#39;binomial&#39;, type.measure = &quot;class&quot;) message(&quot;lassoC time: &quot;, round((proc.time() - start_time)[3],2),&quot;s&quot;) ## lassoC time: 21.92s The ridge regression model takes over 10 times longer to compute. Examine model performance. ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases Figure 4.8: compare fits All models produce cv assessments of Misclassification Error that are slightly better than the test set assessments. lasso performs comparably to enet and better than the ridge model. 4.6 Relaxed lasso and blended mix library(glmnet) cv_lassoR2_sum &lt;- print(cv_lassoR2) ## ## Call: glmnet::cv.glmnet(x = train_lcpm_mtx, y = train_group_vec, type.measure = &quot;auc&quot;, foldid = train_foldid_vec, relax = T, alpha = 1, family = &quot;binomial&quot;) ## ## Measure: AUC ## ## Gamma Lambda Measure SE Nonzero ## min 0.25 0.04153 0.9765 0.003813 32 ## 1se 0.50 0.04558 0.9729 0.003849 31 plot(cv_lassoR2) Figure 4.9: lassoR fit test_pred_1se_vec &lt;- predict( cv_lassoR2, newx=test_lcpm_mtx, s=&quot;lambda.1se&quot;, type=&quot;response&quot; ) test_pred_1se_auc &lt;- suppressWarnings(pROC::auc(test_group_vec,test_pred_1se_vec)[1]) ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases test_pred_min_vec &lt;- predict( cv_lassoR2, newx=test_lcpm_mtx, s=&quot;lambda.min&quot;, type=&quot;response&quot; ) test_pred_min_auc &lt;- suppressWarnings(pROC::auc(test_group_vec,test_pred_min_vec)[1]) ## Setting levels: control = Control, case = HCC ## Setting direction: controls &lt; cases cv_lassoR2_1se_auc &lt;- cv_lassoR2_sum[&#39;1se&#39;,&#39;Measure&#39;] cv_lassoR2_min_auc &lt;- cv_lassoR2_sum[&#39;min&#39;,&#39;Measure&#39;] knitr::kable(rbind( onese=c(cv_eucg=cv_lassoR2_1se_auc, test_auc=test_pred_1se_auc)*100, min=c(cv_eucg=cv_lassoR2_min_auc, test_auc=test_pred_min_auc)*100 ), caption=&quot;CV vs test Errors&quot;, digits=1 ) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.10: CV vs test Errors cv_eucg test_auc onese 97.3 95.8 min 97.7 95.6 Look at confusion matrices Table 4.11: cv lasso confusion matrix: train set Control HCC Control 614 48 HCC 9 396 Table 4.12: cv lassoR2 confusion matrix: train set Control HCC Control 608 39 HCC 15 405 Table 4.13: cv lasso confusion matrix: test set Control HCC Control 148 25 HCC 7 86 Table 4.14: cv lassoR2 confusion matrix: test set Control HCC Control 147 19 HCC 8 92 In all models the sensitivity weak compared to the specificity. Let’s examine the ROC curves to see where the trade-off is. "],
["examine-feature-selection.html", "Section 5 Examine feature selection 5.1 Sparsity stability", " Section 5 Examine feature selection Recall It is known that the ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one of them and discard the others. The elastic-net penalty mixes these two; if predictors are correlated in groups, an $\\alpha$=0.5 tends to select the groups in or out together. This is a higher level parameter, and users might pick a value upfront, else experiment with a few different values. One use of $\\alpha$ is for numerical stability; for example, the *elastic net with $\\alpha = 1 - \\epsilon$ for some small $\\epsilon$&gt;0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations*. 5.1 Sparsity stability "],
["model-suite.html", "Section 6 Fitted Model Suite", " Section 6 Fitted Model Suite We examine the results of fitting a suite of models to investigate the effect of sample size on model performance. Predicted probabilities can be transformed into a sample quality score: \\(Q_i = p_i^y_i(1-p_i)^{1-y_i}\\), where \\(p_i\\) is the out-of-fold estimated probability of HCC for sample i and \\(y_i\\) is 1 for HCC samples and 0 for Controls. ie. we use the fitted likelihood as a sample quality score. The quality scores derived from a particular cv run will depend to some extent on the random assignment of samples to folds. To remove this dependency, we can derive quality scores by averaging over many cv runs, 30 say. Hard to classify samples will have low quality scores. In the results that we discuss below, when we look at variability across repeated random sampling of different sizes, we can use sample quality scores to investigate how much of the variability is due to sample selection. Note that quality here is not used to say anything about the sample data quality. Low quality here only means that a sample is different from the core of the data set in a way that makes it hard to properly classify. That could happen if the sample were mislabeled, in which case we could think of this sample as being poor quality of course. "],
["conclusions.html", "Section 7 Conclusions", " Section 7 Conclusions We have found that … Other questions … "],
["references.html", "References", " References "]
]
