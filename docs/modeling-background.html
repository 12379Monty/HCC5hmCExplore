<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Section 2 Classification Analysis in the n &lt;&lt; p Context | DNA Hydroxymethylation in Hepatocellular Carcinoma</title>
  <meta name="description" content="Data from Cai et al. (2019) paper are explored" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Section 2 Classification Analysis in the n &lt;&lt; p Context | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Data from Cai et al. (2019) paper are explored" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Section 2 Classification Analysis in the n &lt;&lt; p Context | DNA Hydroxymethylation in Hepatocellular Carcinoma" />
  
  <meta name="twitter:description" content="Data from Cai et al. (2019) paper are explored" />
  

<meta name="author" content="Francois Collin" />


<meta name="date" content="2020-09-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="preproc.html"/>
<script src="libs/header-attrs-2.2/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script type="text/javascript">
// source:https://stackoverflow.com/questions/45360998/code-folding-in-bookdown
// toggle visibility of R source blocks in R Markdown output
function toggle_R() {
  var x = document.getElementsByClassName('r');
  if (x.length == 0) return;
  function toggle_vis(o) {
    var d = o.style.display;
    o.style.display = (d == 'block' || d == '') ? 'none':'block';
  }

  for (i = 0; i < x.length; i++) {
    var y = x[i];
    if (y.tagName.toLowerCase() === 'pre') toggle_vis(y);
  }

    var elem = document.getElementById("myButton1");
    if (elem.value === "Hide Global") elem.value = "Show Global";
    else elem.value = "Hide Global";
}

document.write('<input onclick="toggle_R();" type="button" value="Hide Global" id="myButton1" style="position: absolute; top: 10%; right: 2%; z-index: 200"></input>')

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preamble</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="modeling-background.html"><a href="modeling-background.html"><i class="fa fa-check"></i><b>2</b> Classification Analysis in the <code>n &lt;&lt; p</code> Context</a>
<ul>
<li class="chapter" data-level="2.1" data-path="modeling-background.html"><a href="modeling-background.html#caret-for-model-evaluation"><i class="fa fa-check"></i><b>2.1</b> caret for model evaluation</a></li>
<li class="chapter" data-level="2.2" data-path="modeling-background.html"><a href="modeling-background.html#the-glmnet-r-package"><i class="fa fa-check"></i><b>2.2</b> The <code>glmnet</code> R package</a>
<ul>
<li><a href="modeling-background.html#alpha-hyper-parameter"><strong>alpha</strong> hyper-parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="modeling-background.html"><a href="modeling-background.html#signal-to-noise"><i class="fa fa-check"></i><b>2.3</b> Signal-to-noise Ratio</a></li>
<li class="chapter" data-level="2.4" data-path="modeling-background.html"><a href="modeling-background.html#lasso-vs-best-sub"><i class="fa fa-check"></i><b>2.4</b> Lasso vs Best Subset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="preproc.html"><a href="preproc.html"><i class="fa fa-check"></i><b>3</b> 5hmc Data Preprocessing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="preproc.html"><a href="preproc.html#load-the-data"><i class="fa fa-check"></i><b>3.1</b> Load the data</a></li>
<li class="chapter" data-level="3.2" data-path="preproc.html"><a href="preproc.html#dra"><i class="fa fa-check"></i><b>3.2</b> Differential representation analysis</a>
<ul>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#remove-lowly-expressed-genes"><i class="fa fa-check"></i>Remove lowly expressed genes</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#creating-a-design-matrix-and-contrasts"><i class="fa fa-check"></i>Creating a design matrix and contrasts</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#removing-heteroscedasticity-from-the-count-data"><i class="fa fa-check"></i>Removing heteroscedasticity from the count data</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#fit-linear-models-and-examine-the-results"><i class="fa fa-check"></i>Fit linear models and examine the results</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#graphical-representations-of-de-results-md-plots"><i class="fa fa-check"></i>Graphical representations of DE results: MD Plots</a></li>
<li class="chapter" data-level="" data-path="preproc.html"><a href="preproc.html#de-genes-at-10-fold-change"><i class="fa fa-check"></i>DE genes at 10% fold change</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="preproc.html"><a href="preproc.html#snr-regime"><i class="fa fa-check"></i><b>3.3</b> Signal-to-noise ratio regime</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="explore-sparsity.html"><a href="explore-sparsity.html"><i class="fa fa-check"></i><b>4</b> The bet on sparsity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#cross-validation-analysis-setup"><i class="fa fa-check"></i><b>4.1</b> Cross-validation analysis setup</a></li>
<li class="chapter" data-level="4.2" data-path="explore-sparsity.html"><a href="explore-sparsity.html#fit-and-compare-models"><i class="fa fa-check"></i><b>4.2</b> Fit and compare models</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#logistic-regression-in-glmnet"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression in <code>glmnet</code></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="explore-sparsity.html"><a href="explore-sparsity.html#the-relaxed-lasso-and-blended-mix-models"><i class="fa fa-check"></i><b>4.3</b> The relaxed lasso and blended mix models</a></li>
<li class="chapter" data-level="4.4" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examination-of-sensitivity-vs-specificity"><i class="fa fa-check"></i><b>4.4</b> Examination of sensitivity vs specificity</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="explore-sparsity.html"><a href="explore-sparsity.html#training-data-out-of-fold-roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> Training data out-of-fold ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-predictions-at-misclassified-samples"><i class="fa fa-check"></i><b>4.5</b> Compare predictions at misclassified samples</a></li>
<li class="chapter" data-level="4.6" data-path="explore-sparsity.html"><a href="explore-sparsity.html#compare-coefficient-profiles"><i class="fa fa-check"></i><b>4.6</b> Compare coefficient profiles</a></li>
<li class="chapter" data-level="4.7" data-path="explore-sparsity.html"><a href="explore-sparsity.html#examine-feature-selection"><i class="fa fa-check"></i><b>4.7</b> Examine feature selection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-suite.html"><a href="model-suite.html"><i class="fa fa-check"></i><b>5</b> Does Size Matter?</a>
<ul>
<li class="chapter" data-level="5.1" data-path="model-suite.html"><a href="model-suite.html#sample-quality-scores"><i class="fa fa-check"></i><b>5.1</b> Sample quality scores</a>
<ul>
<li class="chapter" data-level="" data-path="model-suite.html"><a href="model-suite.html#get-quality-scores"><i class="fa fa-check"></i>Get quality scores</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="model-suite.html"><a href="model-suite.html#selected-feature-list-stability"><i class="fa fa-check"></i><b>5.2</b> Selected feature list stability</a></li>
<li class="chapter" data-level="5.3" data-path="model-suite.html"><a href="model-suite.html#simulation-design"><i class="fa fa-check"></i><b>5.3</b> Simulation Design</a></li>
<li class="chapter" data-level="5.4" data-path="model-suite.html"><a href="model-suite.html#setup-simulation"><i class="fa fa-check"></i><b>5.4</b> Setup simulation</a></li>
<li class="chapter" data-level="5.5" data-path="model-suite.html"><a href="model-suite.html#run-simulations"><i class="fa fa-check"></i><b>5.5</b> Run simulations</a></li>
<li class="chapter" data-level="5.6" data-path="model-suite.html"><a href="model-suite.html#simulation-results"><i class="fa fa-check"></i><b>5.6</b> Simulation results</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>6</b> Variable importance</a></li>
<li class="chapter" data-level="7" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>7</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="LICENSE.md" style="font:em;" target="blank">Copyright (c) 2020 Francois Collin</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DNA Hydroxymethylation in Hepatocellular Carcinoma</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-background" class="section level1" number="2">
<h1><span class="header-section-number">Section 2</span> Classification Analysis in the <code>n &lt;&lt; p</code> Context</h1>
<p>Refer to <a href="https://hcc-5hmc-analysis.netlify.app/">first pass study</a> for
relevant exploratory data analyis results.</p>
<!--
In the section we look at  some models fitted to discriminate between
early stage HCC and healthy and benign samples (grouped as Controls here)
from the GSE112679 data set.  


* Some questions to address with the baseline model 
   - how separable are the data: what accuracy do we expect 
   - individual sample quality scores: which samples are hard to classify?  Compute a score
in [0, 1], where 1 is perfectly good classification and 0 is perfectly bad.

-->
<p>The main challenge in calibrating predictive models to genomic data is that
there are many more features than there are example cases to fit to;
the now classic <span class="math inline">\(n &lt;&lt; p\)</span> problem.
In this scenario, fitting methods tend to over fit. The problem
can be addressed by selecting variables, regularizing the fit or both.</p>
<!--
See the Trevor Hastie talk: 
[Statistical Learning with Big Data - Trevor Hastie](https://web.stanford.edu/~hastie/TALKS/SLBD_new.pdf)
for a good discussion of this problem and potential solutions.
-->
<p>In this report, we use genomic data to illustrate analyses of regularized regression
models in the <code>n &lt;&lt; p</code> context. Much of the analysis focuses on lasso models fitted
and analyzed using tools from the <code>glmnet</code> R package.</p>
<p>Before providing some background on that glmnet package and the
models that it supports, we briefly describe another R package
which is an essential tool for anyone who is doing predicitive analytics in R - <code>caret</code> R pacakge.</p>
<div id="caret-for-model-evaluation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> caret for model evaluation</h2>
<p><a href="https://topepo.github.io/caret/index.html">The <code>caret</code> Package</a>
provides a rich set of functions that streamline the process for fitting and
evaluating a large number of predictive models in parallel. The package contains tools for:</p>
<ul>
<li>data splitting<br />
</li>
<li>pre-processing<br />
</li>
<li>feature selection<br />
</li>
<li>model tuning using re-sampling<br />
</li>
<li>variable importance estimation</li>
</ul>
<p>The tools facilitate the process of automating randomly splitting data sets into training,
testing and evaluating so that predictive models can be evaluated on a comparable and
exhaustive basis. Especially useful is the functionality that is provided to
repeatedly randomly stratify samples into train and test set so that any
sample selection bias is removed.</p>
<p>What makes the <code>caret</code> package extremely useful is that it provides a common interface
to an exhaustive collection of fitting procedures. Without
this common interface one has to learn the specific syntax that
used in each fitting procedure to be included in a comparative analysis,
which can be quite burdensome.</p>
<p>Some of the models which can be evaluated with caret include:
(only some of these can be used with multinomial responses)</p>
<ul>
<li>FDA - Flexible Discriminant Analysis<br />
</li>
<li>stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection<br />
</li>
<li>stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection<br />
</li>
<li>knn - k nearest neighbors<br />
</li>
<li>pam - Nearest shrunken centroids<br />
</li>
<li>rf - Random forests<br />
</li>
<li>svmRadial - Support vector machines (RBF kernel)<br />
</li>
<li>gbm - Boosted trees<br />
</li>
<li>xgbLinear - eXtreme Gradient Boosting<br />
</li>
<li>xgbTree - eXtreme Gradient Boosting<br />
</li>
<li>neuralnet - neural network</li>
</ul>
<p>Many more models can be implemented and evaluated with <code>caret</code>,
including some <code>deep learning</code> methods, <code>Simulated Annealing Feature Selection</code>
and <code>Genetic Algorithms</code>.
Many other methods found <a href="https://topepo.github.io/caret/available-models.html">here</a>
are also worth investigating.</p>
<p>We mention <code>caret</code> here because it is an extremely useful tool for
anyone interested in exploring and comparing the performance of
many predictive models which could be applied to a given predictive
analysis context. We have gone through this exercise with a number
of genomic scale, RNA-Seq data sets in the past and have found
that in this context regularized regression models perform
as well as any. For this report, we will only consider regularized classification
models and will focus on the particular set of tools for fitting and analyzing
these models provided by the <code>glmnet</code> R package <span class="citation">[<a href="references.html#ref-Friedman:2010aa" role="doc-biblioref">6</a>]</span>.</p>
</div>
<div id="the-glmnet-r-package" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> The <code>glmnet</code> R package</h2>
<p>Several factors favor using the <code>glmnet</code> R package to analyze
classification models fitted to genomic scale data:</p>
<ul>
<li><p>the glmnet package is a well supported package providing
extensive functionality for regularized regression and classification models.</p></li>
<li><p>the hyper-parameters of the elastic net enable us to explore
the relationship between model size, or sparsity, and predictive accuracy.
ie. we can investigate the “bet on sparsity” principle:
<em>Use a procedure that does well in sparse problems, since no procedure
does well in dense problems</em>.</p></li>
<li><p>in our experience building classifiers from genomic scale data, regularized
classification models using the elastic net penalty do as well as any other,
and are more economical in terms of computing time, especially in comparison to
the more exotic boosting algorithms.</p></li>
<li><p>the <code>lasso</code> has been shown to be near optimal for the <span class="math inline">\(n&lt;&lt;p\)</span> problem
over a wide range of signal-to-noise regiments (Hastie et al. (2017) <span class="citation">[<a href="references.html#ref-Hastie:2017aa" role="doc-biblioref">7</a>]</span>).</p></li>
</ul>
<hr />
<p>Much of the following comes from the
<a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">Glmnet Vignette</a>.</p>
<p>Glmnet is a package that fits a generalized linear model via penalized maximum likelihood.
The regularization path is computed for the lasso or elastic net penalty at a
grid of values for the regularization parameter lambda
(<span class="citation">[<a href="references.html#ref-Friedman:2010aa" role="doc-biblioref">6</a>,<a href="references.html#ref-Tibshirani:2012aa" role="doc-biblioref">8</a>–<a href="references.html#ref-Simon:2013aa" role="doc-biblioref">10</a>]</span>).</p>
<p><code>glmnet</code> solves the following problem:</p>
<p><span class="math display">\[\min_{\beta_0,\beta} \frac{1}{N} \sum_{i=1}^{N} w_i l(y_i,\beta_0+\beta^T x_i) + \lambda\left[(1-\alpha)||\beta||_2^2/2 + \alpha ||\beta||_1\right],\]</span></p>
<p>over a grid of values of <span class="math inline">\(\lambda\)</span>.
Here <span class="math inline">\(l(y,\eta)\)</span> is the negative log-likelihood contribution for observation i;
e.g. for the Gaussian case it is <span class="math inline">\(\frac{1}{2}(y-\eta)^2\)</span>.</p>
<div id="alpha-hyper-parameter" class="section level3 unnumbered" number="">
<h3><strong>alpha</strong> hyper-parameter</h3>
<p>The elastic-net penalty is controlled by <span class="math inline">\(\alpha\)</span>, and bridges the gap between
lasso (<span class="math inline">\(\alpha\)</span>=1, the default) and ridge (<span class="math inline">\(\alpha\)</span>=0).
The tuning parameter <span class="math inline">\(\lambda\)</span> controls the overall strength of the penalty.</p>
<p>It is known that the ridge penalty shrinks the coefficients of correlated predictors
towards each other while the lasso tends to pick one of them and discard the others.
The elastic-net penalty mixes these two; if predictors are correlated in groups,
an <span class="math inline">\(\alpha\)</span>=0.5 tends to select the groups in or out together.
This is a higher level parameter, and users might pick a value upfront,
else experiment with a few different values. One use of <span class="math inline">\(\alpha\)</span> is for numerical stability;
for example, the <em>elastic net with <span class="math inline">\(\alpha = 1 - \epsilon\)</span> for some small <span class="math inline">\(\epsilon\)</span>&gt;0
performs much like the lasso, but removes any degeneracies and wild behavior caused
by extreme correlations</em>.</p>
</div>
</div>
<div id="signal-to-noise" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Signal-to-noise Ratio</h2>
<p>A key characteristic of classification problems is the
prevailing signal-to-noise ratio (SNR) of the problem at hand.</p>
<p>To define SNR, let <span class="math inline">\((x_0, y_0) \in \mathbb{R}^p \times \mathbb{R}\)</span>
be a pair of predcitor and response variables and define
<span class="math inline">\(f(x_0) = \mathbb{E}(y_0|x_0)\)</span> and <span class="math inline">\(\epsilon = y_0 - f(x_0)\)</span> so that</p>
<p><span class="math display">\[y_0 = f(x_0) + \epsilon_0.\]</span></p>
<p>The signal-to-noise ratio in this model is defined as</p>
<p><span class="math display">\[SNR=\frac{var(f(x_0))}{var(\epsilon_0)}.\]</span></p>
<p>It is useful to relate the SNR of a model to the proportion of variance explained (PVE).
For a given prediction function g — eg. one trained on n samples
<span class="math inline">\((x_i, y_i) i = 1, \dots, n\)</span> that are i.i.d. to <span class="math inline">\((x_0, y_0)\)</span> — its
associated proportion of variance explained is defined as</p>
<p><span class="math display">\[PVE(g)=1 - \frac{\mathbb{E}(y_0-g(x_0))^2}{Var(y_0)}.\]</span></p>
<p>This is maximized when we take <span class="math inline">\(g\)</span> to be the mean function <span class="math inline">\(f\)</span> itself,
in which case</p>
<p><span class="math display">\[PVE(f) = 1 - \frac{Var(\epsilon_0)}{Var(y_0)} = \frac{SNR}{1+SNR}.\]</span></p>
<p>Or equivalently,</p>
<p><span class="math display">\[SNR = \frac{PVE}{1-PVE}.\]</span></p>
<p>Hastie, Tibshirani, and Tibshirani (2017) <span class="citation">[<a href="references.html#ref-Hastie:2017aa" role="doc-biblioref">7</a>]</span>, point out that
PVE is typically in the 0.2 range, and much lower in financial data. It
is also much lower in 5hmC data, as we will see in the next section.</p>
<p>Note that the SNR is a different characterization of noise level than the
coefficient of variation:</p>
<p><span class="math display">\[c_v = \frac{\sigma}{\mu}=\frac{Var(y)}{\mathbb{E}(y)}\]</span></p>
<p>Note that for small SNR, SNR <span class="math inline">\(\approx\)</span> PVE.</p>
<p>See Xiang et al. (2020) <span class="citation">[<a href="references.html#ref-Xiang:2020aa" role="doc-biblioref">11</a>]</span>, Lozoya et al. (2018) <span class="citation">[<a href="references.html#ref-Lozoya:2018aa" role="doc-biblioref">12</a>]</span>,
Simonson et al. (2018) <span class="citation">[<a href="references.html#ref-Simonsen:2018aa" role="doc-biblioref">13</a>]</span> and
Rapaport et al. (2013) <span class="citation">[<a href="references.html#ref-Rapaport:2013aa" role="doc-biblioref">14</a>]</span> for SNR in RNA-Seq</p>
</div>
<div id="lasso-vs-best-sub" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Lasso vs Best Subset</h2>
<p>Best subset selection finds the subset of k predictors that
produces the best fit in terms of squared error, solving the nonconvex problem:</p>
<p><span class="math display" id="eq:bestSub">\[\begin{equation}
 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_0 \leq k
 \tag{2.1}
\end{equation}\]</span></p>
<p>The lasso solves a covex relaxation of the above where we replace the
<span class="math inline">\(l_0\)</span> norm by the <span class="math inline">\(l_1\)</span> norm, namely</p>
<p><span class="math display" id="eq:lasso">\[\begin{equation}

 \min_{\beta \in \mathcal{R}^p} ||Y - X\beta||^2_2 \, \, subject \, to \, \, ||\beta||_1 \leq t

 \tag{2.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(||\beta||_1 = \sum_{i=1}^{p} |\beta_i|\)</span>, and <span class="math inline">\(t \geq 0\)</span> is a tuning parameter.</p>
<p>Bertsimas et al. (2016) <span class="citation">[<a href="references.html#ref-Bertsimas:2016aa" role="doc-biblioref">15</a>]</span> presented a mixed integer optimization (MIO)
formulation for the best subset selection problem. Using these MIO solvers,
one can solve problems with p in the hundreds and even thousands.
Bertsimas et al. showed evidence that
best subset selection generally gives superior prediction accuracy compared
to forward stepwise selection and the lasso, over a variety of problem setups.</p>
<p>In Hastie et al. (2017) <span class="citation">[<a href="references.html#ref-Hastie:2017aa" role="doc-biblioref">7</a>]</span>, the authors countered by arguing that
neither best subset selection nor the lasso uniformly dominate the other,
with best subset selection generally performing better in high signal-to-noise (SNR)
ratio regimes, and <strong>the lasso better in low SNR regimes</strong>.
Best subset selection and forward stepwise perform quite similarly over
a range of SNR contexts, but the relaxed lasso is the overall best option,
performing just about as well as the lasso in low SNR scenarios,
and as well as best subset selection in high SNR scenarios.
Hastie et al. conclude that a blended mix of lasso and relaxed lasso estimator,
the <em>shrunken relaxed lasso fit</em>, is able to use its auxiliary shrinkage
parameter (<span class="math inline">\(\gamma\)</span>) to get the “best of both worlds”:
it accepts the heavy shrinkage from the lasso when such shrinkage is helpful, and reverses it when it is not.</p>
<!--
* relaxed lasso

$$\hat{\beta}^{relax}(\lambda, \gamma) = \gamma \beta^{lasso}(\lambda) + (1 - \gamma)(\beta^{LS}(\lambda)$$

* shrunken relaxed lasso (aka the blended fit)
-->
<p>Suppose the <strong>glmnet</strong> fitted linear predictor at <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\eta}_\lambda(x)\)</span>
and the relaxed version is <span class="math inline">\(\tilde{\eta}_\lambda(x)\)</span>, then the shrunken relaxed lasso fit is</p>
<p><span class="math display" id="eq:blended">\[\begin{equation}

\tilde{\eta}_{\lambda,\gamma}(x)=(1-\gamma)\tilde{\eta}_\lambda(x) + \gamma \hat{\eta}_\lambda(x)

 \tag{2.3}
\end{equation}\]</span></p>
<p><span class="math inline">\(\gamma \in [0,\, 1]\)</span> is an additional tuning parameter which can be selected by cross validation.</p>
<p>The de-biasing will potentially improve prediction performance, and
cross-validation will typically select a model with a smaller number of variables.
This procedure is very competitive with forward-stepwise and
best-subset regression, and has a considerable speed advantage when the
number of variables is large. This is especially true for best-subset,
but even so for forward stepwise. The latter has to plod through the
variables one-at-a-time, while glmnet will just plunge in and find a good active set.</p>
<p>Further details may be found in
Friedman, Hastie, and Tibshirani (2010),
Tibshirani et al. (2012),
Simon et al. (2011),
Simon, Friedman, and Hastie (2013) and
Hastie, Tibshirani, and Tibshirani (2017)
(<span class="citation">[<a href="references.html#ref-Friedman:2010aa" role="doc-biblioref">6</a>–<a href="references.html#ref-Simon:2013aa" role="doc-biblioref">10</a>]</span>).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="preproc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["HCC5hmCExplore.pdf", "HCC5hmCExplore.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
