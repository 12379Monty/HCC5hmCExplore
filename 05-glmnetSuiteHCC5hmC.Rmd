# HCC 5hmC-Seq: Sample size investigation {#hcc-5hmcseq-model-suite}

We now examine the results of fitting a suite of lasso models to
investigate the effect of sample size on 
various aspects of model performance:  

* assessed accuracy: out-of-fold estimates of precision and variability vs train set estimates 

* selected feature profile stability - to what extent does the
feature set implicitly selected by the lasso vary across random
sampling and what is the effect of sample size.

It is hypothesized that below a certain threshold,
sample sizes are too small to provide reliable estimates
of performance or stable selected feature profiles.  

**Reader Note**
```
It's not clear how to display the effect of sample set composition
on model performance, or that it merits special attention as it
may be obvious to anyone who is paying attention that not all
sample points are created equal and that having a substantial
number of mislabeled or otherwise hard to classify samples
can throw off estimates of the separability of subgroups
when sample sizes are small.  

Keep this stuff on consistency for now.
```

In examining the relationship between sample size and
model performance, including variability and reliability
of performance indicators derived from fits, we should bear in
mind that in addition to sample size, sample composition might 
also play a factor.  If a training data set contains many outliers
or otherwise *hard to properly classify* samples, the performance
of models fitted to such a data set is expected to be negatively impacted.

In the simulations that we run below we will track sample *consistency*
to examine its impact on fitted model performance.  We use 
*consistency* to refer to a measure of how hard it is to
appropriately classify individual samples.
Predicted probabilities from fitted model can be transformed into sample
*consistency* scores: $Q_i = p_i^{y_i}(1-p_i)^{1-y_i}$, where $p_i$ is the
estimated probability of HCC for sample i and $y_i$ is 1 for HCC samples and
0 for Controls.  ie. we use the fitted sample contribution to the
likelihood function as a sample *consistency* score,
which is $P(HCC)$, the predicted probability of HCC for HCC samples
and $1 - P(HCC)$ for Controls.  To derive the sample *consistency* scores,
we will use the predicted response from a lasso model fitted to the entire data set.

Hard to classify samples will have low *consistency* scores.
In the results that we discuss below, when we look at variability across repeated 
random sampling of different sizes, we can use sample *consistency* scores to investigate 
how much of the variability is due to sample selection.
Note that *consistency* here is not used to say anything about the sample data quality.
Low *consistency* here only means that a sample is different from the 
core of the data set in a way that makes it hard to properly classify.
That could happen if the sample were mislabeled, in which case we could 
think of this sample as being poor quality of course.

The variability in sample set *consistency* that we get from simple random sampling,
as is done in the simulation below, is not expected to adequately reflect
sample set *consistency* variation encountered in practice when data for a particular
study are accumulated over extensive time horizons and across varied
physical settings.  In order to accrue study samples at an acceptable rate,
it is not uncommon for the study sponsor to work with several clinics, medical centers,
or other tissue or blood sample provider.
Or the sponsor may sub-contract the sample acquisition
task to a third party who may have suppliers distributed across the globe.  This
is great news for the rate of sample acquisition, but not such great news for 
the uniformity of consistency of samples.  In such a context, the variability
in sample set consistency as the data set grows over time would not
be adequately captured by simple random sampling variability;
the variability would be more akin to cluster sampling with potential confounding
batch effects.  The impact that these effects can have on the classification analysis
results cannot be understated, especially in the context of a new technology that
is exploiting biological processes that are still not fully understood.  All this
to make the point that the variability the we are studying here has to be regarded 
as a lower bound on the variability that is to be expected in practice, and that without
an external validation data set to verify results, one should be cautious
when interpreting empirical findings, especially in the absence of solid
biological underpinnings.  

**Reader Note** 
``` 
We still need to demonstrate the effect of sample set composition
on results.  With random sampling the effects are subtle and not
immediately obvious.  We may have to fabricate sample sets with 
low consistency scores - ie. made up of many hard to classify samples -
in order to show this impact.
```
## Full data set fit

We begin by fitting a model to the entire data set in order to:

* obtain a baseline classification performance against which to judge the performance
obtained from the fits to smaller sample sets,

* obtain sample consistency scores which can be used to explain variability
in the performance of model fitted to data sets of a fixed size, and

* produce a *full model* gene signature which can be used to evaluate
the stability of selected features in models fitted to data sets of different
sizes.


First assemble the data set.  This entails simply re-combining the
train and test data.

```{r hcc5hmC-glmnetSuite-get-all-data, cache=T, cache.vars=c('hcc5hmC_all_lcpm_mtx', 'hcc5hmC_all_group_vec')}

# combine train and test 
hcc5hmC_all_lcpm_mtx <- rbind(hcc5hmC_train_lcpm_mtx, hcc5hmC_test_lcpm_mtx)

# we have to be careful with factors!
# We'll keep as a character and change to factor when needed
hcc5hmC_all_group_vec <- c(
 as.character(hcc5hmC_train_group_vec), 
 as.character(hcc5hmC_test_group_vec)
)
# I suspect adding names to vectors breaks one of the tidy commandments,
# but then again I am sure I have already offended the creed beyond salvation
names(hcc5hmC_all_group_vec) <- c(
 names(hcc5hmC_train_group_vec),
 names(hcc5hmC_test_group_vec)
)

knitr::kable(table(group = hcc5hmC_all_group_vec),
  caption = "samples by group") %>%
   kableExtra::kable_styling(full_width = F)

```

Now fit the lasso model through cross-validation.
Note that the results of a cv fit are random due to the
random allocation of samples to folds.  We can reduce this
variability by properly averaging results over repeated cv fits.
Here we will obtain sample consistency scores by averaging results
over 30 cv runs.


```{r hcc5hmC-glmnetSuite-lasso-fit-all, cache=T, cache.vars=c('hcc5hmC_cv_lassoAll_lst'), eval=F}

set.seed(1)

start_time <-  proc.time()

hcc5hmC_cv_lassoAll_lst <- lapply(1:30, function(REP) {
glmnet::cv.glmnet(
 x = hcc5hmC_all_lcpm_mtx,
 y = factor(hcc5hmC_all_group_vec,levels = c('Control', 'HCC')),
 alpha = 1,
 family = 'binomial',
 type.measure  =  "class",
 keep = T,
 nlambda = 100
)
}
)

message("lassoAll time: ", round((proc.time() - start_time)[3],2),"s")

```

<!-- lasso-fit-all takes a while - save results -->
<!-- DO THIS ONCE -->
```{r hcc5hmC-glmnetSuite-save-hcc5hmC_cv_lassoAll_lst, cache=T, dependson='hcc5hmC-glmnetSuite-lasso-fit-all', cache.vars='', echo=F, eval=F}
 save(list='hcc5hmC_cv_lassoAll_lst', file=file.path("RData",'hcc5hmC_cv_lassoAll_lst'))
```
```{r hcc5hmC-glmnetSuite-load-hcc5hmC_cv_lassoAll_lst, cache=F, echo=F}
 load(file=file.path("RData",'hcc5hmC_cv_lassoAll_lst'))
```


Examine the fits.

```{r hcc5hmC-glmnetSuite-plot-lassoAll, cache=T, dependson='hcc5hmC-glmnetSuite-lasso-fit-all', cache.vars='', fig.height=5, fig.width=6, fig.cap='Repeated cv lasso models fitted to all samples'}
### CLEAR CACHE
plot(
 log(hcc5hmC_cv_lassoAll_lst[[1]]$lambda),
 hcc5hmC_cv_lassoAll_lst[[1]]$cvm,
 lwd=2,
 xlab='log(Lambda)', ylab='CV Misclassification Error', type='l', ylim=c(0, .5)
)

for(JJ in 2:length(hcc5hmC_cv_lassoAll_lst))
 lines(
  log(hcc5hmC_cv_lassoAll_lst[[JJ]]$lambda),
  hcc5hmC_cv_lassoAll_lst[[JJ]]$cvm,
  lwd=2
)

```

These cv curves are remarkably consistent meaning that the determination of the size or sparsity
of the model fitted through cross validation to the full data set is fairly precise:

<!-- DONT CACHE THIS ??? -->

```{r hcc5hmC-glmnetSuite-model-size-lassoAll, cache=T, dependson='hcc5hmC-glmnetSuite-lasso-fit-all', fig.height=5, fig.width=8, fig.cap='Feature selection and estimated error by repeated cv lasso models'}

library(magrittr)

par(mfrow=c(1,2), mar=c(3,4, 2, 1))

# nzero
nzero_1se_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se])

nzero_min_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min])

boxplot(list(`1se`=nzero_1se_vec, min = nzero_min_vec), ylab="Selected Features")

# error
error_1se_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se])

error_min_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min])

boxplot(
 list(`1se`=error_1se_vec, min = error_min_vec), 
 ylab=hcc5hmC_cv_lassoAll_lst[[1]]$name,
 ylim=c(0.06, .10)
)

# tabular format
tmp <- data.frame(rbind(
 `features_1se` = summary(nzero_1se_vec),
 features_min = summary(nzero_min_vec),
 `features:min-1se` = summary(nzero_min_vec - nzero_1se_vec),
 `cv_error_1se` = summary(100*error_1se_vec),
 cv_error_min = summary(100*error_min_vec),
 `cv_error:1se-min` = summary(100*(error_1se_vec-error_min_vec))
))

knitr::kable(tmp %>% dplyr::select(-Mean),
  caption = "Number of selected features",
  digits=1) %>%
   kableExtra::kable_styling(full_width = F)


```

The number of features selected by the minimum lambda models are larger
than the number selected by the "one standard error" rule models by a median
of `r median(nzero_min_vec-nzero_1se_vec)`.
The cv error rates obtained from the minimum lambda models are lower
then  "one standard error" rule models error rates by a median of
`r median(round(100*(error_1se_vec-error_min_vec), 1))`%.  

The cv error rates observed in this set are comparable to the 
rates observed in the lasso models fitted to the training sample set
which consisted of 80% of the samples in this set.  In other words,
there is no obvious gain in performance in moving from 
a data set with 
`r paste(round(table(group = hcc5hmC_all_group_vec)*.8), collapse=' vs ')` samples
to a data set with
`r paste(round(table(group = hcc5hmC_all_group_vec)), collapse=' vs ')` samples.
See Table \@ref(tab:printErrors).  

It's not clear at this point whether the minimum lambda model is truly better than
the  "one standard error" rule  model.  We would need and external validation
set to make this determination.  We can compare the two sets
of out-of-fold predicted values, averaged across cv replicates, to see if
there is a meaningful difference between the two.


```{r hcc5hmC-glmnetSuite-get-sample-pred, cache=T, dependson='hcc5hmC-glmnetSuite-lasso-fit-all', cache.vars=c('lassoAll_predResp_1se_vec','lassoAll_predResp_1se_vec','thres_1se','thres_min'), fig.height=5, fig.width=10, fig.cap="Predicted probabilities - averaged over cv replicates"}

# predicted probs - 1se
lassoAll_predResp_1se_mtx <- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) { 
  ndx_1se <- match(cv_fit$lambda.1se,cv_fit$lambda)
  logistic_f(cv_fit$fit.preval[,ndx_1se])
 })
lassoAll_predResp_1se_vec <- rowMeans(lassoAll_predResp_1se_mtx)

# predicted probs - min
lassoAll_predResp_min_mtx <- sapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit) { 
  ndx_min <- match(cv_fit$lambda.min,cv_fit$lambda)
  logistic_f(cv_fit$fit.preval[,ndx_min])
 })
lassoAll_predResp_min_vec <- rowMeans(lassoAll_predResp_min_mtx)

# plot
par(mfrow=c(1,2), mar=c(5,5,2,1))
tmp <- c(
 `1se` = split(lassoAll_predResp_1se_vec, hcc5hmC_all_group_vec),
 min = split(lassoAll_predResp_min_vec, hcc5hmC_all_group_vec)
)
names(tmp) <- sub('\\.', '\n', names(tmp))

boxplot(
 tmp,
 ylab='Predicted oof probability',
 border=c('green', 'red'),
 xaxt='n'
)
axis(side=1, at=1:length(tmp), tick=F, names(tmp))


# compare the two
plot(
 x = lassoAll_predResp_1se_vec, xlab='1se model oof Prob',
 y = lassoAll_predResp_min_vec, ylab='min lambda model oof Prob',
 col = ifelse(hcc5hmC_all_group_vec == 'HCC', 'red', 'green')
)
 
# Add referecne lines at 10% false positive
thres_1se <- quantile(lassoAll_predResp_1se_vec[hcc5hmC_all_group_vec == 'Control'], prob=.9)
thres_min <- quantile(lassoAll_predResp_min_vec[hcc5hmC_all_group_vec == 'Control'], prob=.9)
abline(v = thres_1se, h = thres_min, col='grey')

```

<!-- THIS PARAGRAPH REFERRED TO THE FITTED PROBS; NOT THE OOF PRED PROBS
We see that the minimum lambda models provide a better fit to the data,
which is to be expected as the minimum lambda models have more estimated
parameters than the one standard error rule models.  
-->

We see that there isn't a big difference in out-of-fold predicted
probabilities between the one-standard-error rule and the minimum lambda models.
One way to quantify
the difference in classification errors is to classify samples
according to each vector of predicted probabilities, setting
the thresholds to achieve a fixed false positive rate, 10% say.
These thresholds are indicated by the grey lines in the scatter plot
on the right side of Figure \@ref(fig:get-sample-pred).  

<!-- APPLIED TO THE FITTED VALUES
We note
that although predicted probability distributions are quite different
for the two models, the class predictions at a 10% false discovery threshold
are largely in agreement.
-->

```{r hcc5hmC-glmnetSuite-get-sample-class, cache=T, dependson='hcc5hmC-glmnetSuite-get-sample-pred', cache.vars=c('lassoAll_predClass_1se_vec','lassoAll_predClass_min_vec'),fig.cap='Predicted classes and 10% false positive rate'}

lassoAll_predClass_1se_vec <- ifelse(
 lassoAll_predResp_1se_vec > thres_1se, 'HCC', 'Control')

lassoAll_predClass_min_vec <- ifelse(
 lassoAll_predResp_min_vec > thres_min, 'HCC', 'Control')

tmp <- cbind(
 table(truth=hcc5hmC_all_group_vec, `1se-pred`=lassoAll_predClass_1se_vec),
 table(truth=hcc5hmC_all_group_vec, `min-pred`=lassoAll_predClass_min_vec)
) 
# Hack for printing
colnames(tmp) <- c('1se-Control', '1se-HCC', 'min-Control', 'min-HCC')

knitr::kable(tmp,
  caption = "Classifications: rows are truth",
  digits=1) %>%
   kableExtra::kable_styling(full_width = F)

```

When we fix the false positive rate at 10% (ie. the control samples error rates are fixed), 
the `1se` model makes 39 false negative calls whereas the minimum lambda model makes 32.  A difference
of `r round(100*(39-32)/555, 1)`%


<!-- APPLIED TO THE FITTED PROBABILITIES
We see that the min lambda model, makes no false negative calls at a 90% sensitivity
setting, and the sensitivity could be increased substantially at no false negative
cost.  This is definitely over-fitting the data set.  For the purpose
of computing sample consistency scores - what do these differences mean? 
-->

### Get sample consistency scores {-}

To compute consistency scores, we will use the out-of-fold predicted probabilities.

```{r hcc5hmC-glmnetSuite-get-sample-qual, cache=T, dependson='hcc5hmC-glmnetSuite-get-sample-pred', cache.vars=c('hcc5hmC_sample_1se_qual_vec','hcc5hmC_sample_min_qual_vec')}
# get qual scores

y <- as.numeric(hcc5hmC_all_group_vec == 'HCC')
# 1se
p <- lassoAll_predResp_1se_vec
hcc5hmC_sample_1se_qual_vec <- p^y*(1-p)^(1-y)

# min
p <- lassoAll_predResp_min_vec
hcc5hmC_sample_min_qual_vec <- p^y*(1-p)^(1-y)

```




We can examine consistency scores as a function of classification bin.


```{r hcc5hmC-glmnetSuite-plot-qual-conf, cache=F, cache.vars='', dependson='hcc5hmC-glmnetSuite-get-sample-pred', fig.height=5, fig.width=8, fig.cap='quality scores by classification - Control=0, HCC=1'}

y <- as.numeric(hcc5hmC_all_group_vec == 'HCC')

# 1se
lassoAll_1se_conf_vec <- paste(
 y, 
 as.numeric(lassoAll_predClass_1se_vec=='HCC'),
 sep = ':'
)

# min
lassoAll_min_conf_vec <- paste(
 y, 
 as.numeric(lassoAll_predClass_min_vec=='HCC'),
 sep = ':'
)


tmp <- c(
 split(hcc5hmC_sample_1se_qual_vec, lassoAll_1se_conf_vec), 
 split(hcc5hmC_sample_min_qual_vec, lassoAll_min_conf_vec)
)

par(mfrow=c(1,2), mar=c(4,3,3,2), oma=c(2,2,2,0))
gplots::boxplot2(split(hcc5hmC_sample_1se_qual_vec, lassoAll_1se_conf_vec), 
  outline=F, ylab = '', 
  border=c('green', 'green', 'red', 'red'),
  ylim=c(0,1))
title('1se Model')

gplots::boxplot2(split(hcc5hmC_sample_min_qual_vec, lassoAll_min_conf_vec), 
  outline=F, ylab = '',
  border=c('green', 'green', 'red', 'red'),
  ylim=c(0,1))
title('min lambda Model')


mtext(side=1, outer=T, cex=1.5, 'Classification - Truth:Predicted')
mtext(side=2, outer=T, cex=1.5, 'Consistency Score')
mtext(side=3, outer=T, cex=1.5, 'Sample Quality vs Classification Outcome')


```

This figure shows that for false positive cases (0:1 or classifying a
control as an affected case), the algorithm is *less certain* of its predicted
outcome than for the false negative cases (1:0 or classifying an affected case as a control).
ie. the misclassified HCC samples are quite similar to Controls, whereas there
is more ambiguity in the misclassified Control samples.

 
We will use the minimum lambda model to provide
the fitted probabilities used to compute quality scores,
but we could have used either one.

```{r hcc5hmC-glmnetSuite-sample-qual}

sample_qual_vec <- hcc5hmC_sample_min_qual_vec


```


## Selected feature list stability 

Before moving on to the simulation, let's examine gene selection stability on the
full data set.  We have two sets of selected features - one for the 
one standard deviation rule model, and one for the minimum lambda model.
We saw in Table \@ref(tab:model-size-lassoAll) that the number of features
selected by the minimum lambda models had an IQR of
`r paste(quantile(nzero_min_vec,1/4), quantile(nzero_min_vec,3/4), sep='-')`,
while the one standard error rule models had an IQR of
`r paste(quantile(nzero_1se_vec,1/4), quantile(nzero_1se_vec,3/4), sep='-')`.

Let's examine the stability of the gene lists across cv replicates.

```{r hcc5hmC-glmnetSuite-feature-list-1se, cache=T, cache.vars=c('genes_by_rep_1se_tbl','lassoAll_coef_1se_mtx'), fig.height=5, fig.width=8, fig.cap="Feature list stability for one standard error rule models"}
### CLEAR CACHE


# 1se
lassoAll_coef_1se_lst <- lapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit){
 cv_fit_coef <- coef(
 cv_fit,
 s = "lambda.1se"
 )
 cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]]
 })

# put into matrix
lassoAll_coef_1se_all <- Reduce(union, lassoAll_coef_1se_lst)
lassoAll_coef_1se_mtx <- sapply(lassoAll_coef_1se_lst, 
  function(LL) is.element(lassoAll_coef_1se_all, LL)
)
rownames(lassoAll_coef_1se_mtx) <- lassoAll_coef_1se_all

genes_by_rep_1se_tbl <- table(rowSums(lassoAll_coef_1se_mtx))
barplot(
 genes_by_rep_1se_tbl,
 xlab='Number of Replicates',
 ylab='Number of features'

)


```

We see that `r genes_by_rep_1se_tbl['30']` features are included in every
cv replicate.  These make up between 
`r round(quantile(genes_by_rep_1se_tbl['30']/colSums(lassoAll_coef_1se_mtx), 1/4)*100,0)`%
and
`r round(quantile(genes_by_rep_1se_tbl['30']/colSums(lassoAll_coef_1se_mtx), 3/4)*100,0)`%
(Q1 and Q3) of the cv replicate one standard error rule models feature lists.


```{r hcc5hmC-glmnetSuite-feature-list-min, cache=T, cache.vars=c('genes_by_rep_min_tbl','lassoAll_coef_min_mtx'), fig.height=5, fig.width=8, fig.cap="Feature list stability for minimum lambda models"}
### CLEAR CACHE


# min
lassoAll_coef_min_lst <- lapply(hcc5hmC_cv_lassoAll_lst, function(cv_fit){
 cv_fit_coef <- coef(
 cv_fit,
 s = "lambda.min"
 )
 cv_fit_coef@Dimnames[[1]][cv_fit_coef@i[-1]]
 })

# put into matrix
lassoAll_coef_min_all <- Reduce(union, lassoAll_coef_min_lst)
lassoAll_coef_min_mtx <- sapply(lassoAll_coef_min_lst, 
  function(LL) is.element(lassoAll_coef_min_all, LL)
)
rownames(lassoAll_coef_min_mtx) <- lassoAll_coef_min_all

genes_by_rep_min_tbl <- table(rowSums(lassoAll_coef_min_mtx))
barplot(
 genes_by_rep_min_tbl,
 xlab='Number of Replicates',
 ylab='Number of features'

)


```

We see that `r genes_by_rep_min_tbl['30']` features are included in every
cv replicate.  These make up between 
`r round(quantile(genes_by_rep_min_tbl['30']/colSums(lassoAll_coef_min_mtx), 1/4)*100,0)`%
and
`r round(quantile(genes_by_rep_min_tbl['30']/colSums(lassoAll_coef_min_mtx), 3/4)*100,0)`%
(Q1 and Q3) of the cv replicate min feature lists.
We will consider the genes that are selected in all cv replicates as a 
gene signature produced by each model.


```{r hcc5hmC-glmnetSuite-minVs1seGenes}

lasso_gene_sign_1se_vec <- rownames(lassoAll_coef_1se_mtx)[rowSums(lassoAll_coef_1se_mtx)==30]
lasso_gene_sign_min_vec <- rownames(lassoAll_coef_min_mtx)[rowSums(lassoAll_coef_min_mtx)==30]

```

`r length(intersect(lasso_gene_sign_1se_vec, lasso_gene_sign_min_vec))` out of
`r length(lasso_gene_sign_1se_vec)` of the genes in the 1se model gene signature
are contained in the min lambda model gene signature.

## Simulation Design

We are now ready to run the simulations.

```{r hcc5hmC-glmnetSuite-simParms, cahce=F}
 SIM <- 30
 SIZE <- c(25, 50, 100, 200, 300)
 CV_REP <- 30

```

Simluation parameters:  

* Number of simulations : SIM = `r SIM`

* Sample sizes: SIZE = `r SIZE`  

* Number of CV Replicates:  CV_REP = `r CV_REP`


We will repeat the simulation process SIM = `r SIM` times.
For each simulation iteration, we will select `r max(SIZE)` Control and 
`r max(SIZE)` HCC samples at random.  Models will be fitted and analyzed
to balanced subsets of SIZE = `r SIZE`, in a `Matryoshka doll` manner to
emulate a typical sample accrual process.  Note that in this accrual process
there is no time effect - the accrual process is completely randomized.  In practice,
there could be significant time effects.  For example, the first 25 HCC samples could come
from Center A, while the next 25 could come from Center B.  And 
affected and control samples could be acquired from different clinics
or in different time intervals.  In other words,
there is no batch effect or shared variability in our simulation,
while these are almost always present in real data, including 
batch effects that are associated with class labels - controls being in
different batches than affected samples is an all too common occurrence,
for example.  One should be especially watchful of potential batch effects
when dealing with blood samples as blood is notoriously finicky in
character [@Huang:2017aa; @Permenter:2015aa;].
Presented with results that look impressively good based on a small data set,
one should definitely be skeptical of the promise of future equally good results.

For a given simulation and a given sample size, we will obtain
CV_REP = `r CV_REP` cross-validated lasso fits.  From these fits,
we can obtain `r CV_REP` out-of-fold assessments of classification accuracy 
to get a sense if its variability. From each cv replicate, we also obtain
an estimated model size and a set of selected features.  We will want
to examine how these stabilize as the sample size increases.

Note that we limit the simulations to a maximum of sample size of 300 in 
order to to have simulations with low overlap.  With 300
randomly selected HCC samples, the expected overlap between two randomly
selected sets of HCC samples is `r round(100*(300/sum(hcc5hmC_all_group_vec=='HCC'))^2,1)`%.
For Controls the expected overlap is `r round(100*(300/sum(hcc5hmC_all_group_vec=='Control'))^2,1)`%. 

**Reader Note**
```
We are currently not following individual performance paths
as a study set grows over time.  We currently simply summarize
results across simulations.  Exmaining individual paths would
show how chaotic the assessments of performance can be when
sample sizes are small.

To Do.
```

## Setup simulation 

To setup the simulation, we only need two master tables: one for the selection of Controls
and one for the selection of HCC samples.

```{r hcc5hmC-glmnetSuite-get-all-vec, cache=T, cache.vars=c('hcc5hmC_all_control_vec', 'hcc5hmC_all_affected_vec')}

hcc5hmC_all_control_vec <- names(hcc5hmC_all_group_vec[hcc5hmC_all_group_vec=='Control']) 
hcc5hmC_all_affected_vec <- names(hcc5hmC_all_group_vec[hcc5hmC_all_group_vec=='HCC'])  

```

We have `r length(hcc5hmC_all_control_vec)` control sample IDs stored in `hcc5hmC_all_control_vec`
and `r length(hcc5hmC_all_affected_vec)` affected sample IDs stored in `hcc5hmC_all_affected_vec`.
To create a suite of random samples from these, we only need to randomly select indices from
each vector.

  
```{r hcc5hmC-glmnetSuite-getSimTable, cache=T, cache.vars=c('hcc5hmC_sim_control_mtx', 'hcc5hmC_sim_affected_mtx')}

set.seed(12379)

hcc5hmC_sim_control_mtx <- sapply(
 1:SIM, 
 function(dummy) 
   sample(1:length(hcc5hmC_all_control_vec), size =  max(SIZE))
)


hcc5hmC_sim_affected_mtx <- sapply(
 1:SIM, 
 function(dummy) 
   sample(1:length(hcc5hmC_all_affected_vec), size =  max(SIZE))
)


```

Each simulation is specified by a given column of the simulation design matrices:
`hcc5hmC_sim_control_mtx` and `hcc5hmC_sim_affected_mtx`, each with dimensions `r dim(hcc5hmC_sim_affected_mtx)`.
Within each simulation, we can run the analyses of size `r SIZE` by simply selecting
samples specified in the appropriate rows of each design matrix.

We can examine how much variability we have in the quality scores of the selected samples.
Here we show results for the small sample sizes where variability will be the greatest.

```{r hcc5hmC-glmnetSuite-look-sim-qual_ARCHIVED, cache=T, cache.vars=c('sim_control_qual_mtx', 'sim_affected_qual_mtx'), fig.height=8, fig.width=10, fig.cap='sample consistency by simulation run', eval=F, echo=F}
### CLEAR CACHE

all_control_qual_vec <- sample_qual_vec[hcc5hmC_all_control_vec]
sim_control_qual_mtx <- sapply(
  1:ncol(hcc5hmC_sim_control_mtx), 
  function(CC) all_control_qual_vec[hcc5hmC_sim_control_mtx[,CC]]
 )

all_affected_qual_vec <- sample_qual_vec[hcc5hmC_all_affected_vec]
sim_affected_qual_mtx <- sapply(
  1:ncol(hcc5hmC_sim_affected_mtx),  
  function(CC) all_affected_qual_vec[hcc5hmC_sim_affected_mtx[,CC]]
 )

# Get stage from SIZE 
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)

sim_control_qual_byStage_lst <- do.call('c', 
 lapply(1:ncol(sim_control_qual_mtx), 
  function(CC) c(split(sim_control_qual_mtx[,CC], stage_vec),NA)
 )
)

sim_affected_qual_byStage_lst <- do.call('c', 
 lapply(1:ncol(sim_affected_qual_mtx), 
  function(CC) c(split(sim_affected_qual_mtx[,CC], stage_vec),NA)
 )
)

# PLOT
par(mfrow=c(2,1), mar = c(2,5,2,1))
# control
boxplot(
  sim_control_qual_byStage_lst, 
  outline = F, 
  border = 1:6,
  ylab = 'Quality Score',
  xaxt = 'n'
)
legend('bottomright', title = 'Stage', ncol = 2,
 legend = names(sim_control_qual_byStage_lst[1:5]), 
 text.col = 1:5,
 bty = 'n', horiz = F
)
sim_ndx <- which(names(sim_control_qual_byStage_lst) =='')
abline(v = sim_ndx, col = 'grey')
axis(
  side = 1, 
  at = sim_ndx-2, 
  label = 1:length(sim_ndx),
  tick = F, 
  line = -1, las = 2,
  cex.axis = 0.8)
title("Control sample consistency by stage and simulation")

# affected
boxplot(
  sim_affected_qual_byStage_lst, 
  outline = F, 
  border = 1:6,
  ylab = 'Quality Score',
  xaxt = 'n'
)
sim_ndx <- which(names(sim_affected_qual_byStage_lst)=='')
abline(v = which(names(sim_affected_qual_byStage_lst)==''), col = 'grey')
axis(
  side=1, 
  at = sim_ndx-2,        
  label = 1:length(sim_ndx),
  tick = F, 
  line = -1, las = 2,
  cex.axis = 0.8)
title("Affected sample consistency by stage and simulation")

```

```{r hcc5hmC-glmnetSuite-look-sim-qual-0-50ONLY, cache=T, cache.vars=c('sim_control_qual_mtx', 'sim_affected_qual_mtx'), fig.height=8, fig.width=10, fig.cap='sample consistency by simulation run for size = 50 '}
### CLEAR CACHE

all_control_qual_vec <- sample_qual_vec[hcc5hmC_all_control_vec]
sim_control_qual_mtx <- sapply(
  1:ncol(hcc5hmC_sim_control_mtx), 
  function(CC) all_control_qual_vec[hcc5hmC_sim_control_mtx[,CC]]
 )

all_affected_qual_vec <- sample_qual_vec[hcc5hmC_all_affected_vec]
sim_affected_qual_mtx <- sapply(
  1:ncol(hcc5hmC_sim_affected_mtx),  
  function(CC) all_affected_qual_vec[hcc5hmC_sim_affected_mtx[,CC]]
 )

# ONLY LOOK AT SAMPLE SIZE == 50
NN <- 50

# PLOT
par(mfrow=c(2,1), mar = c(2,5,2,1))
# control
boxplot(
  sim_control_qual_mtx[1:NN,],
  outline = T, 
  ylab = 'Quality Score',
  xaxt = 'n'
)
title("Control sample consistency across simulations")

# affected
boxplot(
  sim_affected_qual_mtx[1:NN,],
  outline = T, 
  ylab = 'Quality Score'
)
title("Affected sample consistency across simulations")

```

In this figure, we are summarizing the quality measures of 50 samples per group across
30 simulations, or random selections of control and affected samples.
We see significant variability in sample consistency, especially in the affected cases.
This may lead an unwary observer to be overly optimistic, or overly pessimistic,
in the early accrual stages of a study.


## Run simulations


As these take a while to run, 
we will save the results of each simulation to a different
object and store to disk.  These can be easily read from disk
when needed for analysis.


The simulation results are saved to the file system and
only needs to be run once.  The simulation takes $\approx$ 8 to 10 minutes
per iteration, or 4 to 5 hours of run time on a laptop.
(Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Mojave 10.14.6)

<!-- RUN ONCE - THEN GET FROM MEMORY -->
```{r hcc5hmC-glmnetSuite-run-sim, cache=T, cache.vars='start_time', eval=F}
start_time <- proc.time()

# Get stage from SIZE
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0, SIZE), include.lowest = T)

for (SIMno in 1:ncol(sim_control_qual_mtx)) {

  #cat("Running simulation ", SIMno, "\n")

  sim_cv_lst <- lapply(1:length(levels(stage_vec)), function(STGno) {
    Stage_rows_vec <- which(stage_vec %in% levels(stage_vec)[1:STGno])
    #cat("Stage ", STGno, "- analyzing", length(Stage_rows_vec), "paired samples.\n")

    sim_stage_samples_vec <- c(
      hcc5hmC_all_control_vec[hcc5hmC_sim_control_mtx[Stage_rows_vec, SIMno]],
      hcc5hmC_all_affected_vec[hcc5hmC_sim_affected_mtx[Stage_rows_vec, SIMno]]
    )
    sim_stage_lcpm_mtx <- hcc5hmC_all_lcpm_mtx[sim_stage_samples_vec, ]
    sim_stage_group_vec <- hcc5hmC_all_group_vec[sim_stage_samples_vec]
    #print(table(sim_stage_group_vec))

    sim_stage_cv_lst <- lapply(1:CV_REP, function(CV) {
      cv_fit <- glmnet::cv.glmnet(
        x = sim_stage_lcpm_mtx,
        y = sim_stage_group_vec,
        alpha = 1,
        family = "binomial",
        type.measure = "class",
        keep = T,
        nlambda = 30
      )

      # Extract 1se metrics from cv_fit
      #######################
      ndx_1se <- which(cv_fit$lambda == cv_fit$lambda.1se)

      nzero_1se <- cv_fit$nzero[ndx_1se]
      cvm_1se <- cv_fit$cvm[ndx_1se]

      # test error
      sim_stage_test_samples_vec <- setdiff(rownames(hcc5hmC_all_lcpm_mtx), sim_stage_samples_vec)
      sim_stage_hcc5hmC_test_lcpm_mtx <- hcc5hmC_all_lcpm_mtx[sim_stage_test_samples_vec,]
      sim_stage_hcc5hmC_test_group_vec <- hcc5hmC_all_group_vec[sim_stage_test_samples_vec]

      test_pred_1se_vec <- predict(
       cv_fit,
       newx=sim_stage_hcc5hmC_test_lcpm_mtx,
       s="lambda.1se",
       type="class"
      )
      test_1se_error <- mean(test_pred_1se_vec != sim_stage_hcc5hmC_test_group_vec)

      # genes
      coef_1se <- coef(
        cv_fit,
        s = "lambda.1se"
      )
      genes_1se <- coef_1se@Dimnames[[1]][coef_1se@i[-1]]

      # Extract min metrics from cv_fit
      #######################
      ndx_min <- which(cv_fit$lambda == cv_fit$lambda.min)

      nzero_min <- cv_fit$nzero[ndx_min]
      cvm_min <- cv_fit$cvm[ndx_min]

      # test error
      sim_stage_test_samples_vec <- setdiff(rownames(hcc5hmC_all_lcpm_mtx), sim_stage_samples_vec)
      sim_stage_hcc5hmC_test_lcpm_mtx <- hcc5hmC_all_lcpm_mtx[sim_stage_test_samples_vec,]
      sim_stage_hcc5hmC_test_group_vec <- hcc5hmC_all_group_vec[sim_stage_test_samples_vec]

      test_pred_min_vec <- predict(
       cv_fit,
       newx=sim_stage_hcc5hmC_test_lcpm_mtx,
       s="lambda.min",
       type="class"
      )
      test_min_error <- mean(test_pred_min_vec != sim_stage_hcc5hmC_test_group_vec)

      # genes
      coef_min <- coef(
        cv_fit,
        s = "lambda.min"
      )
      genes_min <- coef_min@Dimnames[[1]][coef_min@i[-1]]

      # return cv_fit summary metrics
      list(
       p_1se = nzero_1se, 
       p_min = nzero_min, 
       cv_1se = cvm_1se, 
       cv_min = cvm_min, 
       test_1se=test_1se_error, 
       test_min=test_min_error, 
       genes_1se = genes_1se,
       genes_min = genes_min)
    })
    sim_stage_cv_lst
  })

  # save  sim_cv_lst
  fName <- paste0("hcc5hmC_sim_", SIMno, "_cv_lst")
  assign(fName, sim_cv_lst)
  save(list = fName, file=file.path("RData", fName))

}

```

<!-- DEBUG - rename after the fact -->

```{r hcc5hmC-glmnetSuite-RENAME-sim, echo=F, eval=F}
sim_files_vec <- list.files('RData', '^sim_')

for(FF in sim_files_vec){
  load(file=file.path('RData', FF))
  assign(paste0('hcc5hmC_',FF), get(FF))
  save(list = paste0('hcc5hmC_',FF), file=file.path("RData", paste0('hcc5hmC_',FF)))
  rm(list=c(FF, paste0('hcc5hmC_',FF)))
}
```


## Simulation results

Recall the we have `r SIM` simulations, or randomly selected sets of HCC and Control samples,
analyzed in increasing sizes of `r paste(SIZE, sep=', ')`, with
`r CV_REP` repeated cross-validated lasso fits:


* Sample sizes: SIZE = `r SIZE`

* Number of CV Replicates:  CV_REP = `r CV_REP`



First we extract simulation results and store into one big table
(only showing the top of table here):

```{r hcc5hmC-glmnetSuite-extract-sim-results, cache=T, cache.vars='hcc5hmC_lasso_sim_results_frm'}
### CLEAR CACHE
sim_files_vec <- list.files('RData', '^hcc5hmC_sim_')


# define extraction methods

# Each sumulation is a list of cv results 
## nested in a list of replicates
##############################################

# cvList2frm_f makes a frame out of the inner list
cvList2frm_f <- function(cv_lst) {
 frm1 <- as.data.frame(t(sapply(cv_lst, function(x) x)))
 frm2 <- data.frame(
  unlist(frm1[[1]]), unlist(frm1[[2]]),
  unlist(frm1[[3]]), unlist(frm1[[4]]),
  unlist(frm1[[5]]), unlist(frm1[[6]]),
  frm1[7], frm1[8])
  names(frm2) <- names(frm1)
  data.frame(Rep=1:nrow(frm2), frm2)}

# cv_lst_to_frm loop over replicates, concatenating the inner list frames
cv_lst_to_frm <- function(sim_cv_lst) {
 do.call('rbind', lapply(1:length(sim_cv_lst),
  function(JJ) {
    siz_frm <- cvList2frm_f(sim_cv_lst[[JJ]])
    data.frame(Size=SIZE[JJ], siz_frm)
  }))
}

# we loop across simulations to combine all results into one big table
hcc5hmC_lasso_sim_results_frm <- do.call('rbind', lapply(1:length(sim_files_vec),
 function(SIM_NO) {
  load(file=file.path('RData', sim_files_vec[SIM_NO]))
  assign('sim_cv_lst', get(sim_files_vec[SIM_NO]))
  rm(list=sim_files_vec[SIM_NO])
  
  data.frame(SimNo=paste0('Sim_',formatC(SIM_NO,width = 2,flag = 0)), cv_lst_to_frm(sim_cv_lst))
} 
)) 

```

<!-- 
Have a table of simulation results - `hcc5hmC_lasso_sim_results_frm`:
-->

```{r hcc5hmC-glmnetSuite-sum-table, cache=T, cache.vars='', fig.cap='Simution results table'}
### CLEAR CACHE
 
knitr::kable(head(with(hcc5hmC_lasso_sim_results_frm, table(SimNo, Size))),
  caption = paste("Simulation Results - N Sim =", SIM)) %>%
   kableExtra::kable_styling(full_width = F)

knitr::kable(head(hcc5hmC_lasso_sim_results_frm) %>% dplyr::select(-c(genes_1se, genes_min)),
    caption = paste("Simulation Results - not showing genes column"),
    digits=2) %>%
   kableExtra::kable_styling(full_width = F)

```


### Simulation Results - look at one simulation

#### Model Accuracy Assessment

First examine results for one simulation run.  In the figures that follow,
each boxplot summarized 30 repeated cross validation runs performed on a 
fixed random selection of Control and Affected samples.  Recall that as
we move from 25 to 50, etc., the sample sets are growing to emulate an
accrual of samples over time.

```{r hcc5hmC-glmnetSuite-lasso-simRes-errors-bySim, cache=T, cache.vars='', fig.heigth=5, fig.width=10, fig.cap='lasso Model Errors by Sample Size'}
### CLEAR CACHE

# get full model cv error ref
error_1se_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se])
error_1se_q2 <- quantile(error_1se_vec, prob=1/2)        

error_min_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min])
error_min_q2 <- quantile(error_min_vec, prob=1/2)        

# Utility objects
SIZE0 <- stringr::str_pad(SIZE, width=3, pad='0')
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)


#SIM <- "Sim_01"

for(SIM in unique(hcc5hmC_lasso_sim_results_frm$SimNo)[1]){

SimNum <- as.numeric(sub('Sim_','',SIM))

simNo_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(SimNo==SIM)


# errors
par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0))
###################
# 1se
####################
cv_1se_lst <- with(simNo_results_frm,
 split(cv_1se, Size))
names(cv_1se_lst) <- paste0(stringr::str_pad(names(cv_1se_lst), width=3, pad='0'),'_cv')

test_1se_lst <- with(simNo_results_frm,
 split(test_1se, Size))
names(test_1se_lst) <- paste0(stringr::str_pad(names(test_1se_lst), width=3, pad='0'),'_cv')

error_1se_lst <- c(cv_1se_lst, test_1se_lst)
error_1se_lst <- error_1se_lst[order(names(error_1se_lst))]

boxplot(error_1se_lst, 
  border=c('blue','green'), 
  ylim=c(0, 0.5),
  xaxt='n'
)
mtext(side=2, outer=T,  'Misclassification Error')

LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_1se_lst)), 
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_cv'),names(error_1se_lst))[-1] - 0.5, col='grey')
abline(h= error_1se_q2, col = 'red')
legend('topright', 
   #title='1se errors', title.col = 'black',
   text.col = c('blue','green'),
   legend = c('cv error', 'test set'),
   bty='n'
 )
title(paste('one se lambda models'))

SKIP  <- function() {
# Add qual annotation
control_qual_vec <- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median)
affected_qual_vec <- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median)
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_1se_lst)),
  round(control_qual_vec, 2)
 )
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_1se_lst)),
  round(affected_qual_vec, 2)
 )
}#SKIP

# min
####################
cv_min_lst <- with(simNo_results_frm,
 split(cv_min, Size))
names(cv_min_lst) <- paste0(stringr::str_pad(names(cv_min_lst), width=3, pad='0'),'_cv')

test_min_lst <- with(simNo_results_frm,
 split(test_min, Size))
names(test_min_lst) <- paste0(stringr::str_pad(names(test_min_lst), width=3, pad='0'),'_cv')

error_min_lst <- c(cv_min_lst, test_min_lst)
error_min_lst <- error_min_lst[order(names(error_min_lst))]

boxplot(error_min_lst, 
  border=c('blue','green'), 
  ylim=c(0, 0.5),
  xaxt='n'
)
mtext(side=2, outer=T,  'Misclassification Error')
LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_min_lst)), 
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_cv'),names(error_min_lst))[-1] - 0.5, col='grey')
abline(h= error_min_q2, col = 'red')
legend('topright', 
   #title='min errors', title.col = 'black',
   text.col = c('blue','green'),
   legend = c('cv error', 'test set'),
   bty='n'
 )
title(paste('min lambda models'))

SKIP  <- function() {
# Add qual annotation
control_qual_vec <- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median)
affected_qual_vec <- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median)
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_min_lst)),
  round(control_qual_vec, 2)
 )
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_min_lst)),
  round(affected_qual_vec, 2)
 )
}#SKIP
mtext(side=3, outer=T, cex=1.25, paste('Sim =',  SIM))

} # for(SIM

```

In this one simulation, we see:

* Model accuracy increases with sample size, with minimal improvement going from N=200 to N=300.  

* CV error rates tend to be pessimistic, especially for the small sample sizes.  This is odd
and may be related to sample consistency.  

* There isn't much to chose from between the one standard error and the minimum lambda models.  The
latter may show lower propensity to produce optimistic cv error rates.  

#### Feature Selection 

```{r hcc5hmC-glmnetSuite-lasso-simRes-features-bySim, cache=T, cache.vars='', fig.heigth=5, fig.width=10, fig.cap='lasso Models Selected Features by Sample Size'}
### CLEAR CACHE

# get full model nzero ref
nzero_1se_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.1se])
nzero_1se_q2 <- quantile(nzero_1se_vec, prob=c(2)/4)

nzero_min_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$nzero[cv_fit$lambda == cv_fit$lambda.min])
nzero_min_q2 <- quantile(nzero_min_vec, prob=c(2)/4)

# Utility objects
SIZE0 <- stringr::str_pad(SIZE, width=3, pad='0')
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)


#SIM <- "Sim_01"

for(SIM in unique(hcc5hmC_lasso_sim_results_frm$SimNo)[1]){

SimNum <- as.numeric(sub('Sim_','',SIM))

simNo_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(SimNo==SIM)


par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,0,2,0))
###################
# 1se
####################
# selected feature counts
p_1se_lst <- with(simNo_results_frm,
 split(p_1se, Size))
names(p_1se_lst) <- paste0(stringr::str_pad(names(p_1se_lst), width=3, pad='0'),'_p')

# get selected features that are part of lasso_gene_sign_1se_vec
# - the signature selected genes
sign_genes_1se_lst <- lapply(1:nrow(simNo_results_frm), function(RR)
    intersect(unlist(simNo_results_frm[RR, 'genes_1se']), lasso_gene_sign_1se_vec))

sign_p_1se_lst <- split(sapply(sign_genes_1se_lst, length), simNo_results_frm$Size)
names(sign_p_1se_lst) <- paste0(stringr::str_pad(names(sign_p_1se_lst), width=3, pad='0'),'_signP')


p_singP_1se_lst <- c(p_1se_lst, sign_p_1se_lst)
p_singP_1se_lst <- p_singP_1se_lst[order(names(p_singP_1se_lst))]

boxplot(p_singP_1se_lst,
  border=c('blue','green'),
  ylim=c(0, 200),
  xaxt='n'
)
mtext(side=2, outer=T,  'number of selected features')

LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_p'),names(p_singP_1se_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_p'),names(p_singP_1se_lst))[-1] - 0.5, col='grey')
#abline(h= nzero_1se_q2, col = 'red')
legend('topleft',
   #title='1se errors', title.col = 'black',
   text.col = c('blue', 'green'),
   legend= c('selected genes','signature genes'),
   bty='n'
 )
title(paste('one se lambda models'))


SKIP  <- function() {
# Add qual annotation
control_qual_vec <- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median)
affected_qual_vec <- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median)
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at =  match(paste0(SIZE0,'_p'),names(p_singP_1se_lst)),
  round(control_qual_vec, 2)
 )
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at =  match(paste0(SIZE0,'_p'),names(p_singP_1se_lst)),
  round(affected_qual_vec, 2)
 )
}#SKIP

###################
# min
####################
# selected feature counts
p_min_lst <- with(simNo_results_frm,
 split(p_min, Size))
names(p_min_lst) <- paste0(stringr::str_pad(names(p_min_lst), width=3, pad='0'),'_p')

# get selected features that are part of lasso_gene_sign_min_vec
# - the signature selected genes
sign_genes_min_lst <- lapply(1:nrow(simNo_results_frm), function(RR)
    intersect(unlist(simNo_results_frm[RR, 'genes_min']), lasso_gene_sign_min_vec))

sign_p_min_lst <- split(sapply(sign_genes_min_lst, length), simNo_results_frm$Size)
names(sign_p_min_lst) <- paste0(stringr::str_pad(names(sign_p_min_lst), width=3, pad='0'),'_signP')


p_singP_min_lst <- c(p_min_lst, sign_p_min_lst)
p_singP_min_lst <- p_singP_min_lst[order(names(p_singP_min_lst))]

boxplot(p_singP_min_lst,
  border=c('blue','green'),
  ylim=c(0, 200),
  xaxt='n'
)
LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_p'),names(p_singP_min_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_p'),names(p_singP_min_lst))[-1] - 0.5, col='grey')
#abline(h= nzero_min_q2, col = 'red')
legend('topleft',
   #title='min errors', title.col = 'black',
   text.col = c('blue', 'green'),
   legend= c('selected genes','signature genes'),
   bty='n'
 )
title(paste('min lambda models'))

SKIP  <- function() {
# Add qual annotation
control_qual_vec <- sapply(split(sim_control_qual_mtx[,SimNum], stage_vec), median)
affected_qual_vec <- sapply(split(sim_affected_qual_mtx[,SimNum], stage_vec), median)
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at =  match(paste0(SIZE0,'_p'),names(p_singP_min_lst)),
  round(control_qual_vec, 2)
 )
LL <- LL + 1
axis(side=1, tick=F, line = LL,
  at =  match(paste0(SIZE0,'_p'),names(p_singP_min_lst)),
  round(affected_qual_vec, 2)
 )
}#SKIP

mtext(side=3, outer=T, cex=1.25, paste('Sim =',  SIM))

} # for(SIM


```

In this one simulation, we see:

* The selected number of features in the smaller sample size analyses
are low, with few features belonging to the core signature identified in the
full data set.  

* As the sample size increases the number of features selected in the minimum 
lambda model remains variable, but the number of core signature features
selected in the samples of sizes 200 and 300 is stable and between 40 and 50.  


### Summarize results across simulation runs.

#### Model Accuracy Assessment

Now look across all simulations.  In the figures that follow, each boxplot
summarizes the results of 30 simulations.  For a give sample size and a 
given simulation, each data point is the median across 30 repeated cv runs.


```{r hcc5hmC-glmnetSuite-lasso-simRes-errors-overSim, cache=T, cache.vars=c('error_1se_Bysize_lst','error_min_Bysize_lst'), fig.heigth=5, fig.width=10, fig.cap='lasso Model Errors by Sample Size'}
### CLEAR CACHE

# get full model cv error ref
error_1se_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.1se])
error_1se_q2 <- quantile(error_1se_vec, prob=1/2)        

error_min_vec <- sapply(hcc5hmC_cv_lassoAll_lst,
 function(cv_fit) cv_fit$cvm[cv_fit$lambda == cv_fit$lambda.min])
error_min_q2 <- quantile(error_min_vec, prob=1/2)        

# Utility objects
SIZE0 <- stringr::str_pad(SIZE, width=3, pad='0')
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)

par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0))
# 1se
#########################################
## cv
cv_1se_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_cv_1se_lst <- with(sizeVal_results_frm, split(cv_1se, SimNo))
 sapply(sizeVal_cv_1se_lst, median)
})
names(cv_1se_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_cv')

## test
test_1se_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_test_1se_lst <- with(sizeVal_results_frm, split(test_1se, SimNo))
 sapply(sizeVal_test_1se_lst, median)
})
names(test_1se_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_test')


error_1se_Bysize_lst <- c(cv_1se_Bysize_lst, test_1se_Bysize_lst)
error_1se_Bysize_lst <- error_1se_Bysize_lst[order(names(error_1se_Bysize_lst))]

boxplot(error_1se_Bysize_lst,
  col=0,
  border=c('blue','green'),
  ylim=c(0, .5),
  outline=F,
  xaxt='n'
)
mtext(side=2, outer=T,  'Misclassification Error')

for(JJ in 1:length(error_1se_Bysize_lst))
points(
   x=jitter(rep(JJ, length(error_1se_Bysize_lst[[JJ]])), amount=0.25), 
   y=error_1se_Bysize_lst[[JJ]], cex=0.5,
   col=ifelse(grepl('cv', names(error_1se_Bysize_lst)[JJ]),'blue', 'green')
)
LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_1se_Bysize_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_cv'),names(error_1se_Bysize_lst))[-1] - 0.5, col='grey')
abline(h= error_min_q2, col = 'red')
legend('topright',
   #title='min errors', title.col = 'black',
   text.col = c('blue','green'),
   legend = c('cv error', 'test set'),
   bty='n'
 )
title(paste('one se lambda models'))


# min
#########################################
## cv
cv_min_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_cv_min_lst <- with(sizeVal_results_frm, split(cv_min, SimNo))
 sapply(sizeVal_cv_min_lst, median)
})
names(cv_min_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_cv')

## test
test_min_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_test_min_lst <- with(sizeVal_results_frm, split(test_min, SimNo))
 sapply(sizeVal_test_min_lst, median)
})
names(test_min_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_test')


error_min_Bysize_lst <- c(cv_min_Bysize_lst, test_min_Bysize_lst)
error_min_Bysize_lst <- error_min_Bysize_lst[order(names(error_min_Bysize_lst))]

boxplot(error_min_Bysize_lst,
  col=0,
  border=c('blue','green'),
  ylim=c(0, .5),
  outline=F,
  xaxt='n'
)
for(JJ in 1:length(error_min_Bysize_lst))
points(
   x=jitter(rep(JJ, length(error_min_Bysize_lst[[JJ]])), amount=0.25), 
   y=error_min_Bysize_lst[[JJ]], cex=0.5,
   col=ifelse(grepl('cv', names(error_min_Bysize_lst)[JJ]),'blue', 'green')
)
LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_cv'),names(error_min_Bysize_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_cv'),names(error_min_Bysize_lst))[-1] - 0.5, col='grey')
abline(h= error_min_q2, col = 'red')
legend('topright',
   #title='min errors', title.col = 'black',
   text.col = c('blue','green'),
   legend = c('cv error', 'test set'),
   bty='n'
 )
title(paste('min lambda models'))


mtext(side=3, outer=T, cex=1.25, paste('lasso fit error rates summarized across simulations'))


```

<br/>


* For the smaller samples sizes, cv error rates for the minimum lambda models tend to be
optimistic.  

* **For lower sample sizes, assess performance is quite variable**. This
is key:  with N=25 cases and controls, the estimated classification error
rate produced by a given cohort can range from below 0.2 to higher than 0.5,
or basically no indication of discrimination value in the analyzed features.

<br/>

To appreciate how much variability can be encountered as samples are accrued over time
we need to look at a typical path the assessed model accuracy estimates might take.


```{r hcc5hmC-glmnetSuite-lasso-simRes-errorsPath-overSim, cache=T, cache.vrs='',fig.heigth=5, fig.width=10, fig.cap='lasso Model Error Paths'}
### CLEAR CACHE

error_1se_Bysize_mtx <- do.call('cbind', lapply(error_1se_Bysize_lst, function(LL) LL))

cv_error_1se_Bysize_mtx <- error_1se_Bysize_mtx[,grep('_cv', colnames(error_1se_Bysize_mtx))]

plot(x=c(1, ncol(cv_error_1se_Bysize_mtx)), y=c(0,0.6), 
  xlab='sample size', ylab='Misclassification Error', 
  type='n', xaxt='n')
axis(side=1, at=1:ncol(cv_error_1se_Bysize_mtx), 
 labels=sub('_cv','',colnames(cv_error_1se_Bysize_mtx)))
for(JJ in 1:15)
lines(x=1:ncol(cv_error_1se_Bysize_mtx), y=cv_error_1se_Bysize_mtx[JJ,],
 type='b', pch=JJ, col=JJ)
title('Example Misclassification Error Paths')

```


We see how erratic the assessed model accuracy can be when sample sizes are small,
and that it would be hard to guess the ultimate level of accuracy the
is achievable, or the number of samples required to get a reasonable 
estimate of the achievable level of accuracy.


<br/>


<br/>


```{r hcc5hmC-glmnetSuite-print-lasso-simRes-errors-overSim, cache=T, cache.vars='', fig.cap='lasso Model Errors by Sample Size', include=F}
### CLEAR CACHE

error_1se_Bysize_sum_frm <- t(sapply(error_1se_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4)))
colnames(error_1se_Bysize_sum_frm) <- paste0('1se_', colnames(error_1se_Bysize_sum_frm))

error_min_Bysize_sum_frm <- t(sapply(error_min_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4)))
colnames(error_min_Bysize_sum_frm) <- paste0('min_', colnames(error_min_Bysize_sum_frm))


knitr::kable(cbind(`1se`=error_1se_Bysize_sum_frm, min=error_min_Bysize_sum_frm),
      caption = paste("lasso error rates by sample size across simulations"),
    digits=2) %>%
   kableExtra::kable_styling(full_width = F)

```


#### Feature Selection 


```{r hcc5hmC-glmnetSuite-lasso-simRes-features-OverSim, cache=T, cache.vars=c('p_singP_1se_Bysize_lst','p_singP_min_Bysize_lst'), fig.heigth=5, fig.width=10, fig.cap='lasso Models Selected Features by Sample Size'}
### CLEAR CACHE

# Utility objects
SIZE0 <- stringr::str_pad(SIZE, width=3, pad='0')
stage_vec <- cut(1:nrow(sim_control_qual_mtx), c(0,SIZE), include.lowest = T)

par(mfrow=c(1,2), mar=c(4, 2, 2, 1), oma=c(0,2,2,0))
# 1se
#########################################
# selected features
p_1se_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_p_1se_lst <- with(sizeVal_results_frm, split(p_1se, SimNo))
 sapply(sizeVal_p_1se_lst, median)
})
names(p_1se_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_p')

# selected signatue features
sign_p_1se_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 
  
 sizeVal_sign_genes_1se_lst <- lapply(1:nrow(sizeVal_results_frm), function(RR)
    intersect(unlist(sizeVal_results_frm[RR, 'genes_1se']), lasso_gene_sign_1se_vec))

 sizeVal_sign_p_1se_lst <- split(sapply(sizeVal_sign_genes_1se_lst, length),
    sizeVal_results_frm$SimNo)
 
 sapply(sizeVal_sign_p_1se_lst, median)
})
names(sign_p_1se_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_signP')


p_singP_1se_Bysize_lst <- c(p_1se_Bysize_lst, sign_p_1se_Bysize_lst)
p_singP_1se_Bysize_lst <- p_singP_1se_Bysize_lst[order(names(p_singP_1se_Bysize_lst))]

boxplot(p_singP_1se_Bysize_lst,
  col=0,
  border=c('blue','green'),
  ylim=c(0, 200),
  xaxt='n'
)
mtext(side=2, outer=T,  'number of selected features')

for(JJ in 1:length(p_singP_1se_Bysize_lst))
points(
   x=jitter(rep(JJ, length(p_singP_1se_Bysize_lst[[JJ]])), amount=0.25),
   y=p_singP_1se_Bysize_lst[[JJ]], cex=0.5,
   col=ifelse(grepl('_p', names(p_singP_1se_Bysize_lst)[JJ]),'blue', 'green')
)

LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_p'),names(p_singP_1se_Bysize_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_p'),names(p_singP_1se_Bysize_lst))[-1] - 0.5, col='grey')
#abline(h= nzero_1se_q2, col = 'red')
legend('topleft',
   #title='1se errors', title.col = 'black',
   text.col = c('blue', 'green'),
   legend= c('selected genes','signature genes'),
   bty='n'
 )
title(paste('one se lamdba models'))


# min
#########################################
# selected features
p_min_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 sizeVal_p_min_lst <- with(sizeVal_results_frm, split(p_min, SimNo))
 sapply(sizeVal_p_min_lst, median)
})
names(p_min_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_p')

# selected signatue features
sign_p_min_Bysize_lst <- lapply(unique(hcc5hmC_lasso_sim_results_frm$Size),
function(SizeVal) {
 sizeVal_results_frm <- hcc5hmC_lasso_sim_results_frm %>% dplyr::filter(Size==SizeVal)
 
  
 sizeVal_sign_genes_min_lst <- lapply(1:nrow(sizeVal_results_frm), function(RR)
    intersect(unlist(sizeVal_results_frm[RR, 'genes_min']), lasso_gene_sign_min_vec))

 sizeVal_sign_p_min_lst <- split(sapply(sizeVal_sign_genes_min_lst, length),
    sizeVal_results_frm$SimNo)
 
 sapply(sizeVal_sign_p_min_lst, median)
})
names(sign_p_min_Bysize_lst) <- paste0(
 stringr::str_pad(unique(hcc5hmC_lasso_sim_results_frm$Size), width=3, pad='0'), '_signP')


p_singP_min_Bysize_lst <- c(p_min_Bysize_lst, sign_p_min_Bysize_lst)
p_singP_min_Bysize_lst <- p_singP_min_Bysize_lst[order(names(p_singP_min_Bysize_lst))]

boxplot(p_singP_min_Bysize_lst,
  col=0,
  border=c('blue','green'),
  ylim=c(0, 200),
  xaxt='n'
)
for(JJ in 1:length(p_singP_min_Bysize_lst))
points(
   x=jitter(rep(JJ, length(p_singP_min_Bysize_lst[[JJ]])), amount=0.25),
   y=p_singP_min_Bysize_lst[[JJ]], cex=0.5,
   col=ifelse(grepl('_p', names(p_singP_min_Bysize_lst)[JJ]),'blue', 'green')
)

LL <- -1
axis(side=1, tick=F, line = LL,
  at = match(paste0(SIZE0,'_p'),names(p_singP_min_Bysize_lst)),
  SIZE0
 )
abline(v= match(paste0(SIZE0,'_p'),names(p_singP_min_Bysize_lst))[-1] - 0.5, col='grey')
#abline(h= nzero_min_q2, col = 'red')
legend('topleft',
   #title='min errors', title.col = 'black',
   text.col = c('blue', 'green'),
   legend= c('selected genes','signature genes'),
   bty='n'
 )
title(paste('min lambda models'))

mtext(side=3, outer=T, cex=1.25, paste('lasso fit feature selection summarized across simulations'))


```


```{r hcc5hmC-glmnetSuite-print-lasso-simRes-features-OverSim, cache=T, cache.vars='', fig.cap='lasso Models Selected Features by Sample Size', include=F}
### CLEAR CACHE

p_sing_1se_Bysize_sum_frm <- t(sapply(p_singP_1se_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4)))
colnames(p_sing_1se_Bysize_sum_frm) <- paste0('1se_', colnames(p_sing_1se_Bysize_sum_frm))
 
p_sing_min_Bysize_sum_frm <- t(sapply(p_singP_min_Bysize_lst, function(LL) quantile(LL, prob=(1:3)/4)))
colnames(p_sing_min_Bysize_sum_frm) <- paste0('min_', colnames(p_sing_min_Bysize_sum_frm))

knitr::kable(cbind(p_sing_1se_Bysize_sum_frm, p_sing_min_Bysize_sum_frm),
    caption = paste("lasso feature selection by sample size across simulations"),
    digits=2) %>%
   kableExtra::kable_styling(full_width = F)

```

* The number of selected features increase with sample size.  

*  The number of selected features is quite variable, even for larger sample sizes.

* The number of core signature features among selected features is stable for larger sample
sizes and represents 30 to 40% of the selected features (for N=200 and 300 respectively).

## Effect of sample consistency

To see the effect of sample consistency on classification results, we will
focus on the performance of the small sample fits (N=25) where variability is the
greatest, and the context where investigators should be most aware of the
effect of sample selection over and beyond sample size concerns.

In the plot below, 

* control_Q and affected_Q are summary measures of
sample set consistencies for the control and affected groups in the  different
cv runs.   

* 1se_cv and 1se_test are the cross-validation and test set error rates, respectively.

```{r hcc5hmC-glmnetSuite-small-sample-qual, cache=T, cache.vars=c(''), fig.cap='sample consistency vs classifier performance'}

control_samp25_qual_vec <- apply(sim_control_qual_mtx[1:25, ], 2, median)
affected_samp25_qual_vec <- apply(sim_affected_qual_mtx[1:25, ], 2, median)

samp25_qual_error_mtx <- cbind(
   `1se_cv` = error_1se_Bysize_lst[['025_cv']],
   `1se_test` = error_1se_Bysize_lst[['025_test']],
   `control_Q` = control_samp25_qual_vec,
   `affected_Q` = affected_samp25_qual_vec)
 

# Correlation panel
panel.cor <- function(x, y){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- round(cor(x, y), digits=2)
    txt <- paste0("R = ", r)
    cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = 1.5) ###cex.cor * r)
}

pairs(samp25_qual_error_mtx,
 lower.panel = panel.cor)
   

     
```

We see that although the sample consistency effect is not strong, the consistency of the
affected sample cohorts does have a measurable impact on the 1se cv and test set error
rates (cor = -0.32 and -0.27, respectively).  

This plot is somewhat under-whelming.
