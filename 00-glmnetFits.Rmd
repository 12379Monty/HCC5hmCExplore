# The bet on sparsity {#explore-sparsity}

In this section we explore various fits that can be computed 
ans analyzed with tools provided in the `glmnet` package.
Refer to the [Glmnet Vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
for a quick reference guide.

## CV analysis setup 

```{r setParameters}

K_FOLD <- 10
trainP <- 0.8
EPS <- 0.02    # Have no idea what "small" epsilon means

```

First we divide the analysis dataset into `train` and `test` in a `r trainP/(1-trainP)`:1 ratio.  

```{r getTrainVal, cache=T, cache.vars=c('train_sampID_vec', 'test_sampID_vec','train_group_vec','test_group_vec','train_lcpm_mtx','test_lcpm_mtx')}

set.seed(1)
train_sampID_vec <- with(AF_dgel$samples,
AF_dgel$samples$sampID[caret::createDataPartition(y=group, p=trainP, list=F)]
)

test_sampID_vec <- with(AF_dgel$samples,
setdiff(sampID, train_sampID_vec)
)

train_group_vec <- AF_dgel$samples[train_sampID_vec, 'group']
test_group_vec <- AF_dgel$samples[test_sampID_vec, 'group']

knitr::kable(table(train_group_vec),
  caption="Train set") %>%
   kableExtra::kable_styling(full_width = F)

knitr::kable(table(test_group_vec),
  caption="Test set") %>%
   kableExtra::kable_styling(full_width = F)

train_lcpm_mtx <- t(lcpm_mtx[,train_sampID_vec])
test_lcpm_mtx <- t(lcpm_mtx[,test_sampID_vec])

```

We explore some glmnet fits and the "bet on sparsity"    
  
* Consider models:  
    - lasso: $\alpha = 1.0$ - sparse model  
    - ridge $\alpha = 0$ - shrunken coefficients model
    - elastic net:  $\alpha = 0.5$  - semi sparse model
    - lassoC: $\alpha = 1-\epsilon =$ `r 1- EPS` - lasso for correlated predictors  
* Does the relaxed lasso improve performance?    
* Does the shrunken relaxed lasso (aka the blended mix) improve performance  
* How sparse is the model undelying best 5hmC classifier for Early HCC vs Control?    
* Is the degree of sparsity, or the size of the model, a stable feature of the problem and data set?  

In this analysis, we will only evaluate models in terms of 
model size, stability and performance.  We leave the question
of significance testing of hypotheses about model parameters
completely out.  See Lockhart et al. (2014) [@Lockhart:2014aa]
and Wassermam (2014) [@Wasserman:2014aa] for a discussion of this topic.


Next we create folds for `r K_FOLD`-fold cross-validation of models fitted to
training data.  We'll use caret::createFolds to assign samples
to folds while keeping the outcome ratios constant across folds.


```{r getTrainFolds, cache=T, cache.vars='train_foldid_vec'}
# This is too variable, both in terms of fold size And composition
#foldid_vec <- sample(1:10, size=length(train_group_vec), replace=T)

set.seed(1)
train_foldid_vec <- caret::createFolds(
 factor(train_group_vec), 
 k=K_FOLD,
 list=F)

knitr::kable(sapply(split(train_group_vec, train_foldid_vec), 
  table), caption="training samples fold composition") %>%
   kableExtra::kable_styling(full_width = F)
 
```

Note that the folds identify samples that are left-out of the training
data for each fold fit.


## Fit and compare models 


* cross-validated accuracy
* test set accuracy  
* sparsity
    - for lasso, enet and lassoC, examine number of selected variables

Although “the one standard error rule” can produce a model with fewer predictors, it usually results in increased MSE and more biased parameter estimates
(see Engebretsen et al. (2019) [@Engebretsen:2019aa] for example).
We will look at both the minimum cv error and the one standard error rule model 
preformance.

```{r fitModels, cache=T, cache.vars=c('cv_lasso', 'cv_ridge', 'cv_enet', 'cv_lassoC')}

require(doMC)
registerDoMC(cores=14)

start_time <-  proc.time()

cv_lasso <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=1,
 family='binomial', 
 type.measure = "class")

message("lasso time: ", round((proc.time() - start_time)[3],2),"s")

start_time <-  proc.time()

cv_ridge <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=0,
 family='binomial', 
 type.measure = "class")

message("ridge time: ", round((proc.time() - start_time)[3],2),"s")

start_time <-  proc.time()

cv_enet <- glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=0.5,
 family='binomial',
 type.measure = "class")

message("enet time: ", round((proc.time() - start_time)[3],2),"s")

start_time <-  proc.time()

cv_lassoC <-  glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=1-EPS,
 family='binomial',
 type.measure = "class")

message("lassoC time: ", round((proc.time() - start_time)[3],2),"s")

```

The ridge regression model takes over 10 times longer to compute.


<!-- do not show
Define plotting function.
Maybe show in appendix??
-->
```{r plot_cv_f,echo=F}

plot_cv_f <- function(cv_fit, Nzero=T, ...) {
 
 lambda.1se_p <- cv_fit$nzero[cv_fit$lambda==cv_fit$lambda.1se]
 lambda.min_p <- cv_fit$nzero[cv_fit$lambda==cv_fit$lambda.min]
 
 test_pred_1se_vec <- predict(
  cv_fit, 
  newx=test_lcpm_mtx, 
  s="lambda.1se",
  type="class"
 )
 test_pred_1se_error <- mean(test_pred_1se_vec!=test_group_vec)
 
 test_pred_min_vec <- predict(
  cv_fit, 
  newx=test_lcpm_mtx, 
  s="lambda.min",
  type="class"
 )
 test_pred_min_error <- mean(test_pred_min_vec!=test_group_vec)
 
  
 plot(
  log(cv_fit$lambda),
  cv_fit$cvm,
  pch=16,col="red",
  xlab='',ylab='',
  ...
 )
 abline(v=log(c(cv_fit$lambda.1se, cv_fit$lambda.min)))
 if(Nzero)
 axis(side=3, tick=F, at=log(cv_fit$lambda), 
  labels=cv_fit$nzero, line=-1
 )
 LL <- 2
 #mtext(side=1, outer=F, line=LL, "log(Lambda)")
 #LL <- LL+1
 mtext(side=1, outer=F, line=LL, paste(
  #ifelse(Nzero, paste("1se p =", lambda.1se_p),''),
  "1se: cv =", round(100*cv_fit$cvm[cv_fit$lambda==cv_fit$lambda.1se],1),
  "test =", round(100*test_pred_1se_error,1)
 ))
 LL <- LL+1
 mtext(side=1, outer=F, line=LL, paste(
  #ifelse(Nzero, paste("min p =", lambda.min_p),''),
  "min: cv =", round(100*cv_fit$cvm[cv_fit$lambda==cv_fit$lambda.min],1),
  "test =", round(100*test_pred_min_error,1)
 ))
 
}

```

Examine model performance.

```{r lookFits, cache=T, cache.vars='', fig.height=5, fig.width=11, fig.cap="compare fits", echo=F}
 par(mfrow=c(1,3), mar=c(5, 2, 3, 1), oma=c(3,2,0,0)) 
 plot_cv_f(cv_lasso, ylim=c(0,.5))
 title('lasso')

 plot_cv_f(cv_ridge, Nzero=F, ylim=c(0,.5))
 title('ridge')

 plot_cv_f(cv_enet, ylim=c(0,.5))
 title('enet')

 mtext(side=1, outer=T, cex=1.25, 'log(Lambda)')
 mtext(side=2, outer=T, cex=1.25, cv_lasso$name)

```

All models produce cv assessments of `r cv_lasso$name` that are substnatially lower than
the test set assessments.  `lasso` performs comparably to `enet` and
better than the `ridge` model.

## Relaxed lasso and blended mix

```{r fitLassoR, cache=T, cache.vars=c('cv_lassoR'), include=F}

require(doMC)
registerDoMC(cores=14)


start_time <-  proc.time()

cv_lassoR <-  glmnet::cv.glmnet(
 x=train_lcpm_mtx,
 y=train_group_vec,
 foldid=train_foldid_vec,
 alpha=1,
 relax=T,
 family='binomial',
 type.measure = "class")

message("lassoR time: ", round((proc.time() - start_time)[3],2),"s")

```

<!--
The relaxed fit takes quite a bit longer.  
-->

```{r lookLassoR, cache=T, cache.vars='', fig.height=5, fig.width=5, fig.cap="lassoR fit"}
 library(glmnet)

 cv_lassoR_sum <- print(cv_lassoR)

 plot(cv_lassoR)

test_pred_1se_vec <- predict(
  cv_lassoR, 
  newx=test_lcpm_mtx, 
  s="lambda.1se",
  type="class"
)
test_pred_1se_error <- mean(test_pred_1se_vec!=test_group_vec)
 
test_pred_min_vec <- predict(
  cv_lassoR, 
  newx=test_lcpm_mtx, 
  s="lambda.min",
  type="class"
)
test_pred_min_error <- mean(test_pred_min_vec!=test_group_vec)
  
cv_lassoR_1se_error <- cv_lassoR_sum['1se','Measure']
cv_lassoR_min_error <- cv_lassoR_sum['min','Measure']

knitr::kable(rbind(
 onese=c(cv_error=cv_lassoR_1se_error, test_error=test_pred_1se_error)*100,
 min=c(cv_error=cv_lassoR_min_error, test_error=test_pred_min_error)*100
),
 caption="CV vs test Errors", digits=1
) %>%
  kableExtra::kable_styling(full_width = F)


```

Look at confusion matrices

```{r confMtxTrainLasso, cache=T, cache.vars='', fig.cap="Train set confusion", echo=F}

cv_lasso_cnf <- glmnet::confusion.glmnet(
 cv_lasso, 
 newx=train_lcpm_mtx,
 newy=train_group_vec
)

knitr::kable(cv_lasso_cnf, caption="cv lasso confusion matrix: train set") %>%
  kableExtra::kable_styling(full_width = F)

```

```{r confMtxTrainLassoR, cache=T, cache.vars='', fig.cap="Train set confusion", echo=F}
cv_lassoR_cnf <- glmnet::confusion.glmnet(
 cv_lassoR, 
 newx=train_lcpm_mtx,
 newy=train_group_vec
)

knitr::kable(cv_lassoR_cnf, caption="cv lassoR confusion matrix: train set")  %>%
  kableExtra::kable_styling(full_width = F)


```


```{r confMtxTestLasso, cache=T, cache.vars='', fig.cap="Test set confusion", echo=F}

cv_lasso_cnf <- glmnet::confusion.glmnet(
 cv_lasso, 
 newx=test_lcpm_mtx,
 newy=test_group_vec
)

knitr::kable(cv_lasso_cnf, caption="cv lasso confusion matrix: test set") %>%
  kableExtra::kable_styling(full_width = F)

```

```{r confMtxTestlassoR, cache=T, cache.vars='', fig.cap="Test set confusion", echo=F}

cv_lassoR_cnf <- glmnet::confusion.glmnet(
 cv_lassoR, 
 newx=test_lcpm_mtx,
 newy=test_group_vec
)

knitr::kable(cv_lassoR_cnf, caption="cv lassoR confusion matrix: test set")  %>%
  kableExtra::kable_styling(full_width = F)


```


In all models the sensitivity weak compared to the sensitivity.  Let's examine the 
ROC curves to see where the trade-off is.

